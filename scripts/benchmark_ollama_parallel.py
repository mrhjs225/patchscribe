#!/usr/bin/env python3
"""
Measure Ollama chat latency under different parallel request counts.

The script sends a fixed number of prompts to the specified Ollama endpoint
using thread-based concurrency (1, 2, 3, 4 workers by default) and reports
latency / throughput statistics for each setting. The default model is
``DeepSeek-R1:1.5b``.

Note: Ollama model names are case-sensitive. Use 'ollama list' to check exact names.
"""
from __future__ import annotations

import argparse
import math
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from typing import Iterable, List

try:
    import requests
except ImportError:
    print("âŒ requests ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install requests")
    sys.exit(1)


DEFAULT_PROMPT = (
    "ë‹¤ìŒ C ì½”ë“œê°€ ì™œ ìœ„í—˜í•œì§€ í•œ ë¬¸ìž¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n"
    "\n"
    "void process(char *input) {\n"
    "    char buf[16];\n"
    "    strcpy(buf, input);\n"
    "}\n"
)


@dataclass
class RequestStat:
    idx: int
    elapsed: float
    success: bool
    error: str | None = None


def build_payload(model: str, prompt: str, api: str) -> dict:
    """Construct the Ollama payload for the requested API."""
    if api == "chat":
        return {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "stream": False,
        }
    return {
        "model": model,
        "prompt": prompt,
        "stream": False,
    }


def run_single_request(
    idx: int,
    endpoint: str,
    payload: dict,
    timeout: float,
    api: str,
) -> RequestStat:
    """Send a single request to Ollama and capture latency."""
    start = time.perf_counter()
    try:
        response = requests.post(
            endpoint,
            headers={"Content-Type": "application/json"},
            json=payload,
            timeout=timeout,
        )
        response.raise_for_status()
        data = response.json()
        # Accessing the content verifies response integrity but we ignore the text.
        if api == "chat":
            message = data.get("message", {})
            if not isinstance(message, dict) or "content" not in message:
                raise ValueError("ì‘ë‹µ ë³¸ë¬¸ì— message.content ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            if "response" not in data:
                raise ValueError("ì‘ë‹µ ë³¸ë¬¸ì— response í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as exc:
        elapsed = time.perf_counter() - start
        error_msg = str(exc)
        if api == "chat" and hasattr(requests, "exceptions"):
            http_error = (
                isinstance(exc, requests.exceptions.HTTPError)
                and getattr(exc, "response", None) is not None
                and exc.response.status_code == 404
            )
            if http_error:
                error_msg += " - /api/chat ì—”ë“œí¬ì¸íŠ¸ê°€ ë¹„í™œì„±í™”ëœ Ollama ë²„ì „ ê°™ìŠµë‹ˆë‹¤. --api generate ë¡œ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."
        return RequestStat(idx=idx, elapsed=elapsed, success=False, error=error_msg)
    elapsed = time.perf_counter() - start
    return RequestStat(idx=idx, elapsed=elapsed, success=True)


def percentile(values: Iterable[float], pct: float) -> float | None:
    """Return percentile using linear interpolation."""
    data = sorted(values)
    if not data:
        return None
    k = (len(data) - 1) * (pct / 100.0)
    f = math.floor(k)
    c = math.ceil(k)
    if f == c:
        return data[int(k)]
    return data[f] + (data[c] - data[f]) * (k - f)


def benchmark_concurrency(
    concurrency: int,
    endpoint: str,
    model: str,
    prompt: str,
    requests_per_level: int,
    timeout: float,
    api: str,
) -> dict:
    """Execute the benchmark for a single concurrency level."""
    print(f"\n{'=' * 70}")
    print(f"ðŸ” ë™ì‹œ ìš”ì²­ ìˆ˜: {concurrency}")
    print(f"{'-' * 70}")
    print(
        f"ìš”ì²­ íšŸìˆ˜: {requests_per_level}, ëª¨ë¸: {model}, "
        f"ì—”ë“œí¬ì¸íŠ¸: {endpoint} (API: {api})"
    )

    payload = build_payload(model, prompt, api)
    results: List[RequestStat] = []

    start_wall = time.perf_counter()
    with ThreadPoolExecutor(max_workers=concurrency) as executor:
        futures = [
            executor.submit(run_single_request, idx, endpoint, payload, timeout, api)
            for idx in range(1, requests_per_level + 1)
        ]
        for future in as_completed(futures):
            stat = future.result()
            results.append(stat)
            status = "âœ… ì„±ê³µ" if stat.success else "âŒ ì‹¤íŒ¨"
            print(f"[{status}] ìš”ì²­ #{stat.idx:02d} - {stat.elapsed:.2f}ì´ˆ", end="")
            if stat.error:
                print(f" (ì˜¤ë¥˜: {stat.error})")
            else:
                print()
    wall_time = time.perf_counter() - start_wall

    successes = [r for r in results if r.success]
    failures = [r for r in results if not r.success]
    latencies = [r.elapsed for r in successes]

    avg_latency = sum(latencies) / len(latencies) if latencies else None
    p95 = percentile(latencies, 95) if latencies else None

    throughput = (
        len(successes) / wall_time if wall_time > 0 and successes else 0.0
    )

    print(f"\nì´ ì†Œìš” ì‹œê°„: {wall_time:.2f}ì´ˆ")
    print(f"ì„±ê³µ: {len(successes)} / ì‹¤íŒ¨: {len(failures)}")
    if avg_latency is not None:
        print(f"í‰ê·  ì§€ì—° ì‹œê°„: {avg_latency:.2f}ì´ˆ")
    if p95 is not None:
        print(f"95í¼ì„¼íƒ€ì¼ ì§€ì—° ì‹œê°„: {p95:.2f}ì´ˆ")
    print(f"ì²˜ë¦¬ëŸ‰: {throughput:.3f} ìš”ì²­/ì´ˆ")

    return {
        "concurrency": concurrency,
        "wall_time": wall_time,
        "successes": len(successes),
        "failures": len(failures),
        "avg_latency": avg_latency,
        "p95_latency": p95,
        "throughput": throughput,
    }


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Ollama deepseek-r1 ë³‘ë ¬ ì²˜ë¦¬ ë²¤ì¹˜ë§ˆí¬ ë„êµ¬"
    )
    parser.add_argument(
        "--endpoint",
        help="Ollama API ì—”ë“œí¬ì¸íŠ¸ (ì˜ˆ: http://127.0.0.1:11434/api/generate)",
    )
    parser.add_argument(
        "--model",
        default="DeepSeek-R1:1.5b",
        help="ë²¤ì¹˜ë§ˆí¬í•  ëª¨ë¸ ì´ë¦„ (ëŒ€ì†Œë¬¸ìž ì •í™•ížˆ ìž…ë ¥)",
    )
    parser.add_argument(
        "--requests",
        type=int,
        default=32,
        help="ê° ë™ì‹œì„± ìˆ˜ì¤€ì—ì„œ ë³´ë‚¼ ì´ ìš”ì²­ ìˆ˜",
    )
    parser.add_argument(
        "--levels",
        type=int,
        nargs="+",
        default=[1, 4, 8],
        help="í…ŒìŠ¤íŠ¸í•  ë™ì‹œ ìš”ì²­ ìˆ˜ ëª©ë¡",
    )
    parser.add_argument(
        "--timeout",
        type=float,
        default=120.0,
        help="ê° ìš”ì²­ íƒ€ìž„ì•„ì›ƒ (ì´ˆ)",
    )
    parser.add_argument(
        "--prompt",
        default=DEFAULT_PROMPT,
        help="í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©í•  í”„ë¡¬í”„íŠ¸ (ê¸°ë³¸ê°’ì€ ê°„ë‹¨í•œ C ì˜ˆì œ)",
    )
    parser.add_argument(
        "--api",
        choices=["chat", "generate"],
        default="chat",
        help="Ollama API íƒ€ìž… ì„ íƒ (chat ë˜ëŠ” generate)",
    )
    return parser.parse_args()


def print_summary(results: List[dict]) -> None:
    print(f"\n{'=' * 70}")
    print("ðŸ“Š ë™ì‹œì„± ìˆ˜ì¤€ë³„ ìš”ì•½")
    print(f"{'=' * 70}")
    print(
        f"{'ë™ì‹œì„±':>6} | {'ì„±ê³µ':>4} | {'ì‹¤íŒ¨':>4} | {'ì´ì‹œê°„(ì´ˆ)':>11} | "
        f"{'í‰ê· ì§€ì—°(ì´ˆ)':>12} | {'P95(ì´ˆ)':>8} | {'ì²˜ë¦¬ëŸ‰(req/s)':>16}"
    )
    print("-" * 70)
    for item in results:
        avg = f"{item['avg_latency']:.2f}" if item["avg_latency"] is not None else "-"
        p95 = f"{item['p95_latency']:.2f}" if item["p95_latency"] is not None else "-"
        print(
            f"{item['concurrency']:>6} | "
            f"{item['successes']:>4} | "
            f"{item['failures']:>4} | "
            f"{item['wall_time']:>11.2f} | "
            f"{avg:>12} | "
            f"{p95:>8} | "
            f"{item['throughput']:>16.3f}"
        )


def main() -> None:
    args = parse_args()
    if not args.endpoint:
        if args.api == "chat":
            args.endpoint = "http://127.0.0.1:11434/api/chat"
        else:
            args.endpoint = "http://127.0.0.1:11434/api/generate"

    results = []
    for level in args.levels:
        if level < 1:
            print(f"âš ï¸  ë™ì‹œì„± ìˆ˜ì¤€ {level} ì€(ëŠ”) ë¬´ì‹œí•©ë‹ˆë‹¤. 1 ì´ìƒì˜ ê°’ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            continue
        results.append(
            benchmark_concurrency(
                concurrency=level,
                endpoint=args.endpoint,
                model=args.model,
                prompt=args.prompt,
                requests_per_level=args.requests,
                timeout=args.timeout,
                api=args.api,
            )
        )
    if results:
        print_summary(results)


if __name__ == "__main__":
    main()
