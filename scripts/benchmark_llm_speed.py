#!/usr/bin/env python3
"""
LLM ì†ë„ ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸
Ollama ë¡œì»¬ vs Ollama ì›ê²© (ë™ì¼ ëª¨ë¸ DeepSeek) ì‘ë‹µ ì‹œê°„ ë¹„êµ
"""
import sys
import time
from pathlib import Path

try:
    import requests
except ImportError:
    print("âŒ requests ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install requests")
    sys.exit(1)


def benchmark_ollama(endpoint: str, model: str, label: str, num_requests: int = 10) -> dict:
    """
    Ollama APIë¡œ ë²¤ì¹˜ë§ˆí¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        endpoint: Ollama API ì—”ë“œí¬ì¸íŠ¸ (ì˜ˆ: http://127.0.0.1:11434/api/generate)
        model: ëª¨ë¸ ì´ë¦„ (ì˜ˆ: deepseek-r1:1.5b)
        label: í‘œì‹œ ì´ë¦„ (ì˜ˆ: "ë¡œì»¬" ë˜ëŠ” "ì›ê²©")
        num_requests: ìš”ì²­ íšŸìˆ˜
        
    Returns:
        ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    print(f"\n{'='*60}")
    print(f"ë²¤ì¹˜ë§ˆí‚¹: {label} - {model}")
    print(f"{'='*60}")
    print(f"ì—”ë“œí¬ì¸íŠ¸: {endpoint}")
    
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
    test_prompt = """ë‹¤ìŒ C ì½”ë“œì˜ ë²„í¼ ì˜¤ë²„í”Œë¡œìš° ì·¨ì•½ì ì„ ê°„ë‹¨íˆ ì„¤ëª…í•˜ì„¸ìš”:

void process_data(char *input) {
    char buffer[10];
    strcpy(buffer, input);
}

í•œ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."""
    
    times = []
    responses = []
    
    print(f"\n{num_requests}ë²ˆì˜ ìš”ì²­ì„ ë³´ë‚´ëŠ” ì¤‘...\n")
    
    for i in range(num_requests):
        try:
            start_time = time.time()
            
            # Ollama API í˜¸ì¶œ
            response = requests.post(
                endpoint,
                headers={"Content-Type": "application/json"},
                json={
                    "model": model,
                    "prompt": test_prompt,
                    "stream": False
                },
                timeout=120
            )
            response.raise_for_status()
            
            elapsed = time.time() - start_time
            data = response.json()
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            response_text = data.get("response", "")
            
            times.append(elapsed)
            responses.append(response_text)
            
            print(f"  ìš”ì²­ #{i+1:2d}: {elapsed:.2f}ì´ˆ - "
                  f"ì‘ë‹µ ê¸¸ì´: {len(response_text)}ì")
            
        except Exception as e:
            elapsed = time.time() - start_time
            print(f"  ìš”ì²­ #{i+1:2d}: âŒ ì‹¤íŒ¨ ({elapsed:.2f}ì´ˆ) - {str(e)}")
            times.append(None)
            responses.append(None)
    
    # ì„±ê³µí•œ ìš”ì²­ë§Œ í•„í„°ë§
    successful_times = [t for t in times if t is not None]
    
    if not successful_times:
        print(f"\nâŒ ëª¨ë“  ìš”ì²­ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return None
    
    # í†µê³„ ê³„ì‚°
    total_time = sum(successful_times)
    avg_time = total_time / len(successful_times)
    min_time = min(successful_times)
    max_time = max(successful_times)
    success_rate = len(successful_times) / num_requests * 100
    
    results = {
        "label": label,
        "endpoint": endpoint,
        "model": model,
        "num_requests": num_requests,
        "successful_requests": len(successful_times),
        "success_rate": success_rate,
        "total_time": total_time,
        "avg_time": avg_time,
        "min_time": min_time,
        "max_time": max_time,
        "times": successful_times,
    }
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\n{'â”€'*60}")
    print(f"ğŸ“Š ê²°ê³¼ ìš”ì•½:")
    print(f"{'â”€'*60}")
    print(f"  ì„±ê³µë¥ :        {success_rate:.1f}% ({len(successful_times)}/{num_requests})")
    print(f"  ì´ ì†Œìš” ì‹œê°„:  {total_time:.2f}ì´ˆ")
    print(f"  í‰ê·  ì‘ë‹µ ì‹œê°„: {avg_time:.2f}ì´ˆ")
    print(f"  ìµœì†Œ ì‘ë‹µ ì‹œê°„: {min_time:.2f}ì´ˆ")
    print(f"  ìµœëŒ€ ì‘ë‹µ ì‹œê°„: {max_time:.2f}ì´ˆ")
    
    # ì²« ë²ˆì§¸ ì‘ë‹µ ìƒ˜í”Œ ì¶œë ¥
    first_response = next((r for r in responses if r), None)
    if first_response:
        print(f"\n  ìƒ˜í”Œ ì‘ë‹µ (ì²« 150ì):")
        print(f"  {first_response[:150]}...")
    
    return results


def compare_results(local_results: dict, remote_results: dict):
    """ë‘ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print(f"\n{'='*70}")
    print(f"ğŸ ìµœì¢… ë¹„êµ - {local_results['model']}")
    print(f"{'='*70}\n")
    
    if local_results is None or remote_results is None:
        print("âš ï¸  ì¼ë¶€ ë²¤ì¹˜ë§ˆí¬ê°€ ì‹¤íŒ¨í•˜ì—¬ ë¹„êµí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    local_avg = local_results["avg_time"]
    remote_avg = remote_results["avg_time"]
    
    print(f"{'í•­ëª©':<20} {'ë¡œì»¬ Ollama':<25} {'ì›ê²© Ollama':<25} {'ì°¨ì´':<15}")
    print(f"{'-'*85}")
    print(f"{'ì—”ë“œí¬ì¸íŠ¸':<20} {'127.0.0.1:11434':<25} {'115.145.178.10:11434':<25}")
    print(f"{'ì„±ê³µë¥ ':<20} {local_results['success_rate']:<24.1f}% "
          f"{remote_results['success_rate']:<24.1f}% "
          f"{abs(local_results['success_rate'] - remote_results['success_rate']):.1f}%p")
    print(f"{'í‰ê·  ì‘ë‹µ ì‹œê°„':<20} {local_avg:<24.2f}ì´ˆ {remote_avg:<24.2f}ì´ˆ "
          f"{abs(local_avg - remote_avg):.2f}ì´ˆ")
    print(f"{'ì´ ì†Œìš” ì‹œê°„':<20} {local_results['total_time']:<24.2f}ì´ˆ "
          f"{remote_results['total_time']:<24.2f}ì´ˆ "
          f"{abs(local_results['total_time'] - remote_results['total_time']):.2f}ì´ˆ")
    print(f"{'ìµœì†Œ ì‘ë‹µ ì‹œê°„':<20} {local_results['min_time']:<24.2f}ì´ˆ "
          f"{remote_results['min_time']:<24.2f}ì´ˆ "
          f"{abs(local_results['min_time'] - remote_results['min_time']):.2f}ì´ˆ")
    print(f"{'ìµœëŒ€ ì‘ë‹µ ì‹œê°„':<20} {local_results['max_time']:<24.2f}ì´ˆ "
          f"{remote_results['max_time']:<24.2f}ì´ˆ "
          f"{abs(local_results['max_time'] - remote_results['max_time']):.2f}ì´ˆ")
    
    # ì†ë„ ë°°ìˆ˜ ê³„ì‚°
    print(f"\n{'='*70}")
    print("ğŸ“Š ì„±ëŠ¥ ë¶„ì„")
    print(f"{'='*70}")
    
    if local_avg < remote_avg:
        speedup = remote_avg / local_avg
        slower_time = remote_avg - local_avg
        print(f"âš¡ ë¡œì»¬ Ollamaê°€ ì›ê²©ë³´ë‹¤ {speedup:.2f}ë°° ë¹ ë¦…ë‹ˆë‹¤!")
        print(f"   (ìš”ì²­ë‹¹ í‰ê·  {slower_time:.2f}ì´ˆ ì ˆì•½)")
    else:
        speedup = local_avg / remote_avg
        slower_time = local_avg - remote_avg
        print(f"âš¡ ì›ê²© Ollamaê°€ ë¡œì»¬ë³´ë‹¤ {speedup:.2f}ë°° ë¹ ë¦…ë‹ˆë‹¤!")
        print(f"   (ìš”ì²­ë‹¹ í‰ê·  {slower_time:.2f}ì´ˆ ì ˆì•½)")
    
    # ì²˜ë¦¬ëŸ‰ ë¹„êµ (ì´ˆë‹¹ ìš”ì²­ ìˆ˜)
    local_throughput = local_results['successful_requests'] / local_results['total_time'] if local_results['total_time'] > 0 else 0
    remote_throughput = remote_results['successful_requests'] / remote_results['total_time'] if remote_results['total_time'] > 0 else 0
    
    print(f"\nğŸ“ˆ ì²˜ë¦¬ëŸ‰ (ì´ˆë‹¹ ìš”ì²­ ìˆ˜):")
    print(f"   ë¡œì»¬ Ollama:  {local_throughput:.3f} req/s")
    print(f"   ì›ê²© Ollama:  {remote_throughput:.3f} req/s")
    
    # ë„¤íŠ¸ì›Œí¬ ì˜¤ë²„í—¤ë“œ ì¶”ì •
    network_overhead = abs(remote_avg - local_avg)
    print(f"\nğŸŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë²„í—¤ë“œ ì¶”ì •:")
    print(f"   í‰ê·  ì™•ë³µ ì‹œê°„ ì°¨ì´: {network_overhead:.2f}ì´ˆ")
    
    # ê°œë³„ ì‘ë‹µ ì‹œê°„ ë¹„êµ (í‘œì¤€í¸ì°¨)
    if len(local_results['times']) > 1 and len(remote_results['times']) > 1:
        import statistics
        local_std = statistics.stdev(local_results['times'])
        remote_std = statistics.stdev(remote_results['times'])
        print(f"\nğŸ“‰ ì‘ë‹µ ì‹œê°„ ì•ˆì •ì„± (í‘œì¤€í¸ì°¨):")
        print(f"   ë¡œì»¬ Ollama:  {local_std:.2f}ì´ˆ")
        print(f"   ì›ê²© Ollama:  {remote_std:.2f}ì´ˆ")
        if local_std < remote_std:
            print(f"   â†’ ë¡œì»¬ì´ {remote_std/local_std:.2f}ë°° ë” ì•ˆì •ì ì…ë‹ˆë‹¤")
        else:
            print(f"   â†’ ì›ê²©ì´ {local_std/remote_std:.2f}ë°° ë” ì•ˆì •ì ì…ë‹ˆë‹¤")


def main():
    """ë©”ì¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="ë¡œì»¬ Ollama vs ì›ê²© Ollama ì†ë„ ë²¤ì¹˜ë§ˆí¬ (ë™ì¼ ëª¨ë¸)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
  # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰ (ê° 10ë²ˆì”©, deepseek-r1:1.5b)
  python scripts/benchmark_llm_speed.py
  
  # ìš”ì²­ íšŸìˆ˜ ë³€ê²½
  python scripts/benchmark_llm_speed.py --num-requests 20
  
  # ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©
  python scripts/benchmark_llm_speed.py --model llama3.2:1b
  
  # ë¡œì»¬ë§Œ í…ŒìŠ¤íŠ¸
  python scripts/benchmark_llm_speed.py --only-local
  
  # ì›ê²©ë§Œ í…ŒìŠ¤íŠ¸
  python scripts/benchmark_llm_speed.py --only-remote
        """
    )
    
    parser.add_argument(
        "--num-requests", "-n",
        type=int,
        default=10,
        help="ê° ì—”ë“œí¬ì¸íŠ¸ì— ë³´ë‚¼ ìš”ì²­ íšŸìˆ˜ (ê¸°ë³¸ê°’: 10)"
    )
    parser.add_argument(
        "--model", "-m",
        default="deepseek-r1:1.5b",
        help="í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: deepseek-r1:1.5b)"
    )
    parser.add_argument(
        "--local-endpoint",
        default="http://127.0.0.1:11434/api/generate",
        help="ë¡œì»¬ Ollama ì—”ë“œí¬ì¸íŠ¸ (ê¸°ë³¸ê°’: http://127.0.0.1:11434/api/generate)"
    )
    parser.add_argument(
        "--remote-endpoint",
        default="http://115.145.178.10:11434/api/generate",
        help="ì›ê²© Ollama ì—”ë“œí¬ì¸íŠ¸ (ê¸°ë³¸ê°’: http://115.145.178.10:11434/api/generate)"
    )
    parser.add_argument(
        "--only-local",
        action="store_true",
        help="ë¡œì»¬ë§Œ í…ŒìŠ¤íŠ¸"
    )
    parser.add_argument(
        "--only-remote",
        action="store_true",
        help="ì›ê²©ë§Œ í…ŒìŠ¤íŠ¸"
    )
    
    args = parser.parse_args()
    
    print("\nğŸš€ Ollama ì†ë„ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
    print(f"{'='*70}")
    print(f"   ëª¨ë¸: {args.model}")
    print(f"   ìš”ì²­ íšŸìˆ˜: {args.num_requests}íšŒ")
    print(f"{'='*70}")
    print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ëŒ€ìƒ:")
    if not args.only_remote:
        print(f"   - ë¡œì»¬ Ollama:  {args.local_endpoint}")
    if not args.only_local:
        print(f"   - ì›ê²© Ollama:  {args.remote_endpoint}")
    
    local_results = None
    remote_results = None
    
    # ë¡œì»¬ Ollama ë²¤ì¹˜ë§ˆí¬
    if not args.only_remote:
        print(f"\n{'='*70}")
        print(f"1ï¸âƒ£  ë¡œì»¬ Ollama ë²¤ì¹˜ë§ˆí¬ ì¤€ë¹„")
        print(f"{'='*70}")
        print(f"ğŸ’¡ ë¡œì»¬ Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”:")
        print(f"   $ ollama serve")
        print(f"   $ ollama list  # {args.model} ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸")
        print(f"   $ ollama pull {args.model}  # ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ")
        input("\n   ì¤€ë¹„ë˜ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
        local_results = benchmark_ollama(
            args.local_endpoint,
            args.model,
            "ë¡œì»¬ Ollama",
            args.num_requests
        )
    
    # ì›ê²© Ollama ë²¤ì¹˜ë§ˆí¬
    if not args.only_local:
        print(f"\n{'='*70}")
        print(f"2ï¸âƒ£  ì›ê²© Ollama ë²¤ì¹˜ë§ˆí¬ ì¤€ë¹„")
        print(f"{'='*70}")
        print(f"ğŸ’¡ ì›ê²© Ollama ì„œë²„ê°€ ì ‘ê·¼ ê°€ëŠ¥í•œì§€ í™•ì¸í•˜ì„¸ìš”:")
        print(f"   ì—”ë“œí¬ì¸íŠ¸: {args.remote_endpoint}")
        print(f"   ëª¨ë¸: {args.model}")
        print(f"\n   í…ŒìŠ¤íŠ¸ curl ëª…ë ¹ì–´:")
        print(f"   curl -X POST {args.remote_endpoint} \\")
        print(f"     -H 'Content-Type: application/json' \\")
        print(f"     -d '{{\"model\": \"{args.model}\", \"prompt\": \"test\", \"stream\": false}}'")
        input("\n   ì¤€ë¹„ë˜ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
        remote_results = benchmark_ollama(
            args.remote_endpoint,
            args.model,
            "ì›ê²© Ollama",
            args.num_requests
        )
    
    # ë¹„êµ ê²°ê³¼ ì¶œë ¥
    if local_results and remote_results:
        compare_results(local_results, remote_results)
    elif local_results:
        print(f"\nâœ… ë¡œì»¬ Ollama ë²¤ì¹˜ë§ˆí¬ë§Œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    elif remote_results:
        print(f"\nâœ… ì›ê²© Ollama ë²¤ì¹˜ë§ˆí¬ë§Œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print(f"\n{'='*70}")
    print("âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    main()
