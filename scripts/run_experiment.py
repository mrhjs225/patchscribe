#!/usr/bin/env python3
"""
PatchScribe í†µí•© ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸

ë¡œì»¬ ë° ë¶„ì‚° í™˜ê²½ì—ì„œ ëª¨ë“  ëª¨ë¸ê³¼ ì¡°ê±´ì— ëŒ€í•œ ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
ì´ ìŠ¤í¬ë¦½íŠ¸ í•˜ë‚˜ë¡œ ì „ì²´ ì‹¤í—˜ ì›Œí¬í”Œë¡œìš°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

ì‚¬ìš© ì˜ˆì‹œ:
    # ë¡œì»¬ ì‹¤í—˜ (3ê°œ ì¼€ì´ìŠ¤ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
    python3 scripts/run_experiment.py --quick

    # ë¡œì»¬ ì‹¤í—˜ (ì „ì²´)
    python3 scripts/run_experiment.py --dataset zeroday --limit 10

    # ë¶„ì‚° ì‹¤í—˜ (Server 0, 4ëŒ€ ì„œë²„ ì¤‘)
    python3 scripts/run_experiment.py --distributed 0 4 20 --dataset zeroday

    # íŠ¹ì • ëª¨ë¸ê³¼ ì¡°ê±´ë§Œ
    python3 scripts/run_experiment.py --dataset zeroday --limit 10 \
        --models ollama:llama3.2:1b ollama:llama3.2:3b \
        --conditions c4
"""
import json
import sys
import os
import argparse
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from datetime import datetime
from dataclasses import dataclass

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from patchscribe.llm import (
    DEFAULT_GEMINI_ENDPOINT_TEMPLATE,
    DEFAULT_GEMINI_MODEL as LLM_DEFAULT_GEMINI_MODEL,
    DEFAULT_OLLAMA_ENDPOINT,
    DEFAULT_OPENAI_ENDPOINT,
    PromptOptions,
)


# ì „ì²´ ì‹¤í—˜ ëŒ€ìƒ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (16ê°œ)
DEFAULT_MODELS = [
    "qwen3:14b",
    "qwen3:8b",
    "qwen3:4b",
    "qwen3:1.7b",
    "gemma3:12b",
    "gemma3:4b",
    "gemma3:270m",
    "deepseek-r1:14b",
    "deepseek-r1:8b",
    "deepseek-r1:7b",
    "llama3.2:3b",
    "llama3.2:1b",
    "gpt-oss:20b",
    "gemma3:1b",
    "deepseek-r1:1.5b",
    "qwen3:0.6b",
]

DEFAULT_VLLM_MODEL = "openai/gpt-oss-120b"
DEFAULT_VLLM_ENDPOINT = "http://115.145.135.227:7220/v1/chat/completions"
OPENAI_MODELS = [
    "gpt-5",
    "gpt-5-mini",
    "gpt-5-nano",
    "gpt-4.1-mini",
]
ANTHROPIC_MODELS = [
    "claude-haiku-4-5",
    "claude-sonnet-4-5",
]
GEMINI_MODELS = [
    "gemini-2.5-pro",
    "gemini-2.5-flash",
]

DEFAULT_OPENAI_MODEL = OPENAI_MODELS[1]  # gpt-5-mini
DEFAULT_ANTHROPIC_MODEL = ANTHROPIC_MODELS[0]  # claude-haiku-4-5
DEFAULT_ANTHROPIC_ENDPOINT = "https://api.anthropic.com/v1/messages"
DEFAULT_LLM_MAX_TOKENS = 2048
DEFAULT_GEMINI_MODEL = LLM_DEFAULT_GEMINI_MODEL

CONCURRENCY_ALLOWED_MODELS = {
    'openai': {
        "gpt-5-mini",
        "gpt-4.1-mini",
    },
    'anthropic': {
        "claude-haiku-4-5",
        "claude-3-5-haiku",
    },
    'gemini': {
        "gemini-2.5-flash",
        "gemini-2.0-flash",
    },
}

AUTO_PROVIDER_MAX_TOKENS = {
    'anthropic': 8192,
    'gemini': 8192,
}


def select_default_models(provider: str, *, quick: bool = False) -> List[str]:
    """Return provider-aware default model list."""
    provider = (provider or "ollama").lower()
    if provider == 'ollama':
        return [DEFAULT_MODELS[0]] if quick else list(DEFAULT_MODELS)
    if provider == 'vllm':
        return [DEFAULT_VLLM_MODEL]
    if provider == 'openai':
        return [DEFAULT_OPENAI_MODEL] if quick else list(OPENAI_MODELS)
    if provider == 'anthropic':
        return [DEFAULT_ANTHROPIC_MODEL] if quick else list(ANTHROPIC_MODELS)
    if provider == 'gemini':
        return [DEFAULT_GEMINI_MODEL] if quick else list(GEMINI_MODELS)
    return [DEFAULT_MODELS[0]] if quick else list(DEFAULT_MODELS)


def parse_prompt_components_arg(raw: str | None) -> PromptOptions:
    """Return PromptOptions parsed from CLI input."""
    if not raw or raw.strip().lower() == "all":
        return PromptOptions()
    if raw.strip().lower() == "none":
        return PromptOptions(False, False, False, False)
    tokens = {token.strip().lower() for token in raw.split(",") if token.strip()}
    valid_tokens = {
        "interventions",
        "natural",
        "guidelines",
        "provider_hint",
        "provider",
    }
    invalid = tokens - valid_tokens
    if invalid:
        raise ValueError(f"Unknown prompt component(s): {', '.join(sorted(invalid))}")
    return PromptOptions(
        include_interventions="interventions" in tokens,
        include_natural_context="natural" in tokens,
        include_guidelines="guidelines" in tokens,
        include_provider_hint=bool({"provider_hint", "provider"} & tokens),
    )


def print_llm_settings(llm_config: Dict[str, object]) -> None:
    """Pretty-print LLM runtime settings."""
    provider = llm_config.get('provider', 'ollama')
    endpoint = llm_config.get('endpoint') or 'N/A'
    timeout = llm_config.get('timeout')
    max_tokens = llm_config.get('max_tokens')
    concurrency = llm_config.get('concurrency')
    provider_key = (provider or '').lower()
    model_whitelist = CONCURRENCY_ALLOWED_MODELS.get(provider_key, set())

    print(f"  LLM provider: {provider}")
    print(f"  LLM endpoint: {endpoint}")
    if timeout:
        print(f"  LLM timeout: {timeout}s")
    if max_tokens and provider in {'anthropic', 'gemini'}:
        print(f"  LLM max_tokens: {max_tokens}")
    if concurrency and model_whitelist:
        enabled = ', '.join(sorted(model_whitelist))
        print(f"  LLM concurrency: {concurrency} (enabled for {enabled})")
    if provider_key == 'openai':
        print(f"  Available OpenAI models: {', '.join(OPENAI_MODELS)}")
    elif provider_key == 'anthropic':
        print(f"  Available Claude models: {', '.join(ANTHROPIC_MODELS)}")
    elif provider_key == 'gemini':
        print(f"  Available Gemini models: {', '.join(GEMINI_MODELS)}")


# ============================================================================
# Incomplete Patch Generation for RQ2
# ============================================================================

@dataclass
class IncompletePatch:
    """Represents an intentionally incomplete patch"""
    patch_id: str
    case_id: str
    patched_code: str
    incompleteness_type: str
    description: str
    why_incomplete: str
    should_be_caught_by: List[str]  # List of verification methods that should catch this


class IncompletePatchGenerator:
    """Generates incomplete patches for testing verification methods"""

    def __init__(self, case: Dict):
        self.case = case
        self.case_id = case['id']
        self.source = case['source']
        self.vuln_line = case['vuln_line']
        self.cwe_id = case.get('cwe_id', '')
        self.signature = case.get('signature', '')

    def generate_incomplete_patches(self) -> List[IncompletePatch]:
        """Generate 2-3 incomplete patches based on vulnerability type"""
        patches = []

        # Strategy 1: Specific input check only (misses edge cases)
        patch1 = self._create_specific_input_check()
        if patch1:
            patches.append(patch1)

        # Strategy 2: Partial condition check (misses negatives or other paths)
        patch2 = self._create_partial_condition_check()
        if patch2:
            patches.append(patch2)

        # Strategy 3: Wrong location or incomplete guard
        patch3 = self._create_wrong_location_patch()
        if patch3:
            patches.append(patch3)

        return patches

    def _create_specific_input_check(self) -> Optional[IncompletePatch]:
        """
        Create patch that checks for specific exploit pattern only
        Example: if (len == 256) instead of if (len >= 256)
        """
        lines = self.source.splitlines()
        vuln_idx = self.vuln_line - 1

        if vuln_idx < 0 or vuln_idx >= len(lines):
            return None

        vuln_line_text = lines[vuln_idx]
        indent = self._get_indent(vuln_line_text)

        # Identify vulnerability type and add overly specific check
        if 'strcpy' in vuln_line_text or 'strcat' in vuln_line_text:
            # For buffer overflow: check exact length instead of >=
            guard = f"{indent}if (strlen(input) == 256) return -1;  // Incomplete: only checks exact 256\n"
            incomplete_type = "specific_value_check"
            why = "Checks for equality (==) instead of >= or >, misses other overflow values"
            caught_by = ["V3", "V4"]  # Consistency and triple verification

        elif 'printf' in vuln_line_text and '%' in self.signature:
            # For format string: only check for specific format specifier
            guard = f"{indent}if (strstr(input, \"%s\")) return -1;  // Incomplete: only checks %s\n"
            incomplete_type = "specific_pattern_check"
            why = "Only checks for %s format specifier, misses %n, %x, and other dangerous patterns"
            caught_by = ["V3", "V4"]

        elif 'malloc' in vuln_line_text or 'calloc' in vuln_line_text:
            # For integer overflow: only check positive values
            guard = f"{indent}if (size > INT_MAX) return NULL;  // Incomplete: misses negative overflow\n"
            incomplete_type = "positive_only_check"
            why = "Only checks positive overflow, misses negative values and wraparound"
            caught_by = ["V2", "V3", "V4"]  # Symbolic and consistency

        elif 'scanf' in vuln_line_text or 'gets' in vuln_line_text:
            # For unbounded read: limit to specific size but buffer is smaller
            guard = f"{indent}char limited[128];  // Incomplete: buffer still too small\n"
            incomplete_type = "insufficient_size_limit"
            why = "Adds size limit but the limit is still larger than the buffer size"
            caught_by = ["V2", "V3", "V4"]

        else:
            # Generic: add a check that's always true
            guard = f"{indent}if (1) {{  // Incomplete: tautology, doesn't prevent vulnerability\n"
            guard += f"{indent}    {vuln_line_text.strip()}\n"
            guard += f"{indent}}}\n"
            incomplete_type = "tautology_check"
            why = "Guard condition is always true, provides no actual protection"
            caught_by = ["V2", "V3", "V4"]
            lines[vuln_idx] = ""  # Remove original line

        # Insert guard before vulnerable line
        if 'tautology' not in incomplete_type:
            lines.insert(vuln_idx, guard)

        patched_code = '\n'.join(lines)

        return IncompletePatch(
            patch_id=f"{self.case_id}_incomplete_1",
            case_id=self.case_id,
            patched_code=patched_code,
            incompleteness_type=incomplete_type,
            description="Patch checks for specific exploit input only, misses edge cases",
            why_incomplete=why,
            should_be_caught_by=caught_by
        )

    def _create_partial_condition_check(self) -> Optional[IncompletePatch]:
        """
        Create patch that addresses one path but misses others
        Example: checks input but not when input comes from alternative source
        """
        lines = self.source.splitlines()
        vuln_idx = self.vuln_line - 1

        if vuln_idx < 0 or vuln_idx >= len(lines):
            return None

        vuln_line_text = lines[vuln_idx]
        indent = self._get_indent(vuln_line_text)

        # Add guard that only covers one branch
        if 'strcpy' in vuln_line_text or 'memcpy' in vuln_line_text:
            # Only check if input is from specific source
            guard = f"{indent}// Incomplete: only checks direct input, not processed input\n"
            guard += f"{indent}if (input != NULL && direct_input) {{\n"
            guard += f"{indent}    if (strlen(input) > sizeof(buf)) return -1;\n"
            guard += f"{indent}}}\n"
            incomplete_type = "single_path_check"
            why = "Only guards direct input path, misses processed/indirect input paths"
            caught_by = ["V3", "V4"]

        elif 'malloc' in vuln_line_text:
            # Only check one variable in multiplication
            guard = f"{indent}if (n > 1000) return NULL;  // Incomplete: doesn't check multiplier m\n"
            incomplete_type = "partial_variable_check"
            why = "Checks only one variable in size calculation (n), ignores multiplier (m)"
            caught_by = ["V2", "V3", "V4"]

        else:
            # Generic: add null check but not bounds check
            guard = f"{indent}if (input == NULL) return -1;  // Incomplete: null check only\n"
            incomplete_type = "insufficient_validation"
            why = "Only validates null pointer, doesn't check bounds or other conditions"
            caught_by = ["V3", "V4"]

        lines.insert(vuln_idx, guard)
        patched_code = '\n'.join(lines)

        return IncompletePatch(
            patch_id=f"{self.case_id}_incomplete_2",
            case_id=self.case_id,
            patched_code=patched_code,
            incompleteness_type=incomplete_type,
            description="Patch addresses one causal path but misses others",
            why_incomplete=why,
            should_be_caught_by=caught_by
        )

    def _create_wrong_location_patch(self) -> Optional[IncompletePatch]:
        """
        Create patch at wrong location or with wrong scope
        Example: check after the vulnerable operation instead of before
        """
        lines = self.source.splitlines()
        vuln_idx = self.vuln_line - 1

        if vuln_idx < 0 or vuln_idx >= len(lines):
            return None

        vuln_line_text = lines[vuln_idx]
        indent = self._get_indent(vuln_line_text)

        # Add check AFTER the vulnerable operation (too late)
        post_check = f"{indent}// Incomplete: check is after vulnerable operation\n"
        post_check += f"{indent}if (error_occurred) {{  // Too late - damage already done\n"
        post_check += f"{indent}    return -1;\n"
        post_check += f"{indent}}}\n"

        # Insert after vulnerable line (wrong location)
        lines.insert(vuln_idx + 1, post_check)
        patched_code = '\n'.join(lines)

        return IncompletePatch(
            patch_id=f"{self.case_id}_incomplete_3",
            case_id=self.case_id,
            patched_code=patched_code,
            incompleteness_type="wrong_location",
            description="Patch placed after vulnerable operation instead of before",
            why_incomplete="Validation happens after the vulnerability is exploited, not before",
            should_be_caught_by=["V2", "V3", "V4"]  # Symbolic and consistency should catch
        )

    @staticmethod
    def _get_indent(line: str) -> str:
        """Extract leading whitespace from line"""
        return line[:len(line) - len(line.lstrip())]


# ============================================================================
# Main Experiment Functions
# ============================================================================


def print_header(title: str, width: int = 70):
    """í—¤ë” ì¶œë ¥"""
    print("\n" + "=" * width)
    print(f"  {title}")
    print("=" * width)


def calculate_case_allocation(server_id: int, num_servers: int, total_cases: int) -> Tuple[int, int]:
    """ê° ì„œë²„ì— í• ë‹¹í•  ì¼€ì´ìŠ¤ ë²”ìœ„ ê³„ì‚°"""
    cases_per_server = total_cases // num_servers
    remainder = total_cases % num_servers

    if server_id < remainder:
        start_index = server_id * (cases_per_server + 1)
        count = cases_per_server + 1
    else:
        start_index = remainder * (cases_per_server + 1) + (server_id - remainder) * cases_per_server
        count = cases_per_server

    return start_index, count


def load_cases(dataset: str, start_index: int = 0, count: int = None) -> List[Dict]:
    """ì¼€ì´ìŠ¤ ë¡œë“œ"""
    from patchscribe.dataset import load_cases as load_dataset_cases

    all_cases = load_dataset_cases(dataset)

    if count is None:
        return all_cases[start_index:]
    else:
        return all_cases[start_index:start_index + count]


def get_condition_settings(condition: str) -> Tuple[str, bool]:
    """ì¡°ê±´ì— ë§ëŠ” ì„¤ì • ë°˜í™˜"""
    settings = {
        'c1': ('only_natural', False),  # Baseline: Post-hoc natural language
        'c2': ('natural', False),        # Vague hints
        'c3': ('formal', False),         # Pre-hoc formal (no verification)
        'c4': ('formal', True),          # Full PatchScribe (with verification)
    }
    return settings.get(condition, ('formal', True))


def _supports_parallel_conditions(model_spec: str, llm_config: Dict[str, object]) -> bool:
    """íŠ¹ì • ëª¨ë¸ì´ ì¡°ê±´ ë³‘ë ¬ ì‹¤í–‰ì„ ì•ˆì „í•˜ê²Œ ì§€ì›í•˜ëŠ”ì§€ ì—¬ë¶€."""
    provider = (llm_config.get('provider') or 'ollama').lower()
    if provider in {'ollama', 'vllm'}:
        return False
    concurrency = llm_config.get('concurrency')
    if not concurrency or concurrency <= 1:
        return False
    model_name = model_spec.split(':', 1)[1] if model_spec.startswith('ollama:') else model_spec
    model_basename = model_name.split('/')[-1] if model_name else model_name
    allowed = CONCURRENCY_ALLOWED_MODELS.get(provider, set())
    return model_basename in allowed


def _model_output_dir(output_dir: Path, model_name: str, run_label: Optional[str]) -> Path:
    base_dir = output_dir / model_name
    if run_label:
        return base_dir / run_label
    return base_dir


def run_single_evaluation(
    cases: List[Dict],
    model_spec: str,
    condition: str,
    output_file: Path,
    *,
    llm_config: Dict[str, object],
    disable_consistency_check: bool = False,
    verbose: bool = True,
    stage1_cache_dir: Optional[Path] = None,
    force_stage1_recompute: bool = False,
    skip_judge_evaluation: bool = False,
    use_majority_voting: bool = False,
    judge_models: List[str] = None,
) -> Dict:
    """ë‹¨ì¼ ëª¨ë¸ Ã— ì¡°ê±´ì— ëŒ€í•œ í‰ê°€ ì‹¤í–‰"""
    from patchscribe.pipeline import PatchScribePipeline
    from patchscribe.evaluation import Evaluator

    # ëª¨ë¸ ìŠ¤í™ íŒŒì‹±
    # í˜•ì‹: "qwen3:14b" ë˜ëŠ” "ollama:qwen3:14b"
    # ê¸°ë³¸ providerëŠ” í•­ìƒ ollama
    if model_spec.startswith('ollama:'):
        # "ollama:qwen3:14b" -> "qwen3:14b"
        model_name = model_spec[7:]  # "ollama:" ì œê±°
    else:
        # "qwen3:14b" -> "qwen3:14b"
        model_name = model_spec

    provider = (llm_config.get('provider') or 'ollama').lower()
    endpoint = llm_config.get('endpoint')
    timeout = llm_config.get('timeout')
    max_tokens = llm_config.get('max_tokens')
    concurrency = llm_config.get('concurrency')
    prompt_options = llm_config.get('prompt_options')
    model_basename = model_name.split('/')[-1] if model_name else model_name
    concurrency_allowed = (
        provider == 'openai' and model_basename in CONCURRENCY_ALLOWED_MODELS.get('openai', set())
    ) or (
        provider == 'anthropic' and model_basename in CONCURRENCY_ALLOWED_MODELS.get('anthropic', set())
    ) or (
        provider == 'gemini' and model_basename in CONCURRENCY_ALLOWED_MODELS.get('gemini', set())
    )

    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (provider ë³„ë¡œ ë™ì‘)
    if provider == 'gemini':
        endpoint = DEFAULT_GEMINI_ENDPOINT_TEMPLATE.format(model=model_name)

    os.environ['PATCHSCRIBE_LLM_PROVIDER'] = provider
    os.environ['PATCHSCRIBE_LLM_MODEL'] = model_name

    if endpoint:
        os.environ['PATCHSCRIBE_LLM_ENDPOINT'] = str(endpoint)
    elif 'PATCHSCRIBE_LLM_ENDPOINT' in os.environ:
        del os.environ['PATCHSCRIBE_LLM_ENDPOINT']

    if timeout is not None:
        os.environ['PATCHSCRIBE_LLM_TIMEOUT'] = str(timeout)
    elif 'PATCHSCRIBE_LLM_TIMEOUT' in os.environ:
        del os.environ['PATCHSCRIBE_LLM_TIMEOUT']

    if max_tokens is not None:
        os.environ['PATCHSCRIBE_LLM_MAX_TOKENS'] = str(max_tokens)
    elif 'PATCHSCRIBE_LLM_MAX_TOKENS' in os.environ:
        del os.environ['PATCHSCRIBE_LLM_MAX_TOKENS']

    # concurrencyëŠ” ì§€ì›ë˜ëŠ” ëª¨ë¸ì—ì„œë§Œ ì‚¬ìš©
    evaluator_kwargs: Dict[str, object] = {}
    if provider in {'ollama', 'vllm'}:
        evaluator_kwargs['max_workers'] = 1
        if concurrency and verbose:
            print("    âš ï¸ LLM concurrency is not supported for this provider; running sequentially.")
    elif concurrency and concurrency_allowed:
        evaluator_kwargs['max_workers'] = max(1, int(concurrency))
    else:
        evaluator_kwargs['max_workers'] = 1
        if concurrency and verbose and not concurrency_allowed:
            allowed = ', '.join(sorted(CONCURRENCY_ALLOWED_MODELS.get(provider, set())))
            if allowed:
                print(f"    âš ï¸ LLM concurrency ignored (supported models: {allowed}).")
            else:
                print("    âš ï¸ LLM concurrency ignored (no supported models for this provider).")

    # ì¡°ê±´ë³„ ì„¤ì •
    strategy, condition_consistency = get_condition_settings(condition)

    # Consistency check: ì¡°ê±´ ì„¤ì •ê³¼ CLI ì˜µì…˜ ëª¨ë‘ ê³ ë ¤
    final_consistency_check = condition_consistency and not disable_consistency_check

    if verbose:
        print(f"\n>>> Running: {model_name} - Condition {condition}")
        print(f"    Cases: {len(cases)}")
        print(f"    Strategy: {strategy}")
        print(f"    Consistency check: {final_consistency_check}")

    # íŒŒì´í”„ë¼ì¸ ì„¤ì •
    pipeline = PatchScribePipeline(
        strategy=strategy,
        explain_mode='both',
        enable_consistency_check=final_consistency_check,
        enable_performance_profiling=True,
        stage1_cache_dir=stage1_cache_dir,
        force_stage1_recompute=force_stage1_recompute,
        prompt_options=prompt_options,
    )

    # í‰ê°€ ì‹¤í–‰
    evaluator = Evaluator(
        pipeline=pipeline,
        max_workers=evaluator_kwargs.get('max_workers'),
        enable_judge=not skip_judge_evaluation,  # Skip judge if flag is set
        use_majority_voting=use_majority_voting,
        judge_models=judge_models or ['gpt', 'claude', 'gemini'],
    )
    report = evaluator.run(cases)

    # ê²°ê³¼ ì €ì¥
    output_file.parent.mkdir(parents=True, exist_ok=True)
    with open(output_file, 'w') as f:
        json.dump(report.as_dict(), f, indent=2)

    success_rate = report.metrics.get('success_rate', 0)
    if verbose:
        print(f"    âœ… Success rate: {success_rate:.1%}")

    return report.as_dict()


def generate_incomplete_patches(cases: List[Dict], output_file: Path, verbose: bool = True) -> Dict:
    """ë¶ˆì™„ì „ íŒ¨ì¹˜ ìƒì„± (RQ2)"""
    if verbose:
        print(f"\nGenerating incomplete patches for {len(cases)} cases...")

    all_patches = {}
    for i, case in enumerate(cases, 1):
        case_id = case['id']
        if verbose and i % 5 == 0:
            print(f"  Progress: {i}/{len(cases)}")

        try:
            generator = IncompletePatchGenerator(case)
            patches = generator.generate_incomplete_patches()

            all_patches[case_id] = [
                {
                    'patch_id': p.patch_id,
                    'case_id': p.case_id,
                    'patched_code': p.patched_code,
                    'incompleteness_type': p.incompleteness_type,
                    'description': p.description,
                    'why_incomplete': p.why_incomplete,
                    'should_be_caught_by': p.should_be_caught_by
                }
                for p in patches
            ]
        except Exception as e:
            if verbose:
                print(f"  âš ï¸ Failed for case {case_id}: {e}")
            continue

    # ì €ì¥
    output_file.parent.mkdir(parents=True, exist_ok=True)
    with open(output_file, 'w') as f:
        json.dump(all_patches, f, indent=2)

    total_patches = sum(len(p) for p in all_patches.values())
    if verbose:
        print(f"  âœ… Generated {total_patches} patches for {len(all_patches)} cases")

    return all_patches


def run_experiment(
    dataset: str,
    models: List[str],
    conditions: List[str],
    output_dir: Path,
    llm_config: Dict[str, object],
    start_index: int = 0,
    limit: int = None,
    generate_incomplete: bool = True,
    server_id: int = None,
    verbose: bool = True,
    parallel_conditions: bool = False,
    disable_consistency_check: bool = False,
    stage1_cache_dir: Optional[Path] = None,
    force_stage1_recompute: bool = False,
    precompute_stage1_only: bool = False,
    skip_judge_evaluation: bool = False,
    use_majority_voting: bool = False,
    judge_models: List[str] = None,
):
    """í†µí•© ì‹¤í—˜ ì‹¤í–‰

    Args:
        parallel_conditions: Trueë©´ ëª¨ë“  (ëª¨ë¸, condition) ì¡°í•©ì„ ë³‘ë ¬ ì²˜ë¦¬
        disable_consistency_check: E_bug/E_patch ì¼ê´€ì„± ì²´í¬ ë¹„í™œì„±í™”
        stage1_cache_dir: Stage-1 ìºì‹œ ê²½ë¡œ (Noneì´ë©´ ë¹„í™œì„±)
        force_stage1_recompute: ìºì‹œì— ìˆë”ë¼ë„ Stage-1 ì¬ê³„ì‚° ì—¬ë¶€
        precompute_stage1_only: Trueë©´ ìºì‹œë§Œ ì±„ìš°ê³  LLM í˜¸ì¶œ ì—†ì´ ì¢…ë£Œ
    """

    # ì¼€ì´ìŠ¤ ë¡œë“œ
    if verbose:
        print(f"\nLoading cases from dataset: {dataset}")

    cases = load_cases(dataset, start_index, limit)

    if verbose:
        print(f"  Loaded {len(cases)} cases")
        if start_index > 0:
            print(f"  Range: {start_index} to {start_index + len(cases) - 1}")

    if precompute_stage1_only:
        if not stage1_cache_dir:
            raise ValueError("Stage-1 cache directory must be provided for precompute mode.")
        print_header("Stage-1 Precompute Mode")
        _precompute_stage1_batch(
            cases,
            stage1_cache_dir,
            force_stage1_recompute=force_stage1_recompute,
            verbose=verbose,
        )
        print("\nâœ… Stage-1 artifacts cached for all assigned cases.")
        return []

    run_label = datetime.now().strftime("%Y%m%d-%H%M%S")
    if verbose:
        print(f"  Run identifier: {run_label}")
        print("  Each model's outputs will be stored in model/<timestamp>/ directories.")

    # ì¼€ì´ìŠ¤ ì €ì¥ (ë¶„ì‚° ì‹¤í—˜ìš©)
    if server_id is not None:
        cases_file = output_dir / "assigned_cases.json"
        with open(cases_file, 'w') as f:
            json.dump(cases, f, indent=2)
        if verbose:
            print(f"  Saved assigned cases to: {cases_file}")

    print_header("Running Experiments: All Models Ã— All Conditions")

    # ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“œ ì„ íƒ
    if parallel_conditions:
        results_summary = _run_experiment_parallel(
            cases, models, conditions, output_dir, llm_config,
            server_id, verbose, run_label, disable_consistency_check,
            stage1_cache_dir, force_stage1_recompute,
            skip_judge_evaluation, use_majority_voting, judge_models
        )
    else:
        results_summary = _run_experiment_sequential(
            cases, models, conditions, output_dir, llm_config,
            server_id, verbose, run_label, disable_consistency_check,
            stage1_cache_dir, force_stage1_recompute,
            skip_judge_evaluation, use_majority_voting, judge_models
        )

    # ë¶ˆì™„ì „ íŒ¨ì¹˜ ìƒì„± (RQ2)
    if generate_incomplete:
        print_header("Generating Incomplete Patches (RQ2)")

        try:
            if server_id is not None:
                incomplete_file = output_dir / f"incomplete_patches_server{server_id}.json"
            else:
                incomplete_file = output_dir / "incomplete_patches.json"

            generate_incomplete_patches(cases, incomplete_file, verbose)

        except Exception as e:
            if verbose:
                print(f"âŒ Failed to generate incomplete patches: {e}")
                import traceback
                traceback.print_exc()

    # ì‹¤í—˜ ì™„ë£Œ ìš”ì•½
    print_header("Experiment Summary")

    print("\nResults by model:")
    for model_result in results_summary:
        print(f"\n  {model_result['model']}:")
        for condition, info in model_result['conditions'].items():
            if 'error' in info:
                print(f"    {condition}: âŒ {info['error'][:50]}...")
            else:
                print(f"    {condition}: {info['success_rate']:.1%} success")

    print(f"\nğŸ“ Results saved to: {output_dir}/")

    # ìƒì„±ëœ ì£¼ìš” íŒŒì¼ ëª©ë¡
    print("\nGenerated files:")
    for model_spec in models:
        model_name = model_spec.split(':', 1)[1] if ':' in model_spec else model_spec
        model_dir = _model_output_dir(output_dir, model_name, run_label)
        if not model_dir.exists():
            model_dir = output_dir / model_name
        if model_dir.exists():
            json_files = list(model_dir.glob("*.json"))
            if json_files:
                try:
                    relative_path = model_dir.relative_to(output_dir)
                except ValueError:
                    relative_path = Path(model_name)
                print(f"  {relative_path}/: {len(json_files)} files")

    # Next steps
    print_header("Next Steps")

    if server_id is not None:
        print("\në¶„ì‚° ì‹¤í—˜ - ë‹¤ìŒ ë‹¨ê³„:")
        print("1. ëª¨ë“  ì„œë²„ì—ì„œ ì‹¤í—˜ì´ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°")
        print("2. ì¤‘ì•™ ì„œë²„ì—ì„œ ê²°ê³¼ ìˆ˜ì§‘:")
        print("   scp -r user@server0:~/patchscribe/results/server0 results/")


def _run_experiment_sequential(
    cases: List[Dict],
    models: List[str],
    conditions: List[str],
    output_dir: Path,
    llm_config: Dict[str, object],
    server_id: int,
    verbose: bool,
    run_label: Optional[str] = None,
    disable_consistency_check: bool = False,
    stage1_cache_dir: Optional[Path] = None,
    force_stage1_recompute: bool = False,
    skip_judge_evaluation: bool = False,
    use_majority_voting: bool = False,
    judge_models: List[str] = None,
) -> List[Dict]:
    """ìˆœì°¨ ì²˜ë¦¬ ëª¨ë“œ (ê¸°ì¡´ ë™ì‘)"""
    results_summary = []

    for model_spec in models:
        model_name = model_spec.split(':', 1)[1] if ':' in model_spec else model_spec

        print(f"\n{'#' * 70}")
        print(f"  MODEL: {model_name}")
        print(f"{'#' * 70}")

        # ëª¨ë¸ë³„ ê²°ê³¼ ë””ë ‰í† ë¦¬
        model_output_dir = _model_output_dir(output_dir, model_name, run_label)
        model_output_dir.mkdir(parents=True, exist_ok=True)

        model_results = {
            'model': model_name,
            'conditions': {}
        }

        # ëª¨ë“  ì¡°ê±´ ì‹¤í–‰
        for condition in conditions:
            # ê²°ê³¼ íŒŒì¼ëª…
            if server_id is not None:
                result_filename = f"{condition}_server{server_id}_results.json"
            else:
                result_filename = f"{condition}_results.json"

            output_file = model_output_dir / result_filename

            try:
                result = run_single_evaluation(
                    cases,
                    model_spec,
                    condition,
                    output_file,
                    llm_config=llm_config,
                    disable_consistency_check=disable_consistency_check,
                    verbose=verbose,
                    stage1_cache_dir=stage1_cache_dir,
                    force_stage1_recompute=force_stage1_recompute,
                    skip_judge_evaluation=skip_judge_evaluation,
                    use_majority_voting=use_majority_voting,
                    judge_models=judge_models,
                )
                model_results['conditions'][condition] = {
                    'success_rate': result['metrics'].get('success_rate', 0),
                    'output_file': str(output_file)
                }

                if verbose:
                    print(f"    âœ… Condition {condition} completed")

            except KeyboardInterrupt:
                print("\n\nâš ï¸  Interrupted by user")
                sys.exit(130)
            except Exception as e:
                if verbose:
                    print(f"    âŒ Failed: {e}")
                    import traceback
                    traceback.print_exc()
                model_results['conditions'][condition] = {
                    'success_rate': 0,
                    'error': str(e)
                }
                continue

        results_summary.append(model_results)

        if verbose:
            print(f"\nâœ… Model {model_name} completed")

    return results_summary


def _run_experiment_parallel(
    cases: List[Dict],
    models: List[str],
    conditions: List[str],
    output_dir: Path,
    llm_config: Dict[str, object],
    server_id: int,
    verbose: bool,
    run_label: Optional[str] = None,
    disable_consistency_check: bool = False,
    stage1_cache_dir: Optional[Path] = None,
    force_stage1_recompute: bool = False,
    skip_judge_evaluation: bool = False,
    use_majority_voting: bool = False,
    judge_models: List[str] = None,
) -> List[Dict]:
    """ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“œ: ëª¨ë“  (ëª¨ë¸, condition) ì¡°í•©ì„ ë™ì‹œì— ì²˜ë¦¬"""
    from concurrent.futures import ThreadPoolExecutor, as_completed
    import threading

    parallel_tasks = []
    sequential_tasks = []
    sequential_models = set()

    for model_spec in models:
        model_name = model_spec.split(':', 1)[1] if ':' in model_spec else model_spec
        model_output_dir = _model_output_dir(output_dir, model_name, run_label)
        model_output_dir.mkdir(parents=True, exist_ok=True)
        supports_parallel = _supports_parallel_conditions(model_spec, llm_config)
        if not supports_parallel:
            sequential_models.add(model_name)

        for condition in conditions:
            if server_id is not None:
                result_filename = f"{condition}_server{server_id}_results.json"
            else:
                result_filename = f"{condition}_results.json"

            output_file = model_output_dir / result_filename
            task = (model_spec, model_name, condition, output_file)
            if supports_parallel:
                parallel_tasks.append(task)
            else:
                sequential_tasks.append(task)

    total_tasks = len(parallel_tasks) + len(sequential_tasks)
    if verbose:
        print(f"\nğŸš€ Starting {total_tasks} tasks (parallel where safe)...")
        print(f"   Models: {len(models)}")
        print(f"   Conditions: {len(conditions)}")
        print(f"   Parallel combinations: {len(parallel_tasks)}")
        if sequential_models:
            print(f"   Sequential only: {', '.join(sorted(sequential_models))}")

    # ì¶œë ¥ìš© Lock
    print_lock = threading.Lock()

    def run_task(task_info):
        """ë‹¨ì¼ (ëª¨ë¸, condition) ì‹¤í–‰"""
        model_spec, model_name, condition, output_file = task_info

        try:
            with print_lock:
                print(f"  â–¶ Starting: {model_name} - {condition}")

            result = run_single_evaluation(
                cases,
                model_spec,
                condition,
                output_file,
                llm_config=llm_config,
                disable_consistency_check=disable_consistency_check,
                verbose=False,  # ë³‘ë ¬ ì‹¤í–‰ì‹œ ê°œë³„ verbose ë”
                stage1_cache_dir=stage1_cache_dir,
                force_stage1_recompute=force_stage1_recompute,
                skip_judge_evaluation=skip_judge_evaluation,
                use_majority_voting=use_majority_voting,
                judge_models=judge_models,
            )

            with print_lock:
                success_rate = result['metrics'].get('success_rate', 0)
                print(f"  âœ… Completed: {model_name} - {condition} ({success_rate:.1%})")

            return (model_name, condition, {
                'success_rate': success_rate,
                'output_file': str(output_file)
            }, None)

        except KeyboardInterrupt:
            raise
        except Exception as e:
            with print_lock:
                print(f"  âŒ Failed: {model_name} - {condition}: {str(e)[:50]}")

            return (model_name, condition, {
                'success_rate': 0,
                'error': str(e)
            }, e)

    results_dict = {}

    def record_result(model_name: str, condition: str, result_info: Dict[str, object]) -> None:
        if model_name not in results_dict:
            results_dict[model_name] = {'model': model_name, 'conditions': {}}
        results_dict[model_name]['conditions'][condition] = result_info

    # sequential runs (no safe shared parallelism)
    if sequential_tasks:
        if verbose:
            print("\nâš ï¸  Executing sequentially for non-concurrent models...")
        for task in sequential_tasks:
            model_name, condition, result_info, error = run_task(task)
            record_result(model_name, condition, result_info)

    # ë³‘ë ¬ ì‹¤í–‰ (ì•ˆì „í•œ ëª¨ë¸ë§Œ)
    if parallel_tasks:
        try:
            with ThreadPoolExecutor(max_workers=len(parallel_tasks)) as executor:
                futures = {executor.submit(run_task, task): task for task in parallel_tasks}

                for future in as_completed(futures):
                    try:
                        model_name, condition, result_info, error = future.result()
                        record_result(model_name, condition, result_info)

                    except KeyboardInterrupt:
                        print("\n\nâš ï¸  Interrupted by user")
                        executor.shutdown(wait=False, cancel_futures=True)
                        sys.exit(130)
                    except Exception as e:
                        if verbose:
                            print(f"  âŒ Unexpected error: {e}")
                            import traceback
                            traceback.print_exc()

        except KeyboardInterrupt:
            print("\n\nâš ï¸  Interrupted by user")
            sys.exit(130)

    # ê²°ê³¼ë¥¼ ëª¨ë¸ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    results_summary = [results_dict[model_spec.split(':', 1)[1] if ':' in model_spec else model_spec]
                       for model_spec in models if (model_spec.split(':', 1)[1] if ':' in model_spec else model_spec) in results_dict]

    return results_summary


def main():
    parser = argparse.ArgumentParser(
        description='PatchScribe í†µí•© ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‹¤í–‰ ëª¨ë“œ:

1. ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (3ê°œ ì¼€ì´ìŠ¤):
   python3 scripts/run_experiment.py --quick

2. ë¡œì»¬ ì‹¤í—˜ (ì „ì²´):
   python3 scripts/run_experiment.py --dataset zeroday --limit 10

3. ë¶„ì‚° ì‹¤í—˜ (4ëŒ€ ì„œë²„):
   # Server 0:
   python3 scripts/run_experiment.py --distributed 0 4 20 --dataset zeroday

   # Server 1:
   python3 scripts/run_experiment.py --distributed 1 4 20 --dataset zeroday

4. íŠ¹ì • ëª¨ë¸ë§Œ:
   python3 scripts/run_experiment.py --dataset zeroday --limit 10 \
       --models qwen3:14b gemma3:12b

5. íŠ¹ì • ëª¨ë¸ê³¼ ì¡°ê±´ë§Œ:
   python3 scripts/run_experiment.py --dataset zeroday --limit 10 \
       --models llama3.2:1b \
       --conditions c4

ëª¨ë¸ ì´ë¦„ í˜•ì‹:
  - ê¸°ë³¸ í˜•ì‹: qwen3:14b, gemma3:12b, deepseek-r1:7b
  - provider(ollama)ëŠ” ìë™ ì„¤ì •ë¨

ì „ì²´ ì‹¤í—˜ ëŒ€ìƒ ëª¨ë¸ (16ê°œ):
  qwen3:14b, qwen3:8b, qwen3:4b, qwen3:1.7b,
  gemma3:12b, gemma3:4b, gemma3:270m, gemma3:1b,
  deepseek-r1:14b, deepseek-r1:8b, deepseek-r1:7b, deepseek-r1:1.5b,
  llama3.2:3b, llama3.2:1b, gpt-oss:20b, qwen3:0.6b

ê¸°ë³¸ê°’:
  - Models: ìœ„ 16ê°œ ëª¨ë¸ ì „ì²´
  - Conditions: c1, c2, c3, c4
  - Dataset: zeroday
        """
    )

    # Mode selection
    mode_group = parser.add_mutually_exclusive_group()
    mode_group.add_argument(
        '--quick',
        action='store_true',
        help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (3ê°œ ì¼€ì´ìŠ¤, C4ë§Œ)'
    )
    mode_group.add_argument(
        '--distributed',
        nargs=3,
        metavar=('SERVER_ID', 'NUM_SERVERS', 'TOTAL_CASES'),
        help='ë¶„ì‚° ì‹¤í—˜ ëª¨ë“œ (ì˜ˆ: --distributed 0 4 20)'
    )

    # Data selection
    parser.add_argument(
        '--dataset',
        default='zeroday',
        choices=['zeroday', 'extractfix', 'vulnfix'],
        help='ë°ì´í„°ì…‹ (ê¸°ë³¸ê°’: zeroday)'
    )
    parser.add_argument(
        '--limit',
        type=int,
        help='ì²˜ë¦¬í•  ì¼€ì´ìŠ¤ ìˆ˜'
    )
    parser.add_argument(
        '--offset',
        type=int,
        default=0,
        help='ì‹œì‘ ì¼€ì´ìŠ¤ ì˜¤í”„ì…‹ (ê¸°ë³¸ê°’: 0)'
    )

    # Experiment configuration
    parser.add_argument(
        '--models',
        nargs='+',
        help=f'ì‹¤í—˜í•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: {", ".join(DEFAULT_MODELS)})'
    )
    parser.add_argument(
        '--conditions',
        nargs='+',
        choices=['c1', 'c2', 'c3', 'c4'],
        help='ì‹¤í—˜í•  ì¡°ê±´ (ê¸°ë³¸ê°’: c1 c2 c3 c4)'
    )
    parser.add_argument(
        '--skip-incomplete-patches',
        action='store_true',
        help='ë¶ˆì™„ì „ íŒ¨ì¹˜ ìƒì„± ê±´ë„ˆë›°ê¸°'
    )

    # Verification configuration
    parser.add_argument(
        '--disable-consistency-check',
        action='store_true',
        help='E_bug/E_patch ì¼ê´€ì„± ì²´í¬ ë¹„í™œì„±í™”. ê¸°ë³¸ê°’: í™œì„±í™”'
    )

    # Judge configuration
    parser.add_argument(
        '--skip-judge-evaluation',
        action='store_true',
        help='Skip all judge evaluation (success + explanation). Only generate patches and explanations. Useful for separating experiment from evaluation.'
    )
    parser.add_argument(
        '--use-majority-voting',
        action='store_true',
        help='Use 3 LLM judges (GPT, Claude, Gemini) with majority voting for success evaluation'
    )
    parser.add_argument(
        '--judge-models',
        nargs='+',
        choices=['gpt', 'claude', 'gemini'],
        default=['gpt', 'claude', 'gemini'],
        help='Judge models to use for majority voting (default: gpt claude gemini)'
    )

    # LLM configuration
    parser.add_argument(
        '--llm-provider',
        choices=['ollama', 'vllm', 'openai', 'anthropic', 'gemini'],
        default='ollama',
        help='íŒ¨ì¹˜ ìƒì„±ì„ ìœ„í•œ LLM ì œê³µì ì„ íƒ (ê¸°ë³¸ê°’: ollama)'
    )
    parser.add_argument(
        '--llm-endpoint',
        help='LLM ì—”ë“œí¬ì¸íŠ¸ URL (ë¯¸ì§€ì • ì‹œ ì œê³µìë³„ ê¸°ë³¸ê°’ ì‚¬ìš©)'
    )
    parser.add_argument(
        '--llm-timeout',
        type=int,
        help='LLM HTTP ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ ë‹¨ìœ„, ê¸°ë³¸ê°’: 300)'
    )
    parser.add_argument(
        '--llm-max-tokens',
        type=int,
        help='LLM max_tokens ì„¤ì • (Anthropic/Gemini í˜¸ì¶œ ì‹œ ê¶Œì¥, ê¸°ë³¸ê°’: 2048)'
    )
    parser.add_argument(
        '--llm-concurrency',
        type=int,
        help='LLM ë™ì‹œ ìš”ì²­ ìˆ˜ (OpenAI, Claude, Gemini ì§€ì› ëª¨ë¸ì—ë§Œ ì ìš©)'
    )
    parser.add_argument(
        '--prompt-components',
        default='all',
        help="ìœ ì§€í•  í”„ë¡¬í”„íŠ¸ êµ¬ì„±ìš”ì†Œ ì§€ì • (interventions,natural,guidelines,provider_hint|all|none)."
    )
    parser.add_argument(
        '--parallel-conditions',
        action='store_true',
        help='ëª¨ë“  (ëª¨ë¸, condition) ì¡°í•©ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬ (ê¸°ë³¸ê°’: ìˆœì°¨ ì²˜ë¦¬)'
    )
    parser.add_argument(
        '--stage1-cache-dir',
        default='results/cache/stage1',
        help='Stage-1 ìºì‹œ ë””ë ‰í† ë¦¬ (ë¹ˆ ë¬¸ìì—´ë¡œ ë¹„í™œì„±í™”)'
    )
    parser.add_argument(
        '--precompute-stage1',
        action='store_true',
        help='LLM í˜¸ì¶œ ì „ Stage-1 ìºì‹œë§Œ ìƒì„±í•˜ê³  ì¢…ë£Œ'
    )
    parser.add_argument(
        '--refresh-stage1-cache',
        action='store_true',
        help='ê¸°ì¡´ Stage-1 ìºì‹œë¥¼ ë¬´ì‹œí•˜ê³  ì¬ê³„ì‚°'
    )

    # Output
    parser.add_argument(
        '--output',
        type=Path,
        help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: results/)'
    )
    parser.add_argument(
        '-q', '--quiet',
        action='store_true',
        help='ìµœì†Œ ì¶œë ¥ ëª¨ë“œ'
    )

    args = parser.parse_args()

    stage1_cache_dir = args.stage1_cache_dir.strip() if args.stage1_cache_dir else ""
    if stage1_cache_dir.lower() in {"", "none", "null"}:
        normalized_stage1_cache = None
    else:
        normalized_stage1_cache = Path(stage1_cache_dir).expanduser()

    llm_config = {
        'provider': args.llm_provider,
        'endpoint': args.llm_endpoint,
        'timeout': args.llm_timeout,
        'max_tokens': args.llm_max_tokens if args.llm_max_tokens is not None else None,
        'concurrency': args.llm_concurrency if args.llm_concurrency is not None else None,
    }
    try:
        prompt_options = parse_prompt_components_arg(args.prompt_components)
    except ValueError as exc:
        parser.error(str(exc))

    llm_config['prompt_options'] = prompt_options

    if not llm_config['endpoint']:
        if args.llm_provider == 'ollama':
            llm_config['endpoint'] = DEFAULT_OLLAMA_ENDPOINT
        elif args.llm_provider == 'vllm':
            llm_config['endpoint'] = DEFAULT_VLLM_ENDPOINT
        elif args.llm_provider == 'openai':
            llm_config['endpoint'] = DEFAULT_OPENAI_ENDPOINT
        elif args.llm_provider == 'anthropic':
            llm_config['endpoint'] = DEFAULT_ANTHROPIC_ENDPOINT
        elif args.llm_provider == 'gemini':
            model_for_endpoint = None
            if args.models and len(args.models) == 1:
                model_for_endpoint = args.models[0].split(':', 1)[-1]
            llm_config['endpoint'] = DEFAULT_GEMINI_ENDPOINT_TEMPLATE.format(
                model=model_for_endpoint or DEFAULT_GEMINI_MODEL
            )

    if llm_config['max_tokens'] is None:
        auto_tokens = AUTO_PROVIDER_MAX_TOKENS.get(args.llm_provider.lower())
        if auto_tokens:
            llm_config['max_tokens'] = auto_tokens

    # Configure based on mode
    if args.quick:
        # Quick test mode
        models = args.models if args.models else select_default_models(args.llm_provider, quick=True)
        conditions = args.conditions if args.conditions else ['c4']
        limit = 3
        offset = 0
        output_dir = args.output if args.output else Path('results/quick_test')
        server_id = None

        print_header("Quick Test Mode")
        print_llm_settings(llm_config)
        print(f"  Testing 3 cases with {models[0]}, condition C4")

    elif args.distributed:
        # Distributed mode
        server_id = int(args.distributed[0])
        num_servers = int(args.distributed[1])
        total_cases = int(args.distributed[2])

        models = args.models if args.models else select_default_models(args.llm_provider)
        conditions = args.conditions if args.conditions else ['c1', 'c2', 'c3', 'c4']

        # Calculate case allocation
        offset, limit = calculate_case_allocation(server_id, num_servers, total_cases)

        output_dir = args.output if args.output else Path(f'results/server{server_id}')

        print_header(f"Distributed Mode - Server {server_id}")
        print_llm_settings(llm_config)
        print(f"  Total servers: {num_servers}")
        print(f"  Total cases: {total_cases}")
        print(f"  This server: cases {offset} to {offset + limit - 1} ({limit} cases)")

    else:
        # Local mode
        models = args.models if args.models else select_default_models(args.llm_provider)
        conditions = args.conditions if args.conditions else ['c1', 'c2', 'c3', 'c4']
        limit = args.limit
        offset = args.offset
        output_dir = args.output if args.output else Path('results/local')
        server_id = None

        print_header("Local Experiment Mode")
        print_llm_settings(llm_config)
        if limit:
            print(f"  Processing {limit} cases")
        else:
            print(f"  Processing all cases from dataset")

    # Create output directory
    output_dir.mkdir(parents=True, exist_ok=True)

    # Run experiment
    try:
        run_experiment(
            dataset=args.dataset,
            models=models,
            conditions=conditions,
            output_dir=output_dir,
            llm_config=llm_config,
            start_index=offset,
            limit=limit,
            generate_incomplete=not args.skip_incomplete_patches,
            server_id=server_id,
            verbose=not args.quiet,
            parallel_conditions=args.parallel_conditions,
            disable_consistency_check=args.disable_consistency_check,
            stage1_cache_dir=normalized_stage1_cache,
            force_stage1_recompute=args.refresh_stage1_cache,
            precompute_stage1_only=args.precompute_stage1,
            skip_judge_evaluation=args.skip_judge_evaluation,
            use_majority_voting=args.use_majority_voting,
            judge_models=args.judge_models,
        )

        if args.precompute_stage1:
            print("\nâœ… Stage-1 caching completed successfully!\n")
        else:
            print("\nâœ… Experiment completed successfully!\n")

    except KeyboardInterrupt:
        print("\n\nâš ï¸  Experiment interrupted by user\n")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ Experiment failed: {e}\n")
        import traceback
        traceback.print_exc()
        sys.exit(1)


def _precompute_stage1_batch(
    cases: List[Dict],
    stage1_cache_dir: Path,
    *,
    force_stage1_recompute: bool = False,
    verbose: bool = True,
) -> None:
    """Cache Stage-1 artifacts for all cases without invoking any LLMs."""
    from patchscribe.pipeline import PatchScribePipeline

    if verbose:
        print(f"\nCaching Stage-1 artifacts into: {stage1_cache_dir}")

    pipeline = PatchScribePipeline(
        strategy='formal',
        explain_mode='template',
        enable_consistency_check=False,
        enable_performance_profiling=False,
        stage1_cache_dir=stage1_cache_dir,
        force_stage1_recompute=force_stage1_recompute,
    )
    total = len(cases)
    for idx, case in enumerate(cases, 1):
        pipeline.precompute_stage1(case)
        if verbose:
            identifier = case.get('id') or case.get('case_id') or f"case_{idx}"
            print(f"  [{idx}/{total}] cached {identifier}")


if __name__ == '__main__':
    main()
