# PatchScribe ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

## ğŸ“š ì „ì²´ ì‹¤í—˜ ì‹¤í–‰í•˜ê¸°

ë…¼ë¬¸ì˜ ëª¨ë“  RQ(Research Questions)ë¥¼ ê²€ì¦í•˜ê¸° ìœ„í•œ ì™„ì „í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.

---

## ğŸš€ ê°€ì¥ ë¹ ë¥¸ ì‹œì‘ (30ì´ˆ)

### ì˜µì…˜ 1: ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (10ë¶„)
```bash
# 3ê°œ ì¼€ì´ìŠ¤ë§Œìœ¼ë¡œ íŒŒì´í”„ë¼ì¸ì´ ë™ì‘í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸
./quick_test.sh
```

### ì˜µì…˜ 2: ì „ì²´ ì‹¤í—˜ (2-3ì‹œê°„)
```bash
# 10ê°œ ì¼€ì´ìŠ¤ë¡œ ëª¨ë“  RQ ì‹¤í—˜ ì‹¤í–‰
./run_all_experiments.sh 2>&1 | tee experiment_log.txt
```

---

## ğŸ“– ìƒì„¸ ê°€ì´ë“œ

### 1ï¸âƒ£ í™˜ê²½ ì¤€ë¹„

```bash
# Python ë²„ì „ í™•ì¸ (3.8 ì´ìƒ í•„ìš”)
python3 --version

# LLM ì„¤ì • (ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© ì‹œ)
export PATCHSCRIBE_LLM_PROVIDER=ollama
export PATCHSCRIBE_LLM_MODEL=llama3.2:1b

# Ollama ì‹œì‘ ë° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama serve  # ë³„ë„ í„°ë¯¸ë„ì—ì„œ
ollama pull llama3.2:1b
```

### 2ï¸âƒ£ ë‹¨ê³„ë³„ ì‹¤í–‰

#### Step 1: RQ1 - Theory-Guided Generation (60ë¶„)
```bash
# C1, C2, C3, C4 ëª¨ë“  ì¡°ê±´ í‰ê°€
python3 scripts/run_full_evaluation.py zeroday \
    --conditions c1 c2 c3 c4 \
    --limit 10 \
    --output results/evaluation_full
```

#### Step 2: RQ2 - Incomplete Patches ìƒì„± (2ë¶„)
```bash
# ê° ì·¨ì•½ì ë‹¹ 2-3ê°œì˜ ë¶ˆì™„ì „ íŒ¨ì¹˜ ìƒì„±
python3 scripts/inject_incomplete_patches.py \
    --dataset zeroday \
    --limit 10 \
    --output results/incomplete_patches
```

#### Step 3: RQ2 - Verification Ablation (90ë¶„)
```bash
# V1, V2, V3, V4 ê²€ì¦ ë°©ë²• ë¹„êµ
python3 scripts/run_verification_ablation.py \
    --dataset zeroday \
    --limit 10 \
    --incomplete-patches results/incomplete_patches/incomplete_patches_zeroday.json \
    --output results/verification_ablation
```

#### Step 4: ê²°ê³¼ ë¶„ì„
```bash
# ê° ì¡°ê±´ì— ëŒ€í•œ RQ ë¶„ì„
python3 scripts/run_rq_analysis.py \
    results/evaluation_full/raw_results/full_patchscribe_c4_results.json \
    -o results/rq_analysis/rq_analysis.json

# ìµœì¢… ë³´ê³ ì„œ í™•ì¸
cat results/evaluation_full/EVALUATION_REPORT.md
```

---

## ğŸ“Š ê²°ê³¼ í™•ì¸

### ë¹ ë¥¸ ìš”ì•½
```bash
# ëª¨ë“  ì¡°ê±´ì˜ ì„±ê³µë¥  í™•ì¸
for file in results/evaluation_full/raw_results/*_results.json; do
    echo "=== $(basename $file) ==="
    python3 -c "
import json
with open('$file') as f:
    data = json.load(f)
    print(f\"Success rate: {data['metrics']['success_rate']:.1%}\")
"
done
```

### ìƒì„¸ ê²°ê³¼ ìœ„ì¹˜
```
results/
â”œâ”€â”€ evaluation_full/
â”‚   â”œâ”€â”€ raw_results/              # RQ1 ê²°ê³¼ (C1-C4)
â”‚   â””â”€â”€ EVALUATION_REPORT.md      # ìµœì¢… ìš”ì•½ ë³´ê³ ì„œ
â”œâ”€â”€ incomplete_patches/           # RQ2 ë¶ˆì™„ì „ íŒ¨ì¹˜
â”œâ”€â”€ verification_ablation/        # RQ2 V1-V4 ë¹„êµ
â””â”€â”€ rq_analysis/                  # ëª¨ë“  RQ ë¶„ì„
```

---

## ğŸ¯ ì˜ˆìƒ ê²°ê³¼ (ë…¼ë¬¸ ê¸°ì¤€)

### RQ1: Theory-Guided Generation
| ì¡°ê±´ | ì„±ê³µë¥  | ì„¤ëª… |
|------|--------|------|
| C1 (Baseline) | ~30% | Post-hoc, no formal guidance |
| C2 (Vague Hints) | ~40% | Informal prompts |
| C3 (Pre-hoc) | ~50% | E_bug without verification |
| C4 (Full) | ~70% | E_bug + triple verification |

### RQ2: Dual Verification
| ë°©ë²• | Precision | Recall |
|------|-----------|--------|
| V1 (Exploit-only) | ~60% | ~50% |
| V2 (Symbolic-only) | ~75% | ~70% |
| V3 (Consistency-only) | ~85% | ~75% |
| V4 (Triple) | ~90% | ~80% |

### RQ3: Performance
| ë³µì¡ë„ | í‰ê·  ì‹œê°„ |
|--------|----------|
| Simple (<50 LoC) | ~120s |
| Medium (50-100) | ~160s |
| Complex (>100) | ~240s |

---

## ğŸ”§ ë¬¸ì œ í•´ê²°

### LLM ì—°ê²° ì˜¤ë¥˜
```bash
# Ollama ìƒíƒœ í™•ì¸
curl http://localhost:11434/api/tags

# ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ
ollama pull llama3.2:1b
```

### ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# ìˆœì°¨ ì‹¤í–‰ìœ¼ë¡œ ë³€ê²½
python3 scripts/run_full_evaluation.py zeroday \
    --conditions c4 \
    --limit 5 \
    --max-workers 1
```

### ë°ì´í„°ì…‹ ì—†ìŒ
```bash
# ë°ì´í„°ì…‹ í™•ì¸
ls -la datasets/zeroday_repair/

# ì¼€ì´ìŠ¤ ìˆ˜ í™•ì¸
python3 -c "from patchscribe.dataset import load_cases; print(len(load_cases('zeroday')))"
```

---

## ğŸ“‹ Quick Reference - Research Questions

### RQ1: Theory-Guided Generation Effectiveness
**ì§ˆë¬¸**: ì‚¬ì „ í˜•ì‹ ëª…ì„¸(E_bug)ê°€ ë” ì •í™•í•œ íŒ¨ì¹˜ë¥¼ ìƒì„±í•˜ëŠ”ê°€?

**ì‹¤í–‰**:
```bash
python3 scripts/run_full_evaluation.py zeroday --conditions c1 c2 c3 c4 --limit 10
```

**ì¸¡ì • ì§€í‘œ**:
- Triple verification rate (ì‚¼ì¤‘ ê²€ì¦ í†µê³¼ìœ¨)
- Ground truth similarity (ì‹¤ì œ íŒ¨ì¹˜ ìœ ì‚¬ë„)
- First attempt success rate (ì²« ì‹œë„ ì„±ê³µë¥ )

### RQ2: Dual Verification Effectiveness
**ì§ˆë¬¸**: ì´ì¤‘ ì„¤ëª…(E_bug â†” E_patch) + ì¼ê´€ì„± ê²€ì¦ì´ ë¶ˆì™„ì „ íŒ¨ì¹˜ë¥¼ íƒì§€í•˜ëŠ”ê°€?

**ì‹¤í–‰**:
```bash
python3 scripts/run_full_evaluation.py zeroday --conditions c4 --limit 10
```

**ì¸¡ì • ì§€í‘œ**:
- Incomplete patches caught (ë¶ˆì™„ì „ íŒ¨ì¹˜ íƒì§€ ìˆ˜)
- Consistency violation breakdown (ì¼ê´€ì„± ìœ„ë°˜ ìœ í˜•)

### RQ3: Scalability and Performance
**ì§ˆë¬¸**: 3ë‹¨ê³„ ì›Œí¬í”Œë¡œìš°ì˜ ì‹œê°„ ì˜¤ë²„í—¤ë“œëŠ”?

**ì¸¡ì • ì§€í‘œ**:
- Phase 1/2/3 time (ë‹¨ê³„ë³„ ì‹œê°„)
- Total time (ëª©í‘œ: <3ë¶„)
- Peak memory usage

### RQ4: Explanation Quality
**ì§ˆë¬¸**: ì´ì¤‘ ì„¤ëª…ì´ ê°œë°œìì—ê²Œ ìœ ìš©í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ëŠ”ê°€?

**ì¸¡ì • ì§€í‘œ**:
- Checklist coverage (ìë™)
- Expert quality scores (GPT ê¸°ë°˜)

---

## ğŸ“š ë” ë§ì€ ì •ë³´

- **ì „ì²´ ì›Œí¬í”Œë¡œìš°**: [EXPERIMENT_WORKFLOW.md](EXPERIMENT_WORKFLOW.md)
- **ë°ì´í„°ì…‹ ê°€ì´ë“œ**: [DATASET_GUIDE.md](DATASET_GUIDE.md)
- **RQ í‰ê°€ ê°€ì´ë“œ**: [RQ_EVALUATION_GUIDE.md](RQ_EVALUATION_GUIDE.md)
- **RQ2 ì „ë¬¸ ê°€ì´ë“œ**: [RQ2_EVALUATION_GUIDE.md](RQ2_EVALUATION_GUIDE.md)
- **ë¶„ì‚° ì‹¤í–‰**: [DISTRIBUTED_GUIDE.md](DISTRIBUTED_GUIDE.md)
- **ì„±ëŠ¥ íŠœë‹**: [PERFORMANCE_TUNING.md](PERFORMANCE_TUNING.md)

---

## ğŸ’¡ í•µì‹¬ ëª…ë ¹ì–´ë§Œ ë³´ê¸°

### ë‹¨ì¼ ì„œë²„
```bash
# 1. ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
./quick_test.sh

# 2. ì „ì²´ ì‹¤í—˜
./run_all_experiments.sh

# 3. ê°œë³„ ì‹¤í–‰
python3 scripts/run_full_evaluation.py zeroday --conditions c1 c2 c3 c4 --limit 10
python3 scripts/inject_incomplete_patches.py --dataset zeroday --limit 10
python3 scripts/run_verification_ablation.py --dataset zeroday --limit 10 \
    --incomplete-patches results/incomplete_patches/incomplete_patches_zeroday.json

# 4. ê²°ê³¼ í™•ì¸
cat results/evaluation_full/EVALUATION_REPORT.md
```

### ì—¬ëŸ¬ ì„œë²„ (ë¶„ì‚° ì‹¤í–‰) âš¡

```bash
# 1. ê° ì„œë²„ì—ì„œ ì‹¤í–‰ (ëª¨ë“  ëª¨ë¸ Ã— ëª¨ë“  ì¡°ê±´ C1-C4 ìë™ ì‹¤í–‰)
# Server 0:
python3 scripts/run_distributed.py 0 4 20 zeroday

# Server 1:
python3 scripts/run_distributed.py 1 4 20 zeroday

# Server 2:
python3 scripts/run_distributed.py 2 4 20 zeroday

# Server 3:
python3 scripts/run_distributed.py 3 4 20 zeroday

# 2. ê²°ê³¼ ìˆ˜ì§‘ (ì¤‘ì•™ ì„œë²„)
scp -r user@server0:~/patchscribe/results/server0 results/
scp -r user@server1:~/patchscribe/results/server1 results/
scp -r user@server2:~/patchscribe/results/server2 results/
scp -r user@server3:~/patchscribe/results/server3 results/

# 3. ê²°ê³¼ ë³‘í•©
python3 scripts/aggregate_results.py --mode merge --results-dir results --output results/merged

# 4. RQ ë¶„ì„ (ëª¨ë¸ë³„ë¡œ)
python3 scripts/run_rq_analysis.py results/merged/llama3.2:1b/c4_merged_results.json
python3 scripts/run_rq_analysis.py results/merged/llama3.2:3b/c4_merged_results.json
python3 scripts/run_rq_analysis.py results/merged/qwen2.5-coder:7b/c4_merged_results.json
```

**ì°¸ê³ **:
- í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ëŠ” `--models` ì˜µì…˜ìœ¼ë¡œ ì§€ì • ê°€ëŠ¥ (ê¸°ë³¸ê°’: gemma3:4b, qwen3:4b, deepseek-r1:7b, llama3.2:3b)
- ê° ì„œë²„ëŠ” í• ë‹¹ëœ ë°ì´í„°ì— ëŒ€í•´ ëª¨ë“  ëª¨ë¸ê³¼ ì¡°ê±´ì„ ìë™ìœ¼ë¡œ ì‹¤í—˜

**ìƒì„¸ ê°€ì´ë“œ**: [DISTRIBUTED_GUIDE.md](DISTRIBUTED_GUIDE.md) ì°¸ê³ 

---

**ì™„ë£Œ!** ğŸ‰ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì´ìŠˆë¥¼ ë“±ë¡í•´ì£¼ì„¸ìš”.
