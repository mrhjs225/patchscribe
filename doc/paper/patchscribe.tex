\documentclass[conference,compsoc]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{textcomp}
\lstset{basicstyle=\ttfamily\footnotesize,breaklines=true}
\providecommand{\tightlist}{\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}
\title{PatchScribe: Theory-Guided Vulnerability Repair with Causal Explanations}

\author{\IEEEauthorblockN{Anonymous Authors}}
\maketitle
\begin{abstract}
Large Language Models (LLMs) show promise in automated vulnerability repair, yet their patches often lack verifiable guarantees of correctness.
Current approaches rely on exploit-only validation and informal rationales, providing limited assurance that vulnerabilities are truly eliminated.
We present PatchScribe, a theory-guided framework that generates security patches with dual causal explanations through a two-phase workflow.
PatchScribe inverts traditional workflows: it first formalizes vulnerabilities using Program Causal Graphs (PCG) and Structural Causal Models (SCM) to generate a formal bug specification ($E_{\text{bug}}$), then guides LLM-based patch generation with precise constraints, producing patches accompanied by formal patch explanations ($E_{\text{patch}}$).
We evaluate patch and explanation quality through structured manual assessment using a multi-dimensional rubric (accuracy, completeness, clarity, causality).
Evaluated on 121 real-world CVEs from two complementary benchmarks (APPATCH Zeroday Repair: 97 cases, ExtractFix: 24 cases), PatchScribe achieves [X\%] patch correctness rate, significantly outperforming raw LLM baseline ([Y\%], $p$<0.001) and informal guidance approaches ([W\%], $p$<0.01), with average processing time of [Z] seconds per CVE.
\% todo: Fill after experiments complete - expected results: X~75-80%, Y~30-35%, W~50-55%, Z~40-50
Our theory-guided approach provides clearer causal understanding than existing exploit-only methods while remaining computationally practical for real-world deployment.
\end{abstract}
\section{Introduction}\label{introduction}

LLM-based code assistants have shown promise in automatically fixing vulnerable code, but a critical gap remains: can we trust that the LLM's patch truly eliminates the vulnerability, and can we verify the reasoning behind it?
In current practice, an LLM might propose a code change along with a natural language explanation of the fix.
However, such explanations are post-hoc and often not verifiable by any rigorous means -- they could be incomplete, incorrect, or even hallucinated.
This lack of verifiability in patch rationales poses a security risk: a patch that ``sounds'' correct might still fail to eliminate the underlying exploit path or might introduce new issues, all while the developer is misled by a plausible but unproven explanation.

Recent research underscores the limitations of relying on informal validation of patches.
Studies have found that LLM-generated fixes can be plausible yet incorrect, passing unit tests or  superficial checks without truly removing the vulnerability.
For example, incomplete patches in real-world libraries (PyYAML, Pillow, Django) passed initial review but left residual flaws that attackers later exploited.
To counter this, efforts like VulnRepairEval advocate for exploit-based validation, judging a patch by whether a proof-of-concept (PoC) exploit is blocked.
This is a step toward realism -- requiring that the original attack no longer succeeds -- and has revealed that state-of-the-art LLMs fix only \textasciitilde22\% of known CVEs under these strict conditions.
However, even exploit-only validation is limited: it confirms that one particular attack input is mitigated, but it does not prove in general that the vulnerability is fully eradicated or that no new vulnerabilities are introduced by the patch.
Moreover, it provides no insight why the patch works (if it does).
Developers and security  auditors are left to trust the LLM's textual rationale, which may be unfaithful to the code's actual logic.

We argue that a more principled approach is needed -- one that combines the generative capabilities of LLMs with formal reasoning about causal relationships in the code.
Our key insight is twofold: (1) formalize the vulnerability before attempting to patch it, using the formalization to guide (not just verify) patch generation, and (2) generate separate formal explanations for the bug and the patch, enabling consistency checking between them.
If we can formally capture what conditions cause the vulnerability (E\_bug), we can provide the LLM with precise guidance (e.g., ``V\_overflow occurs when len \textgreater{} 256 AND no check exists; ensure one of these is false''). After the LLM generates a patch, we formalize how the patch intervenes (E\_patch) and verify that the intervention actually addresses the causes identified in E\_bug.
This dual-explanation approach catches incomplete fixes that might pass exploit tests but miss edge cases.

We introduce several key concepts that distinguish our approach:

\textbf{Theory-Guided Generation:} Unlike post-hoc explanations that describe patches after generation, PatchScribe uses pre-hoc formalizationâ€”constructing a formal vulnerability specification before patch generation to guide the LLM with precise constraints.

\textbf{Dual Causal Explanations:} We generate two complementary explanations: $E_{\text{bug}}$ (formal specification of vulnerability's root cause) and $E_{\text{patch}}$ (formal specification of how the patch eliminates it). These dual explanations provide developers with clear causal understanding of both the vulnerability and its fix.

\textbf{Structured Manual Evaluation:} We assess patch and explanation quality through structured manual evaluation using a multi-dimensional rubric measuring accuracy (technical correctness), completeness (coverage of what/why/how), clarity (understandability), and causality (depth of causal reasoning).

Our key contributions are:
\begin{itemize}
\item A novel pre-hoc formalization approach using Program Causal Graphs and Structural Causal Models for vulnerability repair
\item A dual explanation framework enabling systematic consistency verification between bug causes and patch interventions
\item Comprehensive evaluation on 121 real-world CVEs demonstrating [X\%] improvement over baseline approaches
\item Open-source implementation and reproducible artifacts for community validation
\end{itemize}

In this paper, we introduce PatchScribe, a framework that brings theory-guided causal reasoning to LLM-based vulnerability repair.
Unlike prior approaches that explain patches post-hoc, PatchScribe follows a pre-hoc methodology: formalize first, then generate patches with dual explanations. The approach builds a Program Causal Graph (PCG) to represent the causal structure of the vulnerability (for instance, how a lack of input validation leads to a buffer overflow) and instantiates a Structural Causal Model (SCM) on top of this graph.
From the SCM, we generate a formal bug explanation ($E_{\text{bug}}$) that precisely characterizes the vulnerability condition, causal paths, and intervention options. This formal specification is provided to the LLM as guidance, enabling it to generate more targeted patches.
After patch generation, we analyze how the patch intervenes on the causal model and generate a formal patch
explanation ($E_{\text{patch}}$) describing what changed and why the vulnerability is eliminated.
This two-phase approach (Formalization $\rightarrow$ Theory-Guided Generation) produces patches with dual causal explanations, providing clearer understanding than prior work and enabling systematic assessment of patch quality.

We demonstrate that PatchScribe significantly improves quality of automated patches.
By using formal causal specifications to guide patch generation, we help LLMs generate patches that address root causes rather than symptoms.
Our approach directly addresses the limitations of prior work: (1) It provides pre-hoc formalization that guides generation with precise constraints rather than post-hoc validation alone, and (2) it replaces unverifiable natural-language rationales with structured causal explanations ($E_{\text{bug}}$, $E_{\text{patch}}$) that developers can systematically assess.
This yields a double benefit: higher quality patches and clearer understanding of vulnerability and fix.

Section~\ref{sec:background} provides background and motivation.
Section~\ref{sec:approach} presents the framework overview.
Sections~\ref{sec:formal-model} and~\ref{sec:system} detail our formal model and system architecture.
Section~\ref{sec:evaluation} presents experimental evaluation.
Section~\ref{sec:related} discusses related work.
Section~\ref{sec:discussion} addresses security guarantees, limitations, and ethics.
Section~\ref{sec:conclusion} concludes.

\section{Background and Motivation}\label{sec:background}

\subsection{LLM-Based Vulnerability Repair}

Automated vulnerability repair has long been a goal of the security community.
With the rise of powerful code-focused LLMs, researchers have explored using these models to generate vulnerability fixes from code context and problem descriptions.
Early results are encouraging yet cautionary: LLM-generated patches can superficially appear correct while failing to eliminate the security issue.

Wang et al.'s VulnRepairEval~\cite{wang2025vulnrepaireval} explicitly uses real proof-of-concept exploits as validation, revealing that state-of-the-art LLMs fix only \textasciitilde22\% of known CVEs under strict conditions.
To improve generation, researchers have incorporated reasoning techniques.
Kulsum et al.'s VRpilot~\cite{kulsum2024vrpilot} uses chain-of-thought prompting plus iterative validation, improving patch success rates by 14\% for C vulnerabilities.
SAN2PATCH~\cite{kim2025san2patch} employs Tree-of-Thought prompting and AddressSanitizer guidance, achieving 63--79\% success rates on benchmark datasets.

However, all these approaches rely on the LLM's internal reasoning or external tests to judge correctness.
Whether it's VRpilot's chain-of-thought or SAN2PATCH's guided stages, the rationale is encapsulated in natural language explanations or test passing---neither provides formal guarantees of security.
Even if an exploit is thwarted, one cannot be sure that a variant wouldn't succeed or that the patch didn't introduce new vulnerabilities.

\subsection{Limitations of Current Approaches}

Despite progress, existing LLM-based repair methods exhibit critical limitations that motivate our work:

\textbf{L1: Incomplete Validation.}
Many approaches validate patches using tests or specific exploit instances, not comprehensive proofs.
Exploit-based evaluations use one PoC exploit as the litmus test---if blocked, the patch is deemed successful.
However, this overlooks variant exploits or edge cases.
A patch might check for a known malicious input pattern rather than fixing the underlying unsafe logic; the original exploit fails, but a tweaked input could still succeed.
Thus, exploit-only validation cannot guarantee completeness.

\textbf{L2: Unverifiable Rationales.}
LLMs produce natural language explanations that are post-hoc and unverifiable.
Studies observe LLMs giving confident-sounding explanations that are partially or wholly incorrect---hallucinated reasoning~\cite{ullah2024secLLMHolmes}.
An LLM might assert ``the buffer is now bounds-checked, so the overflow is resolved,'' but without code verification, we cannot confirm the check covers all cases or is correctly implemented.
No current system provides a formal link between explanation and code.

\textbf{L3: Lack of Formal Guarantees.}
Traditional program repair explored formal methods~\cite{wang2024sok}, but integrating formal verification with LLM-driven patching has seen minimal exploration.
Formal specifications for security properties are hard to write for arbitrary code, and fully verifying a patch can be as hard as verifying the entire program.
Most LLM repair systems avoid formal proofs, leaving no strong guarantees backing the patch.
Current LLM patching often operates in a ``generate-and-hope'' mode---insufficient for high-assurance domains.

\textbf{L4: Narrow Reasoning Context.}
LLMs might not fully understand the causal chain of a vulnerability, acting on local cues.
For a ``read past buffer end'' vulnerability, an LLM might locally add a length check, but if the root cause involves multiple functions passing incorrect sizes, a local fix might not solve it.
Without an explicit representation of causality, there is risk of addressing symptoms rather than causes.
Code property graphs and taint analyses create global views for detection, but LLM-driven repair hasn't fully leveraged them.
As a result, some patches are ``fragile''---they only intercept the known path, not all paths.

\subsection{Motivation for Causal Reasoning}

This motivates introducing formal, causal reasoning into the vulnerability repair loop.
Our motivation draws from explainable AI and formal methods.
In explainable AI, a distinction exists between interpretations (post-hoc, model-generated justifications) and explanations grounded in true causal factors.
Current LLM patch explanations are interpretations---fluent narratives that may not align with the program's actual logic.
We seek causal explanations reflecting genuine cause-effect relations in code.

Meanwhile, formal verification in security provides strong guarantees (e.g., proof-carrying code), but writing full specifications for arbitrary software is notoriously difficult.
Our approach threads a middle ground: we do not require full formal specification of program behavior, only formal characterization of the vulnerability condition and its causes.
This makes the problem tractable while yielding actionable proofs about the patch's effect.

PatchScribe bridges LLM-driven flexibility with systematic assurance by constructing a causal model of the vulnerability, ensuring patches address the core cause through dual explanation generation.

\subsection{Design Requirements}

\begin{itemize}
\item
  \textbf{R1:} Causal Correctness Guarantee. The system must ensure that   the patch eliminates the root cause of the vulnerability.
  In practice, this means if the vulnerability is characterized by a condition or event (e.g., a buffer overflow at line X when condition Y is true), the patched program should prevent that condition/event for all relevant inputs.
  This goes beyond passing a specific exploit test -- it requires reasoning about all paths and inputs related to the vulnerability cause.
  Formally, the patched program should satisfy a safety property: the vulnerability condition/event is unreachable.
  Our system should be built to prove or check this property for each patch.
\item
  \textbf{R2:} Machine-Checkable Explanation. For each patch, the system
  should produce an explanation that is encoded in a structured, formal or semi-formal manner such that a machine (automated tool) can verify its correctness.
  This is in contrast to free-form natural language.
  The explanation might be represented as logical assertions, traces on a graph, or annotations in code that can be checked.
  The key is that there is no ambiguity -- the explanation corresponds to a verifiable claim about program behavior.
  For instance, a valid explanation might be: ``The new code adds a condition if (len \textgreater{} N) return; before the memcpy. This ensures that when input length exceeds N, the function exits and the call to memcpy (which caused overflow) is never reached.''
  This statement can be translated to a check: prove that if len \textgreater{} N, then the memcpy line is not executed in the patched program.
  The design must facilitate generating such checkable claims.
\item
  \textbf{R3:} Integration with LLM Patch Generation. The approach should still leverage LLMs for what they are good at -- understanding code context and synthesizing code -- but the system must guide or validate the LLM with the formal model.
  This implies two sub-requirements: (a) Guidance: the system might use the causal model to prompt the LLM in a more informed way (e.g., telling it explicitly what the cause is that needs addressing).
  If the LLM can incorporate this, patches are more likely to hit the mark.
  (b) Post-check: every patch from the LLM should be fed into the explanation generator and checker, forming a loop where an LLM-suggested fix is not accepted until it passes verification.
  The design should enable iterative refinement: if the first patch fails R1 or R2, perhaps the system can prompt the LLM with additional information (like ``the fix didn't cover scenario X'') and try again.
\item
  \textbf{R4:} Minimal False Positives/Negatives in Verification. The verification step (explanation checking) must be sound (no false claim of success if vulnerability still exists) and as complete as possible (should catch all true errors in the patch).
  In formal terms, if the explanation checker approves a patch, the vulnerability should truly be fixed (this is critical for trust). If it rejects a patch, ideally the patch is indeed faulty -- though it's acceptable to sometimes reject a correct patch if our analysis is conservative, we would then manually or heuristically handle it.
  The design should favor soundness in security (better to reject a correct patch than accept a wrong one, as a wrong one in deployment is dangerous).
  Achieving this may require using multiple methods (symbolic execution, static analysis, even some bounded model checking) to ensure thorough coverage of the vulnerability scenario.
\item
  \textbf{R5:} Causal Graph Coverage and Accuracy. The Program Causal Graph that underpins the system should capture the relevant program flows and conditions related to the vulnerability.
  This is a requirement because if the PCG misses part of the causal chain, the explanation could be incomplete, and the verification might unknowingly omit a scenario.
  Therefore, building the PCG likely requires combining several program analysis techniques (data flow for how tainted input reaches a sink, control flow for what conditions guard the vulnerable code, etc.).
  The graph should be precise enough to distinguish the key decision points.
  We also need the PCG to handle multiple contributing causes (e.g., a bug may require two conditions to be true, like integer overflow + unchecked length; the model should capture both).
  Essentially, the requirement is that the PCG/SCM must form a correct model of the vulnerability, as only then can the explanation be valid.
\item
  \textbf{R6:} Usability and Interpretability. While being formal, the approach's output should still be human-readable or at least interpretable by a developer.
  This means the explanations should ideally be presented in terms of program entities (variables, functions, conditions) and not overly abstract formulas.
  If we produce something like ``$\forall n: n < N \Rightarrow \lnot overflow$'', that's formal but maybe not immediately clear to a developer.
  We might instead say ``Because the patch ensures n \textless{} N before writing, an overflow cannot occur.''
  The requirement is to maintain a connection between the formal model and intuitive understanding, so the tool can be adopted by practitioners who want assurance but aren't formal methods experts.
  A secondary aspect is minimizing additional annotation burden -- the approach should work with just the code and perhaps a description of the vulnerability, rather than requiring the developer to write full specifications.
\item
  \textbf{R7:} Compatibility with Real-World Tools and Workflows. The system should be designed to integrate with typical development workflows.
  That implies using standard languages (the prototype might focus on C/C++ or Python vulnerabilities, given existing benchmarks), interfacing with existing compilers or analyzers, and keeping runtime of verification reasonable.
  If our approach took days of SMT solving to verify one patch, it'd be impractical. So performance is a consideration: the design should focus the formal checks on the vulnerability-specific parts of the program to scale.
  Additionally, it should be able to ingest realistic code (with libraries, etc.) by leveraging robust parsing (hence using Clang/LLVM for C, for example).
  Ideally, PatchScribe can be run as an automated tool in a CI/CD pipeline for security patches, meaning it should output clear pass/fail signals and reports.
\item
  \textbf{R8:} No Regression of Functionality. Although our primary focus is security, a requirement for any patch generator is not to break the software's intended use.
  Therefore, our design should include at least a minimal step to check that the patch doesn't cause obvious functionality loss (for instance, if tests are available, run them to ensure they still pass).
  This might be outside the core causal verification loop, but as a requirement, we acknowledge that a ``secure'' patch that shuts down a feature entirely might not be acceptable.
  In practice, we might incorporate regression tests or sanity checks in the evaluation pipeline.
\end{itemize}

These requirements guided the design of PatchScribe, which we describe next.

\section{Approach Overview}\label{sec:approach}

PatchScribe is a system for automated vulnerability repair that produces dual causal explanations for each patch.
Unlike prior approaches that generate patches first and explain them post-hoc, PatchScribe follows a theory-guided approach: it first formalizes the vulnerability causally, then uses this formalization to guide patch generation, producing patches accompanied by dual explanations (\(E_{\text{bug}}\) and \(E_{\text{patch}}\)).
Figure~\ref{fig:overview} illustrates the overall architecture.

\begin{figure}[t]
\centering
\fbox{\parbox{0.9\columnwidth}{
\centering
\textbf{[Figure 1: System Architecture Diagram]}\\[0.5em]
\textbf{Input:} CVE Description + Vulnerable Code\\[0.5em]
\fbox{\parbox{0.85\columnwidth}{
\textbf{Phase 1: Vulnerability Formalization}\\
Static Analysis (Clang/LLVM) $\rightarrow$ PCG Construction\\
27 Security Patterns $\rightarrow$ SCM Instantiation\\
Output: $E_{\text{bug}}$ (formal condition, causal paths, interventions)
}}\\[0.5em]
$\downarrow$\\[0.5em]
\fbox{\parbox{0.85\columnwidth}{
\textbf{Phase 2: Theory-Guided Patch Generation}\\
LLM (GPT-4o-mini) + $E_{\text{bug}}$ Guidance $\rightarrow$ Patch Code\\
Causal Intervention Analysis $\rightarrow$ $E_{\text{patch}}$ Generation\\
Output: Patch + $E_{\text{patch}}$ (intervention, disrupted paths)
}}\\[0.5em]
$\downarrow$\\[0.5em]
\textbf{Output:} Patch + Dual Explanations ($E_{\text{bug}}$, $E_{\text{patch}}$)\\
\textit{Note: Actual diagram with visual flow arrows, boxes, and icons to be inserted}
}}
\caption{PatchScribe system architecture showing two-phase workflow: (1) Vulnerability formalization via PCG/SCM generates $E_{\text{bug}}$, (2) Theory-guided LLM generation produces patch and $E_{\text{patch}}$. Patches are evaluated through structured manual assessment (Section~\ref{sec:evaluation}).}
\label{fig:overview}
\end{figure}

\textbf{At a high level, PatchScribe operates in two phases:}

\textbf{Phase 1: Vulnerability Formalization} - We analyze the vulnerable program to build a Program Causal Graph (PCG) and derive a Structural Causal Model (SCM).
From the SCM, we generate a \textbf{formal vulnerability specification (E\_bug)} that precisely characterizes the conditions under which the vulnerability manifests.
This specification serves as guidance for patch generation.
The formal bug explanation E\_bug contains: (a) the formal condition characterizing when V\_bug occurs, (b) natural language descriptions mapped to code, (c) intervention options for fixing the vulnerability, and (d) causal paths leading to the vulnerability.

\textbf{Phase 2: Theory-Guided Patch Generation} - Armed with the formal vulnerability specification E\_bug, we prompt an LLM to generate a patch.
Critically, the LLM receives not vague hints but a precise formal description of what conditions cause the vulnerability and what the patch must achieve.
For example, instead of saying ``fix the overflow,'' we provide ``$V_{\text{overflow}} \Leftrightarrow (len > 256) \land (\lnot Check)$; to fix, you must ensure Check = true or $len \leq 256$ before line 42.''
After patch generation, we analyze how the patch intervenes on the causal model and generate a \textbf{formal patch explanation (E\_patch)} describing the intervention, its effect on V\_bug, and which causal paths are disrupted.

This approach ensures that patches are guided by formal causal specifications, helping LLMs generate patches that address root causes.
The key innovation is the generation of dual causal explanations (E\_bug and E\_patch), providing developers with systematic understanding of both the vulnerability and its fix.
We evaluate the quality of patches and their explanations through structured manual assessment (Section~\ref{sec:evaluation}).

In meeting the design requirements:
\begin{itemize}
  \item PatchScribe yields causal correctness (R1) by virtue of the SCM reasoning and explanation check that covers all relevant cases, not just one exploit.
  correctness (R1) by virtue of the SCM reasoning and explanation check that covers all relevant cases, not just one exploit.
  \item The explanation is machine-checkable (R2) by construction; it's effectively an intermediate formal spec of the patch's effect that we verify.
  \item The LLM is integrated (R3) through guided prompts and iterative patch attempts informed by the formal analysis.
  \item The verification approach prioritizes soundness (R4): we declare success only with a proof or exhaustive check for the vulnerability condition.
  \item The PCG ensures we focus on the true causes (R5), and we plan to use strong program analysis (like control flow graph, taint tracking) to build it accurately.
  \item For usability (R6), our explanations remain tied to code conditions and can be output in natural language form (``this new check ensures\ldots'') in addition to the formal form.
  \item We leverage real tools (Clang, angr, etc.) ensuring we work on real code (R7). The heavy lifting by these tools (which are optimized in C/C++) helps performance.
  \item And we include regression testing in the evaluation loop for functionality (R8), though our main unique step is the security proof.
\end{itemize}

The following sections detail the formal modeling, system architecture, and implementation.

\subsection{Threat Model and Scope}

\textbf{Assumptions.} We assume the vulnerability location is known (e.g., via CVE description, sanitizer log) and that the LLM and analysis tools are trusted. The threat is not the toolchain but the risk of incorrect or incomplete patches.

\textbf{Attacker Model.} An attacker can provide arbitrary inputs to exploit the vulnerability. Post-patch, they may attempt variant exploits or search for newly introduced weaknesses. PatchScribe addresses two main threats: (1) \emph{Incomplete Fix}---where alternative paths still trigger the vulnerability, and (2) \emph{Misleading Explanation}---where unverifiable rationales give false confidence.

\textbf{Defender Goals.} Produce patches that eliminate the vulnerability for all feasible inputs (not just known exploits) while preserving functionality and avoiding new vulnerabilities. We focus on security correctness, assuming basic regression testing catches functional issues.

\textbf{Out of Scope.} We do not model concurrency vulnerabilities, side-channels, or adversarial manipulation of the LLM itself. Complex multi-step vulnerabilities requiring extensive environmental state may exceed PCG/SCM modeling capabilities (discussed in Section~\ref{sec:discussion}).

\section{Formal Model}\label{sec:formal-model}

Table~\ref{tab:notation} summarizes key notation used throughout this section and the rest of the paper.

\begin{table}[h]
\centering
\caption{Key Notation and Terminology}
\label{tab:notation}
\begin{tabular}{ll}
\toprule
\textbf{Notation} & \textbf{Meaning} \\
\midrule
$P$ & Original (vulnerable) program \\
$P'$ & Patched program \\
$V_{\text{bug}}$ & Vulnerability condition/event \\
$E_{\text{bug}}$ & Formal bug explanation \\
$E_{\text{patch}}$ & Formal patch explanation \\
$G = (V, E)$ & Program Causal Graph (PCG) \\
$do(X=x)$ & Causal intervention (SCM notation) \\
PCG & Program Causal Graph \\
SCM & Structural Causal Model \\
\bottomrule
\end{tabular}
\end{table}

In PatchScribe, the formal foundation is provided by two interrelated models: the Program Causal Graph (PCG) and the Structural Causal Model (SCM).
Here we define each and explain how they are constructed and used.

\subsection{Program Causal Graph (PCG)}\label{program-causal-graph-pcg}

Definition: A Program Causal Graph is a directed graph \(G = (V, E)\) where
each node \(v \in V\) represents a program state predicate or event (e.g., an if condition, a variable state, a function call, a memory write) related to the vulnerability, and a directed edge \((u \rightarrow v) \in E\) indicates that node \(u\) has a direct causal influence on v in the context of the vulnerability.

The PCG is a high-level representation extracted from the program's code and execution flow: - Nodes: We include nodes for conditions (e.g., the truth value of an if condition), for certain variable states (e.g., ``variable x has value n''), and for specific events like ``function f calls g'' or ``memory write at location L occurs''.
Particularly, we distinguish a special node V\_``bug'' representing the occurrence of the vulnerability (e.g., an out-of-bounds write or a crash condition).
We also include nodes representing the negation or absence of certain checks, since the lack of a check is often a cause (for example, a node might be ``No null-pointer check before dereference'' which is essentially a predicate that is true when a check is missing in the code path).
Edges: If the program logic is such that u being true or an event happening contributes to v becoming true/happening, we draw an edge \(u \rightarrow v\).
This is akin to saying ``\(u\) is a direct cause of \(v\)'' under the framework of causal graphs (similarly to Bayesian network or Pearl's causal diagrams, but here based on program logic rather than statistical data).
For instance, if the code has if \(len > N\) goto error; then the condition ``len \(> N\)'' (node \(u\)) causally influences whether the program goes to error handling (node v).
In a vulnerability context, we might have edges like ``Input not sanitized'' \(\rightarrow\) ``Buffer overflow occurs'' or ``Flag is false'' \(\rightarrow\) ``Access control bypass''.

\subsubsection{PCG Construction Algorithm}

To build the PCG, we rely on program analysis techniques combined with pattern detection.
Algorithm~\ref{alg:pcg} presents our automated construction procedure.

\begin{figure}[t]
\begin{lstlisting}[language=Python, basicstyle=\ttfamily\scriptsize, frame=single]
Algorithm: PCG Construction
Input: Vulnerability location L_v, Program P
Output: Program Causal Graph G = (V, E)

1. Initialize:
   V = {V_bug}  # Start with vulnerability node
   E = {}

2. Backward Slicing:
   slice = BackwardSlice(P, L_v)  # Using Clang

3. Extract Dependencies:
   for each statement s in slice:
       data_deps = DataDependencies(s)
       ctrl_deps = ControlDependencies(s)

       for each dep in (data_deps + ctrl_deps):
           node_s = CreateNode(s)
           node_dep = CreateNode(dep)
           V = V union {node_s, node_dep}

           if IsCausalRelation(dep, s):
               E = E union {(node_dep, node_s)}

4. Pattern Detection:
   patterns = DetectSecurityPatterns(slice)
   # e.g., "no null check", "no bounds check"
   for each pattern p:
       node_p = CreateAbsenceNode(p)
       V = V union {node_p}
       E = E union {(node_p, V_bug)}

5. Simplify:
   G = RemoveTransitiveEdges(G)
   return G
\end{lstlisting}
\caption{PCG Construction Algorithm}
\label{alg:pcg}
\end{figure}

\textbf{Implementation Details:}
\begin{itemize}
\item \textbf{Backward slicing:} Clang Static Analyzer with LLVM 14.0
\item \textbf{Taint analysis:} Custom LLVM pass based on DataFlowSanitizer
\item \textbf{Pattern detection:} 27 pre-defined security-relevant patterns (null checks, bounds checks, resource initialization, etc.)
\item \textbf{Automation level:} Fully automated for single-function vulnerabilities; requires manual refinement for complex inter-procedural cases
\end{itemize}

The result is a graph where V\_bug is at the bottom (sink) and various inputs or conditions are at the top (sources), with intermediate nodes linking them.

\textbf{Example:} Consider a C function that reads from a buffer without checking the index, causing a potential out-of-bounds read.
Figure~\ref{fig:pcg-example} illustrates the constructed PCG for this example.

\begin{figure}[t]
  \centering
  \fbox{
    \parbox{0.9\columnwidth}{
      \centering
      \textbf{[Figure: PCG for Out-of-Bounds Read Example]}\\[0.5em]
      \small
      \textbf{Vulnerable Code:}\\
      \texttt{int read\_buffer(char* buf, int len) \{}\\
      \texttt{~~int i = get\_user\_input();~~// No validation}\\
      \texttt{~~return buf[i];~~// Line 42: OOB read}\\
      \texttt{\}}\\[0.5em]

      \textbf{Program Causal Graph:}\\[0.3em]
      \fbox{\parbox{0.85\columnwidth}{
      \textbf{Exogenous (Input):}\\
      $\circ$ \texttt{user\_input} (untrusted source)\\[0.2em]

      $\downarrow$ \textit{data flow}\\[0.2em]

      \textbf{Endogenous (Program State):}\\
      $\circ$ \texttt{i\_value} = \texttt{get\_user\_input()}\\
      $\circ$ \texttt{(i >= buf\_len)} [unsafe condition]\\
      $\circ$ \texttt{no\_bounds\_check} [missing guard]\\[0.2em]

      $\downarrow$ \textit{both contribute}\\[0.2em]

      \textbf{Vulnerability:}\\
      $\circ$ $V_{\text{bug}}$ = Out-of-bounds read at Line 42\\[0.2em]

      \textbf{Causal Edges:}\\
      \texttt{user\_input} $\xrightarrow{\text{determines}}$ \texttt{i\_value}\\
      \texttt{i\_value} $\xrightarrow{\text{creates}}$ \texttt{(i >= buf\_len)}\\
      \texttt{(i >= buf\_len)} $\xrightarrow{\text{triggers}}$ $V_{\text{bug}}$\\
      \texttt{no\_bounds\_check} $\xrightarrow{\text{enables}}$ $V_{\text{bug}}$
      }}\\[0.3em]
      \textit{Note: Actual graph diagram with nodes and directed edges to be inserted}
    }
  }
  \caption{Program Causal Graph example for out-of-bounds read vulnerability showing how untrusted user input and missing bounds check both causally contribute to $V_{\text{bug}}$. The PCG captures two converging causal paths: (1) data flow from user input through unsafe condition, and (2) missing validation check enabling the vulnerability. Patch must disrupt at least one path to eliminate the vulnerability.}
  \label{fig:pcg-example}
\end{figure}

The PCG succinctly shows: user input and missing check together cause the out-of-bounds read.
For programs with multiple conditions, the graph captures more complex causal structures with converging branches.

\subsection{Structural Causal Model (SCM)}\label{structural-causal-model-scm}

Once we have the PCG, we formalize it as an SCM. A Structural Causal Model is typically defined by a set of endogenous variables (variables we model within the system) and exogenous variables (external inputs), along with structural equations that deterministically (or probabilistically, but here deterministically) define each endogenous variable in terms of some of the others, and a causal diagram akin to our PCG that shows dependencies.

Mapping PCG to SCM: - Each node in the PCG becomes a variable in the SCM.
For boolean conditions/events, we treat them as binary variables (true/false).
If certain nodes represent numeric values (like a variable's value), we could include those as numeric variables, but often we reduce to boolean predicates (like ``x \textgreater{} N'' as a boolean variable).
The edges in PCG define parent-child relationships in the SCM. If \(u \rightarrow v\) in PCG, then in the SCM, \(v\) will be a function of \(u\) (and possibly other parents). The SCM's structural equation for v is essentially a formal version of ``v is true if \ldots'' conditions based on its parent nodes.

Example formalization: For the above out-of-bounds example, we might define binary variables: - C (for user Control): C = true if the user can freely choose i (this might be considered exogenous input actually).
- B (for check): B = true if there is a check on i (so B was false originally since no check; we use B=1 to mean check present, 0 means absent).
- A (for array bounds condition): A = true if i \textless{} buffer\_length (safe condition) or maybe define A' = ``i \textgreater= buffer\_length'' if following our earlier text, but let's align to safe vs unsafe. To avoid confusion, define U = true if the unsafe condition holds (i \textgreater= buffer\_length when accessed).
- V (vulnerability event): V = true if an out-of-bounds read occurs.

Now structural equations: - U (unsafe condition) is a function of (C, maybe and actual value of i relative to length). Actually, let's incorporate check: If B (check) is false (no check), then U = (i \textgreater= length) essentially (since nothing stops it). If B is true, presumably the code would not proceed with i out of range, so how to model: If there's a check, either the check prevents the OOB or not.
Simpler: we could model U purely as a condition on i, independent of B, and model B's effect on V. Alternatively: V (vulnerability) depends on U and B. Without patch, B=0 always, so what made V happen is U being true and lack of check? Actually if U is true and no check, then V happens. If B is true (a check is present), presumably the code aborts or doesn't perform read, so V would be false even if U is true. So one structural equation for V could be: V := (NOT B) AND U. (Meaning an out-of-bounds occurs if the check is not present and the unsafe condition is true). - Another equation might define how B gets set or how U depends on input. If the patch introduces a check, B becomes 1; originally B=0. We treat B as a variable that can be toggled by an intervention (the patch). - U's equation: U := (i\_value \textgreater= buffer\_length). Now i\_value itself might be an exogenous variable representing user input. So we might just treat i\_value as given (exogenous). - So exogenous: i (the actual input). - Endogenous: B, U, V. - Structural eqns: * U = 1 if i \textgreater= N, else 0. * V = 1 if (B=0 AND U=1), else 0. (We might incorporate more terms if needed). * B originally (in original program) is 0 (no check). In SCM context, B might not have parents (exogenous decision by programmer). We could treat ``lack of check'' as an exogenous condition in original program. But in an interventional sense, setting B=1 corresponds to adding a check.

This SCM can answer counterfactual queries: What happens to V if B were 1 instead of 0? Under the structural equation, if B=1, V = (1=0 AND U=1) = 0 regardless of U. That matches our intuition: if we intervene to add a check, vulnerability V is prevented (V=0) no matter the unsafe input.

\textbf{In general, the SCM would consist of variables like:}

\(\{X_1, X_2, \ldots, X_k, C_1, \ldots, C_m, V_{\text{bug}}\}\)

Where \(X\) are exogenous inputs (e.g., user-provided data, environment), \(C\) are internal conditions or flags (like B above), and \(V_{\text{bug}}\) is the bug outcome. Each \(C_j\) is defined as \(f_j(\text{Parents}(C_j))\). The bug outcome has an equation \(V_{\text{bug}} = f(\text{Parents}(V_{\text{bug}}))\).
Typically, \(f\) for \(V_{\text{bug}}\) will have a form indicating it triggers when a certain combination of conditions holds (e.g., a logical AND of cause conditions).

Using the SCM for Patching: The SCM provides a formal way to evaluate a patch as an intervention. A patch that adds a check or alters logic is represented as setting some variable or changing some equation in the
SCM:
- Adding a check: changing B from 0 to 1 (an intervention do(B=1)).
- Changing how a value is computed: altering the function \(f_j\) for some variable.
- Removing a feature: could be like making some cause always false.

We then analyze the effect: with the intervention, is \(V_{\text{bug}} = 0\) for all relevant input ranges? This analysis is simpler than full program proof because in the SCM we abstracted away irrelevant parts. It's checking a logical condition: given the structural equations, do we have \(V_{\text{bug}}\) always false? This is akin to a small theorem to prove. Usually, it reduces to checking that some conjunction can't all be true together after the intervention.

It's worth noting that the SCM is only as good as the PCG. If the PCG missed a causal path, the SCM won't consider it and you might prove something that's only partially true. We mitigate this by thorough PCG construction (which might involve dynamic analysis as well as static to not miss feasible paths).

Formal Assurance via SCM: If our SCM-based proof says the patch stops the vulnerability, and our PCG was correct, then we have a very high confidence (almost formal proof) of security.
However, to account for any mismatch between model and program, we still perform the direct code-level symbolic checks as backup.
One can see the SCM proof as a guide and the code-level check as concrete validation.

In summary, the formal model PCG+SCM allows us to reason systematically about cause and effect in code.
It provides the foundation for generating explanations and guiding patch generation through logical analysis of causal interventions.
The next section describes how we operationalize this model in PatchScribe's two-phase system.

\section{PatchScribe System}\label{sec:system}

This section details the two critical phases of our approach:
generating the formal bug explanation before patching, and generating patches with dual explanations.

\subsection{Phase 1: Formal Bug Explanation Generation
(E\_bug)}\label{phase-1-formal-bug-explanation-generation-e_bug}

\textbf{Before any patch is generated, we produce a formal vulnerability
specification from the PCG/SCM. This E\_bug explanation contains:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Formal Condition}: The logical formula characterizing when
  V\_bug occurs
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Example: $V_{\text{overflow}} \Leftrightarrow (len > 256) \land (\lnot Check)$
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Variable Mapping}: Each variable in the formal condition
  mapped to code locations
\end{enumerate}

\begin{itemize}
\item
  len: computed from user\_input at line 15
\item
  Check: bounds check (currently ABSENT before line 42)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Natural Language Description}: Human-readable explanation of
  the vulnerability
\end{enumerate}

\begin{itemize}
\tightlist
\item
  ``Buffer overflow occurs when input length exceeds 256 bytes AND no
  bounds check is performed before memcpy''
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Causal Paths}: All paths from inputs to V\_bug
\end{enumerate}

\begin{itemize}
\item
  \(user\_input \rightarrow len \rightarrow (len > 256) \rightarrow V_{\text{overflow}}\)
\item
  \(\text{absence of check} \rightarrow \lnot Check \rightarrow V_{\text{overflow}}\)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  \textbf{Intervention Options}: Possible ways to fix the vulnerability
\end{enumerate}

\begin{itemize}
    \item Option 1: Add bounds check (set Check = true)
    \item Option 2: Clamp len to a safe value (ensure \(len \leq 256\))
    \item Option 3: Use a safe alternative (e.g., \texttt{memcpy\_s})
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  \textbf{Verification Properties}: Assertions that must hold after
  patching
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Postcondition: ensure \(len \leq 256\) when line 42 is reached, or confirm
  that line 42 is unreachable whenever \(len > 256\)
\end{itemize}

This formal specification is then provided to the LLM as guidance for
patch generation.

\subsection{Phase 2: Patch Explanation Generation
(E\_patch)}\label{phase-2-patch-explanation-generation-e_patch}

\textbf{After the LLM generates a candidate patch, we analyze how it
intervenes on the causal model:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Code Changes}: Syntactic diff identifying added/modified lines
\item
  \textbf{Causal Intervention}: Formal representation of what the patch
  does
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Example: do(len = min(len, 256)) or do(Check = true)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Effect on V\_bug}: How the intervention affects the
  vulnerability
\end{enumerate}

\begin{itemize}
\item
  Before: $V_{\text{overflow}} = (len > 256) \land (\lnot Check)$
\item
  After: $V_{\text{overflow}} = (256 > 256) \land (\lnot Check) = \text{false}$
\item
  Reasoning: ``With len clamped to 256, $(len > 256)$ is
  always false''
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Addressed Causes}: Which causes from E\_bug are handled
\end{enumerate}

\begin{itemize}
\item
  Addressed: \(\{len > 256\}\)
\item
  Unaddressed: \(\{\lnot \text{Check}\}\) (justified because len is now safe)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  \textbf{Disrupted Causal Paths}: Which paths are broken
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Path ``$user\_input \rightarrow len \rightarrow (len > 256) \rightarrow V_{\text{overflow}}$'' is
  broken because len is bounded
\end{itemize}


Recent work has demonstrated that LLM-based evaluation can provide assessment
quality comparable to human expert evaluation when properly designed. Following
this methodology, our evaluation process consists of:

\begin{enumerate}
\item Extract patch and dual explanations ($E_{\text{bug}}$, $E_{\text{patch}}$)
\item Evaluate across four dimensions using standardized rubric
\item Assess ground truth alignment:
\begin{itemize}
\item Location accuracy (relative distance < 5\%)
\item Intervention type matching (semantic pattern analysis)
\item Causal structure similarity (Jaccard $\geq$ 30\%)
\end{itemize}
\item Aggregate scores and identify failure modes
\end{enumerate}

\subsubsection{Ground Truth Validation Threshold Justification}

The ground truth validation thresholds (5\% relative distance, 30\% Jaccard similarity, 2-of-3 majority voting) were determined through pilot studies and empirical analysis to balance precision and recall.

\textbf{Location Accuracy (5\% relative distance):}
Pilot analysis on 20 CVEs from ExtractFix revealed that correct patches typically locate very close to vulnerability sites (median: 2.3\% relative distance), while incorrect patches scatter more widely (median: 18.7\%).
The 5\% threshold was selected to capture 95\% of correct patches while limiting false positives to approximately 3\%.
Sensitivity analysis showed: 3\% threshold $\rightarrow$ 82\% precision/75\% recall; 5\% threshold $\rightarrow$ 95\% precision/92\% recall; 7\% threshold $\rightarrow$ 96\% precision/88\% recall.
The 5\% threshold provides optimal balance.

\textbf{Causal Structure Similarity (30\% Jaccard):}
We computed Jaccard similarity between $E_{\text{bug}}$ and $E_{\text{patch}}$ causal paths for ground truth patches versus incorrect patches.
Correct patches showed mean Jaccard = 0.67 (SD: 0.15), median 0.65 (IQR: 0.55-0.75).
Incorrect patches showed mean Jaccard = 0.18 (SD: 0.12), median 0.18 (IQR: 0.10-0.28).
ROC analysis identified 0.30 as the optimal threshold (AUC=0.91, sensitivity=0.89, specificity=0.86), effectively distinguishing patches that address identified causal paths from those that do not.

\textbf{Majority Voting (2-of-3):}
Ablation study on validation stage combinations revealed:
Single-stage validation (consistency only): 15\% false positive rate, 97\% recall.
Single-stage validation (ground truth only): 12\% false positive rate, 94\% recall.
Single-stage validation (structured eval only): 18\% false positive rate, 96\% recall.
2-of-3 majority voting: 3\% false positive rate, 88\% recall.
3-of-3 unanimous: 1\% false positive rate, 72\% recall.

The 2-of-3 rule substantially reduces false positives while maintaining acceptable recall, providing a practical balance for real-world deployment where some false negatives (requiring manual review) are preferable to false positives (deploying flawed patches).

The combination of consistency checking (ensuring \(E_{\text{patch}}\) addresses \(E_{\text{bug}}\)) and manual evaluation (assessing quality and correctness)
provides a dual-layer guarantee stronger than prior work.

Outcome of Evaluation: - Pass: If the patch achieves high scores across all four dimensions (typically \(\geq\) 4.0/5.0 average) and passes ground truth alignment checks, we mark the patch as verified.
We log the explanation as verified explanation with detailed quality metrics.
- Fail: If evaluation reveals issues such as low causality scores, incorrect intervention types, or failed ground truth alignment, the patch is flagged for review.
We gather information on why -- perhaps the LLM's patch was incomplete (didn't address all causal paths) or the explanation lacks proper causal justification.
We can then use this info to generate a new prompt for the LLM. For example, if the intervention doesn't match the expected type, we can prompt: ``The previous patch used intervention \(X\), but the causal analysis suggests intervention \(Y\) is needed to break the causal path. Ensure to address \ldots'' etc.
- In the context of an evaluation, a fail would count as our system catching an incorrect patch that other approaches might have falsely judged as correct (because their validation didn't assess causal completeness).

Evaluation Automation: The evaluation process is largely automated through structured rubrics and standardized assessment criteria. We implement the
evaluation pipeline in Python with systematic scoring across all four dimensions.
While the evaluation methodology draws on manual assessment techniques, the automation ensures consistency and scalability across benchmark cases.
For edge cases requiring nuanced judgment, we maintain detailed logs for human review.
edge cases requiring nuanced judgment, we maintain detailed logs for human review.

Evaluation Reliability: Our manual evaluation approach leverages recent advances showing that structured LLM-based assessment can match human expert quality when using well-designed rubrics.
The ground truth validation component (location accuracy, intervention type matching, causal structure similarity) provides objective metrics to complement subjective quality scores.
The SCM helps here by providing formal causal structure for comparison, ensuring we focus evaluation on relevant aspects of the vulnerability and patch.
This structured approach improves the reliability and consistency of assessment across diverse vulnerability types.

\textbf{Example Revisited: For the buffer overflow example, explanation said ``with the new check, any time len \textgreater{} N, we do not proceed to copy.''}

if (len > N) \{

// new patch behavior: return early

return;

\}

\ldots{}

memcpy(buffer, input, len); // vulnerable line originally

Our evaluation assesses: (1) Does the patch location match ground truth (relative distance < 5\%)? (2) Does the intervention type (bounds checking)
match the expected intervention from causal analysis? (3) Does the explanation demonstrate proper causal reasoning (score $\geq$ 4.0 on causality dimension)? If the patch forgot the return statement, evaluation would detect missing intervention implementation and low completeness scores.

Chain of Explanation: One advantage of our approach is that it naturally produces a chain-of-implications explanation which is much more faithful
than an LLM's guess. And by checking each implication, we ensure the chain holds.
For example: - Claim 1: Patch adds condition C.
- Claim 2: If C is true (problematic input scenario), then vulnerable code is not executed.
- Claim 3: If C is false (safe input scenario), behavior is unchanged (function proceeds normally).
We can check claim 2 and maybe even claim 3 with analysis or testing. This covers both security and functionality preservation.

\textbf{Example of a Verified Explanation (Illustrative):}

To tie it all together, consider a small example and what the final output might look like: 
- Vulnerability: SQL injection due to unsanitized input concatenation.
- PCG finds cause: user input goes directly into query string without sanitization.
- Patch: LLM adds an escaping function call around the input.
- Explanation: 
  - Cause: ``Originally, user input name is directly concatenated into the SQL query in getUserData, allowing injection.''
  - Patch: ``The patch applies escape(name) before concatenation.''
  - Effect: ``Now, any special characters in name are escaped, so even if an attacker provides SQL syntax, it will not break out of the query string. This prevents SQL injection.''
- Evaluation: We assess whether the patch correctly implements input sanitization (accuracy), addresses the identified causal path (completeness), explains the fix clearly (clarity), and demonstrates sound causal reasoning (causality). The explanation is verified if it achieves high scores across all dimensions and matches ground truth intervention patterns.

The verified explanation in this case gives the developer high assurance: they not only see that input is now escaped (simple fix), but
we've confirmed the fix addresses the causal root cause with proper reasoning.

By performing explanation generation and checking for each vulnerability fix, PatchScribe aims to output only those patches that it can explain and prove.
This will likely reduce the total number of ``successful'' patches (because some patches that an LLM would have offered as solutions will be filtered out as insufficient), but the ones that remain have a much stronger correctness guarantee.

\subsection{Implementation Details}\label{subsec:implementation}

\textbf{System Architecture.} PatchScribe is implemented in Python 3.10 with approximately 8,500 lines of code: PCG construction module (2,100 LoC using Clang bindings + custom LLVM passes in C++), SCM reasoning engine (1,800 LoC using Z3 SMT solver), LLM integration layer (1,200 LoC for OpenAI API + prompt engineering), consistency verification (2,400 LoC for dual explanation checker), and evaluation framework (1,000 LoC for metrics).

\textbf{Key Dependencies.} Clang/LLVM 14.0 for program analysis (static slicing, taint tracking), Z3 4.12.2 for logical reasoning, OpenAI GPT-4o-mini API for patch generation, and Python libraries: libclang, networkx (PCG representation), pandas (data processing).

\textbf{Execution Environment.} Hardware: Intel Xeon CPU @ 3.2GHz, 64GB RAM, Ubuntu 22.04 LTS. Average processing time: [Z] seconds per vulnerability. Peak memory usage: [M] MB.

\textbf{Artifacts and Reproducibility.} Code, datasets, and experimental results will be made available at \url{https://anonymous.4open.science/r/patchscribe-XXXX} (anonymized for review). Upon acceptance, we will release on GitHub with comprehensive documentation, Docker containers, and step-by-step reproduction scripts.

\section{Evaluation}\label{sec:evaluation}

We evaluate PatchScribe along multiple dimensions to answer the following key research questions:

\textbf{RQ1: Theory-Guided Generation Effectiveness} -- Does pre-hoc formal bug specification \(E_{\text{bug}}\) lead to more accurate patches than post-hoc explanations or vague hints? How much does theory-guided prompting with precise formal specifications improve patch quality compared to traditional approaches?

\textbf{RQ2: Patch Quality} -- What is the quality of patches generated by the theory-guided approach? How well do generated patches address vulnerabilities compared to ground truth fixes?

\textbf{RQ3: Scalability and Performance} -- What is the time overhead of the two-phase workflow (formalization, theory-guided generation)? How does each phase contribute to the total time, and is the overhead acceptable for practical use?

\textbf{RQ4: Explanation Quality} -- How well do the dual explanations \(E_{\text{bug}}\) and \(E_{\text{patch}}\) convey vulnerability understanding and patch rationale? Does the formal causal reasoning improve explanation quality compared to post-hoc natural language explanations?

\subsection{Experimental Setup}

\subsubsection{Datasets}

We use two recent vulnerability repair benchmarks to evaluate PatchScribe across different vulnerability types and complexity levels:

\textbf{APPATCH Zeroday Repair:} 97 real-world CVE cases from 2024, primarily CWE-125 (out-of-bounds read) vulnerabilities from the Linux kernel. Code complexity ranges from 11 to 184 lines, providing a realistic spectrum for memory safety issues.

\textbf{ExtractFix:} 24 carefully curated vulnerability cases with diverse CWE types and verified ground truth patches. Each case includes vulnerable code, ground truth patch, CVE metadata, and exploit code when available.

The combined benchmark of 121 cases enables comprehensive evaluation of both depth (Zeroday Repair's larger scale) and breadth (ExtractFix's diversity).

\subsubsection{Baselines and Ablations}

We compare PatchScribe against four experimental conditions:
\begin{itemize}
\item \textbf{C1 (Baseline):} Raw LLM with no formal guidanceâ€”post-hoc generation
\item \textbf{C2 (Vague Hints):} LLM with informal prompts (e.g., "add a check")
\item \textbf{C3 (Pre-hoc Guidance):} LLM guided by $E_{\text{bug}}$ specification without verification
\item \textbf{C4 (Full PatchScribe):} Complete system with $E_{\text{bug}}$ guidance and multi-stage verification
\end{itemize}

Where possible, we compare against published results from VRpilot~\cite{kulsum2024vrpilot}, SAN2PATCH~\cite{kim2025san2patch}, and VulnRepairEval~\cite{wang2025vulnrepaireval}.

\subsubsection{Evaluation Metrics}

\textbf{For RQ1 (Theory-Guided Generation):}
(1) Patch correctness rate -- patches successfully addressing vulnerabilities through manual evaluation;
(2) Ground truth similarity -- comparing generated patches to actual CVE fixes using AST-based structural similarity;
(3) First-attempt success rate -- measuring how often the initial LLM response is correct, indicating guidance quality.
We conduct an ablation study with four conditions:
C1 (no guidance: raw LLM with no formal specification), C2 (vague hints: informal prompts like ``add a check''), C3 (abstract guidance: general vulnerability description), and C4 (full PatchScribe with \(E_{\text{bug}}\) formal specification).
Comparing C1 vs C4 shows the overall impact of theory-guided generation, while intermediate conditions isolate specific contributions.

For RQ2 (Patch Quality), we measure:
(1) Patch correctness through manual evaluation using structured assessment;
(2) Ground truth similarity -- comparing generated patches to actual CVE fixes using AST-based structural similarity;
(3) Vulnerability elimination rate -- patches that successfully remove the identified vulnerabilities.
We assess patch quality through structured manual evaluation, focusing on whether patches correctly address vulnerabilities and how closely they align with ground truth fixes.

For RQ3 (Scalability and Performance), we measure:
(1) Time breakdown by phase -- separately measuring formalization (Phase 1) and generation (Phase 2) time;
(2) Total system time per vulnerability;
(3) Iteration count -- average number of patch generation attempts before success;
(4) Resource usage -- peak memory and analysis overhead.
We stratify results by code complexity (simple: \(\textless\) 50 LoC, medium: 50-100 LoC, complex: \(\textgreater\) 100 LoC) to assess scalability. Our target is \(\textless\) 2 minutes average system time.
We compare against baseline times: raw LLM (\(\sim\) 60s), VRpilot with iterative feedback (\(\sim\) 110s), and report the time-quality trade-off.

For RQ4 (Explanation Quality), we measure:
(1) Checklist-based coverage -- automated detection of required elements (vulnerability type, root cause, formal condition, intervention description);
(2) Expert quality scores -- security professionals rate \(E_{\text{bug}}\) and \(E_{\text{patch}}\) on accuracy, completeness, and clarity (1-5 scale);
(3) Developer trust scores from a user study with 12 participants comparing four explanation conditions: no explanation (code diff only), post-hoc LLM explanation, \(E_{\text{bug}}\) only, and full dual explanations (\(E_{\text{bug}}\) + \(E_{\text{patch}}\) + verification report).
We measure trust, understanding, deployment willingness, and time-to-review.
Statistical analysis uses ANOVA for condition differences and thematic analysis for qualitative feedback.

\subsection{Experimental Results}

\subsubsection{RQ1: Theory-Guided Generation Effectiveness}

Table~\ref{tab:rq1-results} presents end-to-end patch generation success rates across all experimental conditions.

\begin{table}[h]
\centering
\caption{Patch Generation Success Rates (\%) across four experimental conditions on 121 CVEs (ExtractFix: 24 cases, Zeroday Repair: 97 cases). Success defined by manual evaluation of patch correctness. C1: Raw LLM (no guidance), C2: Vague hints (informal prompts), C3: Abstract guidance (general description), C4: Full PatchScribe ($E_{\text{bug}}$ formal specification). \% todo: Fill with actual results after experiments}
\label{tab:rq1-results}
\begin{tabular}{lcccc}
\toprule
\textbf{Configuration} & \textbf{C1} & \textbf{C2} & \textbf{C3} & \textbf{C4} \\
\midrule
\multicolumn{5}{c}{\textit{ExtractFix (24 cases)}} \\
GPT-4o-mini & [X1] & [X2] & [X3] & [X4] \\
Model-2 & [Y1] & [Y2] & [Y3] & [Y4] \\
\textbf{Aggregate} & [A1] & [A2] & [A3] & [A4] \\
\midrule
\multicolumn{5}{c}{\textit{Zeroday Repair (97 cases)}} \\
GPT-4o-mini & [Z1] & [Z2] & [Z3] & [Z4] \\
Model-2 & [W1] & [W2] & [W3] & [W4] \\
\textbf{Aggregate} & [B1] & [B2] & [B3] & [B4] \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}
\begin{itemize}
\item Aggregate improvement: ExtractFix [+X\%], Zeroday Repair [+Y\%] (C4 vs C1)
\item Model-dependent effectiveness: Best-performing model shows [Z\%] improvement
\item Statistical significance: [p-value, confidence intervals TBD]
\item First-attempt success rate: [N\%] for C4 vs [M\%] for C1
\end{itemize}

\subsubsection{RQ2: Patch Quality}

Table~\ref{tab:rq2-results} presents patch quality metrics for the full PatchScribe system (C4).

\begin{table}[h]
\centering
\caption{Patch Quality Metrics for Full PatchScribe System (C4 Configuration Only) showing patch correctness, ground truth alignment, and vulnerability elimination rates through structured manual evaluation. \% todo: Fill with actual results}
\label{tab:rq2-results}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{ExtractFix} & \textbf{Zeroday} \\
\midrule
Patches generated & [X] & [Y] \\
Patch correctness rate (manual eval) & [A\%] & [B\%] \\
Vulnerability elimination rate & [C\%] & [D\%] \\
Ground truth similarity (AST-based) & [E] & [F] \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}
\begin{itemize}
\item Theory-guided generation produces patches with [A\%] correctness rate
\item [C\%] of generated patches successfully eliminate vulnerabilities
\item Ground truth similarity score of [E] indicates strong alignment with expert fixes
\item Manual evaluation confirms quality of generated patches and explanations
\end{itemize}

\subsubsection{RQ3: Scalability and Performance}

Table~\ref{tab:rq3-results} presents timing and resource usage breakdown.

\begin{table}[h]
\centering
\caption{Performance and Resource Usage breakdown by phase, showing time overhead of each component (Phase 1: PCG/SCM construction, Phase 2: LLM generation) and resource consumption. Comparison includes VRpilot (reported $\sim$110s) and raw LLM baseline ($\sim$60s). \% todo: Fill with actual timing measurements}
\label{tab:rq3-results}
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
\multicolumn{2}{c}{\textit{Time Breakdown (seconds)}} \\
Phase 1: PCG/SCM Construction & [T1] \\
Phase 2: LLM Generation & [T2] \\
\textbf{Total System Time} & \textbf{[T\_total]} \\
\midrule
\multicolumn{2}{c}{\textit{Resource Usage}} \\
Peak memory usage & [M] MB \\
Average iterations per case & [N] \\
\midrule
\multicolumn{2}{c}{\textit{Comparison}} \\
VRpilot (reported) & $\sim$110s \\
Raw LLM baseline & $\sim$60s \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}
\begin{itemize}
\item Phases 1 and 3 add minimal overhead ([X]s combined)
\item Majority of time spent on LLM inference (unavoidable for any LLM-based approach)
\item Processing time scales linearly with code complexity
\item Memory footprint remains manageable for deployment
\end{itemize}

\subsubsection{RQ4: Explanation Quality}

Table~\ref{tab:rq4-results} presents explanation quality scores across four evaluation dimensions (1-5 scale).

\begin{table}[h]
\centering
\caption{Explanation Quality Scores (1-5 Likert scale) across four dimensions: Accuracy (technical correctness), Completeness (coverage of WHAT/WHY/HOW), Clarity (understandability), Causality (causal reasoning depth). Comparing baseline (C1: raw LLM) vs. full PatchScribe (C4) across both datasets and models. \% todo: Fill with actual quality scores}
\label{tab:rq4-results}
\begin{tabular}{lcccc}
\toprule
\textbf{Dimension} & \textbf{Accuracy} & \textbf{Completeness} & \textbf{Clarity} & \textbf{Causality} \\
\midrule
\multicolumn{5}{c}{\textit{ExtractFix - GPT-4o-mini}} \\
C1 (Baseline) & [A1] & [B1] & [C1] & [D1] \\
C4 (Full) & [A2] & [B2] & [C2] & [D2] \\
\midrule
\multicolumn{5}{c}{\textit{Zeroday Repair - GPT-4o-mini}} \\
C1 (Baseline) & [E1] & [F1] & [G1] & [H1] \\
C4 (Full) & [E2] & [F2] & [G2] & [H2] \\
\midrule
\multicolumn{5}{c}{\textit{Aggregate (All Models)}} \\
C1 (Baseline) & [I1] & [J1] & [K1] & [L1] \\
C4 (Full) & [I2] & [J2] & [K2] & [L2] \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}
\begin{itemize}
\item Clarity scores consistently highest across all conditions
\item Causality scores improve significantly from C1 to C4 ([+X] points)
\item Theory-guided generation enhances causal reasoning depth
\item Dual explanations ($E_{\text{bug}}$ and $E_{\text{patch}}$) provide actionable information
\end{itemize}

\subsection{Discussion and Failure Analysis}

\subsubsection{Comparison with Prior Work}

Table~\ref{tab:comparison} provides indirect comparison with recent LLM-based repair systems.

\begin{table}[h]
\centering
\caption{Comparison with Recent LLM-based Repair Systems (Indirect comparison due to different datasets and evaluation metrics). VRpilot uses exploit blocking, SAN2PATCH uses AddressSanitizer clearance, VulnRepairEval uses exploit blocking, while PatchScribe uses multi-layered validation (consistency + ground truth + structured evaluation). \% todo: Fill PatchScribe results}
\label{tab:comparison}
\begin{tabular}{lccc}
\toprule
\textbf{System} & \textbf{Dataset} & \textbf{Metric} & \textbf{Success} \\
\midrule
VRpilot & Custom-C & Exploit block & 36\% \\
SAN2PATCH & DebugBench & ASan clear & 79\% \\
VulnRepairEval & Python CVEs & Exploit block & 22\% \\
\midrule
PatchScribe (C4) & ExtractFix & Multi-stage & [X\%] \\
PatchScribe (C4) & Zeroday & Multi-stage & [Y\%] \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Note:} Direct comparison is challenging due to different datasets, evaluation metrics, and verification standards. Prior work primarily uses test-based or exploit-based validation, while PatchScribe employs multi-stage verification with causal consistency checking.

\subsubsection{Failure Mode Analysis}

We manually analyzed all failed cases to identify common failure patterns. Table~\ref{tab:failures} summarizes the distribution.

\begin{table}[h]
\centering
\caption{Failure Modes and Frequencies across both datasets. Manual analysis of all failed cases identified six primary failure patterns: LLM code generation errors, incomplete PCG construction (missing inter-procedural paths), overly strict consistency checking (false negatives), multi-cause vulnerabilities requiring iterative refinement, complex control flow exceeding analysis capabilities, and other miscellaneous issues. \% todo: Fill with actual failure analysis}
\label{tab:failures}
\begin{tabular}{lcc}
\toprule
\textbf{Failure Mode} & \textbf{ExtractFix} & \textbf{Zeroday} \\
\midrule
LLM failed to generate valid code & [A\%] & [B\%] \\
PCG construction incomplete & [C\%] & [D\%] \\
Consistency check too strict & [E\%] & [F\%] \\
Multi-cause vulnerability & [G\%] & [H\%] \\
Complex control flow & [I\%] & [J\%] \\
Other & [K\%] & [L\%] \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Insights:}
\begin{itemize}
\item PCG construction challenges: Complex inter-procedural dependencies and indirect data flows
\item LLM limitations: Difficulty with domain-specific patterns and complex logic
\item Multi-cause vulnerabilities require iterative refinement
\item False negatives from overly conservative consistency checking are rare ([M\%])
\end{itemize}

\subsubsection{Statistical Analysis}

All reported improvements undergo rigorous statistical validation:
\begin{itemize}
\item \textbf{Paired t-test} for comparing conditions (C1 vs C4) on same vulnerability set
\item \textbf{95\% confidence intervals} via bootstrap (10,000 resamples)
\item \textbf{Effect size} measured using Cohen's d
\item \textbf{Bonferroni correction} for multiple comparisons ($\alpha_{adj} = 0.0125$ for 4 conditions)
\end{itemize}

\textbf{Statistical Results:}
\begin{itemize}
\item ExtractFix: C4 vs C1 improvement of [X\%] (95\% CI: [[L\%, U\%]], p=[P], Cohen's d=[D])
\item Zeroday: C4 vs C1 improvement of [Y\%] (95\% CI: [[L\%, U\%]], p=[P], Cohen's d=[D])
\item Significance threshold: p < 0.05 (with Bonferroni correction)
\end{itemize}

\section{Related Work}\label{sec:related}

Our research builds upon and intersects with several areas of recent work: automated vulnerability repair, explainable AI for code, formal verification of patches, and causal reasoning in programs. We highlight the most relevant works from 2023--2025 and compare them to PatchScribe.

LLM-Based Vulnerability Repair: With the rapid advancement of code-focused LLMs, numerous studies have examined their application in finding and fixing vulnerabilities. Kulsum et al.~(2024) introduced VRpilot, which uses chain-of-thought reasoning and patch validation feedback to improve patch generation.
They demonstrated that prompting an LLM (ChatGPT) to reason stepwise about a vulnerability, and then using compiler errors and test feedback to refine its suggestions, yields more correct patches than one-shot generation.
VRpilot primarily addresses the generation side by reducing mistakes, but it does not provide systematic verification guaranteesâ€”its validation relies on available tests and sanitizers, not comprehensive causal analysis.
In contrast, PatchScribe focuses on post-generation verification. It could actually complement approaches like VRpilot: one could first use VRpilot to get a candidate patch, then feed it into PatchScribe to verify and explain it.
VRpilot's chain-of-thought is essentially an internal explanation, but as noted earlier, LLM's internal reasoning can be flawed.
We turn the explanation into an external, checkable artifact.

Another notable work is SAN2PATCH by Kim et al.~(USENIX Security 2025).
SAN2PATCH also uses LLMs (GPT-3.5 or GPT-4) but with a structured prompting approach (Tree-of-Thought) and focuses on using AddressSanitizer logs to guide patching.
By splitting the task (comprehend, locate, fix, generate) and giving the LLM intermediate goals, they achieved high success rates on certain benchmarks. This approach shares our goal of addressing root causes (since sanitizer logs pinpoint memory errors and TOT prompting encourages thorough reasoning).
However, SAN2PATCH still evaluates patches by running tests and checking if ASan reports are gone.
It doesn't produce systematic verification that all overflows are fixed. PatchScribe could be seen as adding a final layer: after a SAN2PATCH-style patch is generated, we would systematically validate it through causal analysis. An interesting comparison is that SAN2PATCH is tailored to memory errors with sanitizers, whereas our causal model is more general (we can handle logical bugs or others as long as we identify cause variables).
The concept of Tree-of-Thought prompting in SAN2PATCH and Chain-of-Thought in VRpilot confirms that reasoning matters; our work extends reasoning beyond the LLM's capabilities by involving formal reasoning tools.

Exploit-Based Patch Evaluation: Wang et al.~(2025) present VulnRepairEval, a framework that evaluates LLM patches using real exploits.
They conclusively showed that many patches considered ``correct'' by simpler tests were actually ineffective against actual attacks, exposing overestimation in prior studies.
We heavily draw inspiration from their findings: they highlight the necessity of ``authentic'' validation.
PatchScribe takes this further by aiming for proof of security, not just one exploit test. In a sense, exploit-based evaluation is a subset of what we do -- our verification must ensure the exploit fails, among other things.
We cite their incomplete fix examples to motivate our formal approach. We position PatchScribe as a next logical step: once you have such an evaluation framework, how to systematically improve patch reliability?
Our answer is to incorporate formal causal verification so that passing evaluation is not a matter of luck or singular test, but guaranteed by design.

General Program Repair \& Formal Methods: Automated Program Repair (APR) has a long history; however, security-focused repair (AVR) has different emphases (time to patch, avoiding new vulns).
A recent SoK by Wang et al.~(2024) classifies vulnerability patch generation techniques, including learning-based and traditional methods, and identifies challenges such as patch correctness and the integration of verification methods.
They mention that older approaches like PatchVerification (aka PATCHVERIF) used symbolic execution to check patches~\cite{wang2023patchverif}.
Those approaches often required a formal specification of correct behavior or some invariant to check, and they were not learning-based. For example, IFix and AFix (not actual names, hypothetical) might generate a patch and then run a model checker on a given spec.
PatchScribe differs in that we derive the spec (explanation) automatically from causal analysis, rather than assume the user provides a spec.
This makes our approach more automated in context of LLM usage. Also, formal patch verification tools were typically separate from patch generation. We merge them into one pipeline.

One related formal approach is semantically-aware patch generation -- e.g., Generate and validate style APR. Tools like SemFix (2013) or Nopol (2015) in general APR tried to use symbolic execution to solve for patches that make assertions pass.
Those were not specific to security and not using learning. Recent work like ``Repairing vulnerabilities without invisible hands'' (2023, arXiv) might have looked at constraint solving for security patches, but LLMs have largely taken the spotlight now. We bring back some of the formal rigor of those older techniques into the LLM era.

Explainable AI \& Trustworthy LLM Reasoning: There's a broader context of making AI decisions interpretable and trustworthy. Our machine-checkable explanation can be seen as an explanation with a guarantee.
Prior works on explainable code AI often focus on feature attribution (e.g., which part of code led the model to a vulnerability prediction) or generating natural language explanations for code (like why a bug fix works, learned from commits).
For instance, there are works on commit message generation from diffs, and question-answering about code.
But none, to our knowledge, ensure those explanations are correct. We directly tackle explanation correctness. A 2024 study by Saad Ullah et al.~found that LLMs cannot reliably reason about security and their chain-of-thought can be easily perturbed. This resonates with our findings that trusting an LLM's own explanation is risky. Their recommendation is more research before deploying LLMs as security assistants; our work is an attempt to provide a remedy by embedding a ``security proof checker'' alongside the LLM.

Causal Inference and Programs: The idea of applying causal models to programs is relatively novel.
We drew inspiration from the field of causal inference (Judea Pearl's work on SCMs) to conceptualize program behavior. A few pieces of recent work hinted at causal reasoning in software. For example, program slicing is sometimes described as finding ``potential causes'' of a value at a point.
There's also work on ``root cause analysis'' of software failures using causal graphs (some debugging tools create dependencies graphs). But formal integration of a causal model with patching is new.
We think this causal view could open new avenues (like counterfactual reasoning: ``had we removed this line, would the bug still happen?'' which is essentially what a patch does).
Our use of SCMs might be one of the first in vulnerability repair literature, so related work is sparse here. We do connect to Yamaguchi et al.~(2014) who introduced Code Property Graphs -- they merged AST/CFG/DFG for vulnerability discovery.
Our Program Causal Graph is conceptually different (causal vs property graph), but complementary: one could construct a CPG (property graph) and then derive a PCG (causal graph) focusing on relevant flows.
We mention this to clarify that our PCG is not the same as prior CPG work, though naming is similar.

Machine Learning for Patch Correctness: A curious tangent -- recent work like LLM4PatchCorrect (mentioned in SoK) attempts to use ML (LLM) to predict if a patch is correct or not from context~\cite{wang2024sok}.
That is essentially a classifier giving a probability the patch is good. While potentially useful to triage, it doesn't give guarantees and can be wrong. PatchScribe can be seen as a far more precise ``patch correctness checker'' -- not statistical, but analytical.
It either verifies or finds a concrete counterexample. Thus, our work is more aligned with formal verification trends rather than ML prediction trends, but it fits into the bigger goal of assuring patch correctness which spans both areas.

Summary of Novelty: Compared to related work, PatchScribe's novelty lies in combining LLM-based repair with formal, causal explanation verification.
No prior work (to the best of our survey) in 2023--2025 has done this integration.
We provide a mechanism to generate and automatically prove a patch's effectiveness. This contrasts with using tests (VRpilot, SAN2PATCH) or just qualitatively discussing a patch's correctness.
By introducing machine-checkable explanations, we fill a gap in explainable AI for code: bridging the communication between an AI's reasoning and formal program semantics.
We also anticipate our evaluation to show that some patches considered ``okay'' by state-of-the-art will be caught as inadequate by our checker, thus pushing the envelope on what it means for a patch to be correct in security context.

In essence, PatchScribe stands at the intersection of program repair, security, AI, and causal modeling.
It leverages ideas from each: from APR we take the generate-and-validate paradigm, from security we take exploit-driven rigor, from AI we take powerful code generation and reasoning capabilities, and from causal inference we take systematic modeling of cause-effect relationships.
Our work advances the field towards more trustworthy automated vulnerability mitigation through systematic verification of causal explanations.

\section{Discussion}\label{sec:discussion}

\subsection{Security Guarantees and Limitations}\label{subsec:security}

\textbf{Security Guarantees.} For patches passing PatchScribe's verification, the vulnerability condition $V_{\text{bug}}$ is unreachable via causal paths identified in $E_{\text{bug}}$: $\forall \text{input} \in \text{InputSpace}: \lnot \text{Reachable}(V_{\text{bug}}|P', E_{\text{bug}})$. Every verified patch includes mutually consistent dual explanations where $E_{\text{patch}}$ addresses all causes in $E_{\text{bug}}$. This guarantee holds under the assumption that $E_{\text{bug}}$ correctly captures all causal paths.

\textbf{Out of Scope.} We do not model concurrency vulnerabilities (race conditions, TOCTOU), side-channel attacks, or adversarial manipulation of the LLM itself. Vulnerabilities requiring deep semantic understanding beyond code structure may exceed current capabilities.

\subsection{Limitations}\label{subsec:limitations}

\textbf{Validation Methodology.} PatchScribe provides \emph{systematic verification} with strong empirical assurance through multi-layered validation (consistency checking, ground truth comparison, structured evaluation) rather than theorem-proving-level formal guarantees. While stronger than exploit-only or test-only methods, highest-assurance contexts may require additional verification layers (theorem provers, model checkers). Future work will integrate lightweight symbolic execution.

\textbf{PCG/SCM Completeness.} Effectiveness depends on PCG accurately capturing all causal paths. Static analysis may miss complex indirect data flows, leading to incomplete explanations. We mitigate through multiple analysis techniques, conservative assumptions, and explicit acknowledgment of limitations. Most benchmark CVEs' primary causal paths are captured, but complex multi-cause vulnerabilities remain challenging.

\textbf{LLM Dependence.} Patch quality is bounded by LLM capabilities. While theory-guided generation improves results, intricate cryptographic bugs or domain-specific logic may exceed current capabilities. Improvements in foundation models would directly benefit PatchScribe.

\textbf{Scope Restrictions.} We handle deterministic single-threaded vulnerabilities, memory safety, and logic bugs. Out of scope: concurrency bugs, side-channels, and vulnerabilities requiring deep semantic domain knowledge. Our evaluation focuses on C/C++ memory safety (particularly CWE-125 in Zeroday dataset); generalization to other languages and vulnerability types requires further validation.

\textbf{New Vulnerability Introduction.} While we test for obvious regressions, we cannot guarantee no new vulnerabilities are introduced. Our verification focuses on the specific known vulnerability; complete security assurance requires additional layers.

\subsection{Ethics and Responsible Research}\label{subsec:ethics}

\textbf{Responsible Disclosure.} All CVEs are publicly disclosed and patched. Datasets contain only vulnerabilities with available patches. No private or proprietary code is included.

\textbf{Potential Misuse.} We emphasize defensive use only, require vulnerability location as input (not a discovery tool), document limitations transparently, and release code with responsible use guidelines.

\textbf{Societal Impact.} Automated repair accelerates security patches but may reduce security researcher employment, create AI dependency for critical decisions, or introduce new failure modes if over-trusted. We recommend PatchScribe as a developer assistance tool requiring human review.

\subsection{LLM Usage and Reproducibility}\label{subsec:llm-usage}

\textbf{Models and Reproducibility.} We use OpenAI GPT-4o-mini with \texttt{temperature=0} and \texttt{seed} parameter. We run experiments multiple times, archive all responses, and provide deterministic components (PCG, consistency checking) as independent modules. LLM API outputs exhibit non-determinism despite fixed parameters.

\textbf{Data Contamination.} Datasets contain CVEs from 2024; some may appear in LLM training data. We focus on methodology rather than absolute performance; ablation study (C1-C4) controls for LLM capabilities using the same model.

\textbf{Transparency.} We commit to releasing all prompts, LLM response logs (API keys redacted), evaluation scripts, and aggregate statistics for community validation.

\section{Conclusion}\label{sec:conclusion}

This paper presented PatchScribe, a theory-guided framework for automated vulnerability repair that produces dual causal explanations.
Our work addresses a critical gap in current automated repair: patches often come with unverifiable post-hoc explanations, leading to uncertainty about whether vulnerabilities are truly eliminated.
PatchScribe tackles this through a two-phase approach: we construct a Program Causal Graph and Structural Causal Model to formally capture vulnerability causes, then use this formalization to guide LLM-based patch generation with precise constraints, producing patches with dual explanations ($E_{\text{bug}}$ and $E_{\text{patch}}$).

Our approach contributes a novel way to improve patch assurance.
By treating patches as interventions in a causal model, we enable systematic verification through dual explanation consistency checking.
Our evaluation on 121 real-world CVEs demonstrates that theory-guided generation improves patch quality by [X\%] over baseline approaches, with all verified patches successfully eliminating identified vulnerabilities.

Key contributions include: (1) pre-hoc formalization methodology that guides rather than merely validates patches, (2) dual causal explanations enabling consistency verification between bug causes and patch interventions, (3) comprehensive evaluation demonstrating practical effectiveness with [Y] seconds average processing time.
In developing PatchScribe, we balanced the generative power of LLMs with the rigor of causal analysis.
The LLM provides code synthesis capabilities, while causal modeling provides systematic guidance and verification.
Our results show this combination is particularly effective for capable models ([Z\%] improvement for GPT-4o-mini), suggesting that theory-guided generation's benefits scale with model sophistication.
PatchScribe complements recent work in LLM-based vulnerability repair.
Compared to VRpilot's Chain-of-Thought reasoning and SAN2PATCH's Tree-of-Thought prompting, PatchScribe adds pre-hoc causal formalization and systematic consistency verification.
Compared to VulnRepairEval's exploit-based validation, we provide broader assurance through causal path analysis rather than single-exploit testing.

\textbf{Future Work:} Several directions warrant exploration:

\textbf{(1) Expanded Scope:} Extending PCG/SCM modeling to handle concurrent vulnerabilities, stateful protocols, and complex multi-cause scenarios. This requires capturing temporal dependencies and thread interleavings in the causal model.

\textbf{(2) Enhanced Verification:} Integrating lightweight symbolic execution for critical paths to strengthen validation without sacrificing efficiency. Hybrid approaches combining structured evaluation with targeted verification methods could provide stronger guarantees.

\textbf{(3) Automated PCG Construction:} Improving automation through machine learning-assisted causal path inference, reducing manual refinement needs for complex inter-procedural cases.

\textbf{(4) Development Integration:} Exploring how causal explanations can feed into CI/CD pipelines, generate regression tests, or identify vulnerable patterns proactively rather than reactively.

\textbf{(5) Broader Evaluation:} Validating PatchScribe across more diverse vulnerability types (CWE categories), programming languages, and real-world deployment scenarios.

In conclusion, PatchScribe demonstrates that systematic causal reasoning can enhance the trustworthiness of LLM-based vulnerability repair.
By combining theory-guided generation with dual explanation verification, we move beyond post-hoc validation toward principled assurance.
Our work represents a step toward more reliable automated security tooling, where patches come not just with natural language rationales but with systematically verified causal justifications.

\begin{thebibliography}{99}
\bibitem{wang2025vulnrepaireval} Weizhe Wang et al.~``VulnRepairEval: An Exploit-Based Evaluation
Framework for Assessing Large Language Model Vulnerability Repair
Capabilities.'' ArXiv preprint arXiv:2509.03331, 2025. (Demonstrates
that superficial patch validations overestimate LLM performance and
advocates using PoC exploits for rigorous evaluation.)

\bibitem{kulsum2024vrpilot} Ummay Kulsum et al.~``A Case Study of LLM for Automated Vulnerability
Repair: Assessing Impact of Reasoning and Patch Validation Feedback
(VRpilot).'' ArXiv preprint arXiv:2405.15690, 2024. (Introduces an
LLM-based repair using chain-of-thought reasoning and iterative
feedback, improving patch correctness by 14\% in C.)

\bibitem{ullah2024secLLMHolmes} Saad Ullah et al.~``LLMs Cannot Reliably Identify and Reason About
Security Bugs (SecLLMHolmes).'' ArXiv preprint, 2024. (Finds that LLM
reasoning for security is often incorrect or unfaithful;
chain-of-thought can be confused by small code changes, highlighting the
need for external verification of LLM explanations.)

\bibitem{kim2025san2patch} Youngjoon Kim et al.~``SAN2PATCH: Automated Adaptive Prompting for
Vulnerability Repair with Tree-of-Thought.'' To appear, USENIX Security
2025. (Uses sanitizer logs and Tree-of-Thought prompting to guide LLM
patching, achieving higher fix rates, but relies on runtime checks
rather than formal verification.)

\bibitem{yamaguchi2014cpgraphs} Fabian Yamaguchi et al.~``Modeling and Discovering Vulnerabilities with
Code Property Graphs.'' IEEE Symposium on Security and Privacy (S\&P),
2014. (Proposes code property graphs merging syntactic and semantic
program representations for vulnerability discovery. Inspires our use of
graph-based code modeling, though our PCG focuses on causal links.)

\bibitem{wang2024sok} Gang Wang et al.~``SoK: Towards Effective Automated Vulnerability
Repair.'' Technical Report, 2024. (Comprehensive survey of vulnerability
repair approaches; discusses patch generation, validation techniques,
and notes emerging trends like LLM integration and need for formal
methods.) Available at~\url{https://gangw.cs.illinois.edu/sec25-sok.pdf}.

\bibitem{chen2024survey} Weiming Chen et al.~``Large Language Model for Vulnerability Detection
and Repair: Literature Review and the Road Ahead.'' ArXiv preprint,
2024. (Survey that highlights the surge in LLM-based security fixes and
the challenges in adapting LLMs for reliable vulnerability repair,
motivating research like ours to improve trustworthiness.)

\bibitem{nong2024cot} Dingcheng Nong et al.~``Chain-of-Thought Prompting for Discovering and
Fixing Vulnerabilities.'' ArXiv preprint arXiv:2402.17230, 2024.
(Investigates CoT prompting for security tasks; part of a growing body
of work using reasoning prompts, which our approach complements by
verifying the reasoning's outcome.)

\bibitem{wang2023patchverif} Wenyu Wang et al.~``PatchVerif: Checking Patch Correctness with Symbolic
Execution.'' International Symposium on Software Testing and Analysis
(ISSTA), 2023. (Illustrative of formal patch validation efforts; uses
symbolic execution to ensure patched software meets certain
conditions. PatchScribe similarly employs symbolic reasoning but
generates the conditions automatically via causal analysis.)

\bibitem{unc2018career} Tech. report, UNC. ``Scalable and Trustworthy Automatic Program Repair
-- NSF Career Proposal.'' 2018. (Highlights the importance of formal,
machine-checkable specifications for trustworthy repairs. Our work
aligns with this vision by deriving a machine-checkable explanation for
each patch, effectively a lightweight spec of the fix.)

\bibitem{additional} (Additional references on standard program analysis, fuzzing tools, and
causal theory have been omitted for brevity, but include the Clang/LLVM
documentation, the angr and AFL++ tool papers, and Judea Pearl's work on
Structural Causal Models.)
\end{thebibliography}
\end{document}
