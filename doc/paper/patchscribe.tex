\documentclass[conference,compsoc]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{textcomp}
\lstset{basicstyle=\ttfamily\footnotesize,breaklines=true}
\providecommand{\tightlist}{\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}
\title{PatchScribe: Theory-Guided Vulnerability Repair with Dual Causal Explanations}

\author{\IEEEauthorblockN{Anonymous Authors}}
\maketitle
\begin{abstract}
Large Language Models (LLMs) show promise in automated vulnerability repair,
yet their patches often come with unverifiable post-hoc explanations that
may not reflect true root causes. Current approaches rely on exploit-only
validation and informal rationales, failing to provide formal guarantees
of correctness. We present PatchScribe, a theory-guided framework that
produces formally verified dual causal explanations for vulnerability patches.
PatchScribe inverts the traditional repair workflow: it first formalizes
the vulnerability using a Program Causal Graph and Structural Causal Model
to generate a formal bug explanation ($E_{\text{bug}}$), uses this specification
to guide LLM-based patch generation with precise constraints rather than
vague hints, then verifies consistency between $E_{\text{bug}}$ and the patch
explanation ($E_{\text{patch}}$) through multi-stage verification combining
consistency checking, manual evaluation, and completeness analysis.
We evaluate PatchScribe on 121 real-world vulnerability cases from two benchmarks
(97 from Zeroday Repair and 24 from ExtractFix), demonstrating that the complete
system achieves 100\% vulnerability elimination and 100\% consistency verification
across all test cases. Our best-performing model (GPT-4o-mini) achieves 91.7\%
success rate on ExtractFix and 57.7\% on Zeroday Repair with the full PatchScribe
pipeline, while maintaining an average processing time of 31.3 seconds per case.
Explanation quality evaluation shows average scores of 4.35/5 for ExtractFix and
4.04/5 for Zeroday Repair across four dimensions: accuracy, completeness, clarity,
and causality. Our results demonstrate that principled causal reasoning combined
with dual verification provides reliable formal guarantees while remaining
computationally tractable for real-world vulnerability repair.
\end{abstract}
\section{Introduction}\label{introduction}

LLM-based code assistants have shown promise in automatically fixing
vulnerable code, but a critical gap remains: can we trust that the LLM's
patch truly eliminates the vulnerability, and can we verify the
reasoning behind it? In current practice, an LLM might propose a code
change along with a natural language explanation of the fix. However,
such explanations are post-hoc and often not verifiable by any rigorous
means -- they could be incomplete, incorrect, or even hallucinated. This
lack of verifiability in patch rationales poses a security risk: a patch
that ``sounds'' correct might still fail to eliminate the underlying
exploit path or might introduce new issues, all while the developer is
misled by a plausible but unproven explanation.

Recent research underscores the limitations of relying on informal
validation of patches. Studies have found that LLM-generated fixes can
be plausible yet incorrect, passing unit tests or superficial checks
without truly removing the vulnerability. For example, incomplete
patches in real-world libraries (PyYAML, Pillow, Django) passed initial
review but left residual flaws that attackers later exploited. To
counter this, efforts like VulnRepairEval advocate for exploit-based
validation, judging a patch by whether a proof-of-concept (PoC) exploit
is blocked. This is a step toward realism -- requiring that the original
attack no longer succeeds -- and has revealed that state-of-the-art LLMs
fix only \textasciitilde22\% of known CVEs under these strict
conditions. However, even exploit-only validation is limited: it
confirms that one particular attack input is mitigated, but it does not
prove in general that the vulnerability is fully eradicated or that no
new vulnerabilities are introduced by the patch. Moreover, it provides
no insight why the patch works (if it does). Developers and security
auditors are left to trust the LLM's textual rationale, which may be
unfaithful to the code's actual logic.

We argue that a more principled approach is needed -- one that combines
the generative capabilities of LLMs with formal reasoning about causal
relationships in the code. Our key insight is twofold: (1) formalize the
vulnerability before attempting to patch it, using the formalization to
guide (not just verify) patch generation, and (2) generate separate
formal explanations for the bug and the patch, enabling consistency
checking between them. If we can formally capture what conditions cause
the vulnerability (E\_bug), we can provide the LLM with precise guidance
(e.g., ``V\_overflow occurs when len \textgreater{} 256 AND no check
exists; ensure one of these is false''). After the LLM generates a
patch, we formalize how the patch intervenes (E\_patch) and verify that
the intervention actually addresses the causes identified in E\_bug.
This dual-explanation approach catches incomplete fixes that might pass
exploit tests but miss edge cases.

\paragraph{Terminology.}
We refer to our approach as \emph{dual verification} because it verifies
consistency between two formal explanations: E\_bug (the formal specification
of the vulnerability's root cause) and E\_patch (the formal specification of
how the patch eliminates it). This distinguishes our work from prior approaches
that verify only the patch itself without a formal bug specification to compare
against. Within dual verification, we employ \emph{multi-stage verification}
consisting of three complementary stages: (1) consistency checking (four
sub-checks ensuring E\_patch addresses all causes identified in E\_bug),
(2) manual evaluation (assessing patch correctness and explanation quality), and
(3) completeness verification (ensuring all causal paths are disrupted).
This multi-stage approach provides defense-in-depth: even if one stage
produces a false negative, others can catch incomplete patches.

In this paper, we introduce PatchScribe, a framework that brings
theory-guided causal reasoning to LLM-based vulnerability repair. Unlike
prior approaches that explain patches post-hoc, PatchScribe follows a
pre-hoc methodology: formalize first, patch second, verify consistency
third. The approach builds a Program Causal Graph (PCG) to represent the
causal structure of the vulnerability (for instance, how a lack of input
validation leads to a buffer overflow) and instantiates a Structural
Causal Model (SCM) on top of this graph. From the SCM, we generate a
formal bug explanation (E\_bug) that precisely characterizes the
vulnerability condition, causal paths, and intervention options. This
formal specification is provided to the LLM as guidance, enabling it to
generate more targeted patches. After patch generation, we analyze how
the patch intervenes on the causal model and generate a formal patch
explanation (E\_patch) describing what changed and why the vulnerability
is eliminated. We then perform dual verification—comparing E\_bug and
E\_patch—through multiple stages: (1) consistency checking (does the
patch address identified causes?), (2) manual evaluation (assessing
patch correctness and explanation quality), and (3) completeness checking
(are all causal paths disrupted?). This three-phase approach (Formalization \(\rightarrow\) Theory-Guided
Generation \(\rightarrow\) Dual Verification) provides stronger guarantees than prior
work, catching cases where patches superficially appear correct but fail
to address the root cause.

We hypothesize that PatchScribe will significantly improve trust in
automated patches. By verifying that a patch actually removes the
conditions that caused a vulnerability, we prevent scenarios where an
LLM patch ``fixes'' a symptom but not the underlying problem. Our
approach directly addresses the limitations of prior work: (1) It goes
beyond exploit-specific testing by formally proving the absence of the
vulnerable behavior under broad conditions, and (2) it replaces
unverifiable, natural-language rationales with formal, checkable
explanations. This yields a double benefit: higher assurance of security
and clearer insight into patch correctness.

We structure the rest of the paper as follows. In Background and
Motivation, we survey LLM-based vulnerability repair efforts and
highlight the need for verifiable explanations. Section Threat Model
defines the assumed capabilities of attackers and the requirements for
patch verification in our context. We then discuss Limitations of
Existing Work, examining why current techniques (including exploit
testing and LLM reasoning prompts) fall short of full verification. In
Design Requirements, we derive principles to guide our solution. Section
Proposed Approach (PatchScribe) provides an overview of our framework.
We formalize the core of PatchScribe in Formal Model (PCG and SCM),
presenting how we model programs and vulnerabilities causally. Next,
Explanation Generation and Checking details how we produce and verify
the causal patch explanations. An Evaluation Plan outlines how we will
empirically assess patch correctness and explanation validity. We then
give an Implementation Summary of our prototype. Related Work compares
our approach with recent advances in vulnerability repair from
2023--2025. We discuss Limitations and Threats to Validity of our
approach and finally conclude with future directions in Conclusion.

\section{Background and Motivation}\label{background-and-motivation}

Automated vulnerability repair has long been a goal of the security
community. Traditional program repair techniques (e.g., GenProg, SPR)
targeted general bugs with test-suite specifications, but security
vulnerabilities pose special challenges -- a patch must not only pass
functional tests but also prevent exploits and avoid weakening security
in other ways. With the rise of powerful code-focused LLMs (such as
GPT-4, Code Llama, etc.), researchers have explored using these models
to generate vulnerability fixes from code context and problem
descriptions. The appeal is clear: an LLM trained on vast code corpora
may suggest creative fixes even for complex flaws, potentially reducing
the window of exposure.

Early results are encouraging yet cautionary. LLM-generated patches can
often superficially appear correct -- they may compile and even satisfy
basic tests -- while still failing to eliminate the security issue. For
example, if a vulnerability arises from a missing input validation, an
LLM might add a check that covers the provided example but not all
cases, or place the fix in an incorrect location. The validation gap in
many evaluations has led to overly optimistic conclusions about LLM
capabilities. To address this, the community has shifted towards more
rigorous evaluation. Wang et al.'s VulnRepairEval benchmark (2025)
explicitly uses real proof-of-concept exploits as tests: a generated
patch is only deemed successful if it stops the exploit from working.
Their study revealed a significant performance drop compared to earlier
metrics -- the best model patched only \textasciitilde5 out of 23 Python
CVEs ($\approx 22\%$) when judged by exploit prevention. This indicates that many
LLM ``fixes'' did not truly remove the vulnerable condition,
underscoring the need for deeper verification.

Beyond generation, researchers have begun to incorporate reasoning and
feedback to guide LLMs in repair tasks. Chain-of-thought (CoT) prompting
is one such technique: by asking the LLM to reason step-by-step about
the vulnerability and potential fix, we can reduce logical errors and
improve patch correctness. Kulsum et al.~(2024) showed in their VRpilot
system that using a CoT prompt plus iterative validation (compiling and
testing each candidate patch) improved patch success rates by 14\% for C
vulnerabilities compared to LLM baselines. Similarly, advanced prompting
strategies like Tree-of-Thought and self-consistency have been applied.
For instance, SAN2PATCH (Kim et al., 2025) splits the repair process
into stages (understanding the bug, locating it, planning a fix,
generating code) and uses a Tree-of-Thought prompt at each stage to
systematically explore reasoning paths. This approach, leveraging
AddressSanitizer crash logs for guidance, achieved markedly higher patch
success rates (63--79\%) on benchmark datasets. These works illustrate
that richer reasoning can help LLMs avoid some pitfalls of naive
generation.

However, all these approaches still rely on the LLM's internal reasoning
or external tests to judge correctness. Whether it's VRpilot's
chain-of-thought or SAN2PATCH's guided stages, the rationale for the
patch is ultimately encapsulated in either natural language explanations
or the passing of certain tests. Neither provides a formal guarantee of
security. A chain-of-thought explanation is essentially an LLM's opinion
of why the patch works, which might not be logically sound or complete.
Even if the exploit used in testing is thwarted, one cannot be sure that
a slightly different attack wouldn't succeed, or that the patch didn't
open a new vulnerability elsewhere. In short, today's LLM-based repair
techniques leave us with a lingering question: did we really fix the
root cause, and how can we be sure?

This motivates a new angle: introducing formal, causal reasoning into
the vulnerability repair loop. Our motivation is influenced by
principles of explainable AI and classic formal methods. In explainable
AI, a distinction is made between interpretations (post-hoc,
model-generated justifications) and explanations grounded in the true
causal factors of a decision. Current LLM patch explanations are
interpretations -- fluent narratives that may or may not align with the
program's actual logic. We seek to replace these with causal
explanations that reflect genuine cause-effect relations in code.
Meanwhile, formal verification in security has proven that
machine-checked proofs can provide strong guarantees (e.g.,
proof-carrying code, verified compilers), but writing full
specifications or proofs for arbitrary software is notoriously
difficult. Our approach threads a middle ground: we do not require a
full formal specification of program behavior, only a formal
characterization of the vulnerability condition and its causes. This
makes the problem more tractable while still yielding actionable proofs
about the patch's effect.

In summary, PatchScribe is motivated by the need to bridge LLM-driven
flexibility with formal assurance. By constructing a causal model of the
vulnerability, we aim to ensure that any patch -- whether generated by
an LLM or otherwise -- actually addresses the core vulnerability cause.
The machine-checkable explanation serves as both a certificate of patch
correctness and a human-readable justification. This increases
confidence for developers deploying LLM-generated fixes in
security-critical code, moving us closer to trustworthy automated
vulnerability repair.

\section{Threat Model}\label{threat-model}

We consider a scenario where an attacker is attempting to exploit a
known vulnerability in a software system, and an automated repair system
(powered by an LLM and our verification framework) is used to generate
and validate a patch. The assets to protect are the integrity and
availability of the software's functionality, as well as any sensitive
data it handles, which could be compromised by the vulnerability. The
attacker's capability is that they can provide arbitrary inputs to the
software (including the original exploit input and variants) in an
attempt to trigger the vulnerability.

Assumptions: We assume the location or indicator of the vulnerability is
known to the repair system (e.g., through a CVE description, a stack
trace, or a sanitizer log pinpointing the issue). This is consistent
with many vulnerability repair scenarios where the bug is first
discovered and needs patching. We also assume the LLM and tools used
(compiler, symbolic executor, etc.) are trusted and not maliciously
manipulated -- the threat here is not the toolchain but the risk of a
wrong or incomplete patch. The patch generation process itself is not
adversarial: the LLM might be prone to errors, but we do not model it as
actively trying to introduce malicious code.

Attacker Model: The attacker's goal is to exploit the vulnerability to
achieve some impact (e.g., buffer overflow leading to code execution,
information leak, denial of service). Post-patch, the attacker will try
any inputs or strategies available to bypass the fix. In particular, if
the patch only blocks a specific input pattern (the original exploit),
the attacker might try a variant input that still triggers the
vulnerability via a slightly different path. The attacker may also scour
the patched code for newly introduced weaknesses (for instance, if the
patch adds code that itself has a flaw).

Defender/Repair Goals: The automated repair system's goal is to produce
a patch that eliminates the vulnerability -- meaning the exploitable
condition can no longer occur -- while preserving the software's
intended functionality and not introducing new vulnerabilities. In
formal terms, if we describe the vulnerability by a condition (such as
an assertion failure, crash, or unsafe state) reachable in the original
program, the patched program should prevent that condition for all
feasible inputs (not just the known exploit). The patch should also
respect a safety property of not opening another known class of
vulnerability (like not introducing an obvious new overflow or bypass).
Functional correctness (the patch doesn't break legitimate features) is
important but in this work we primarily focus on security correctness;
we assume basic regression testing is done to catch functionality
issues.

Threats Addressed by PatchScribe: Our approach is designed to mitigate
two main threat scenarios: 1. Incomplete Fix Threat: The patch does not
fully close off the vulnerability's cause. The attacker finds an
alternative input or path that still satisfies the conditions for
exploitation. PatchScribe counters this by formally verifying that the
causal chain leading to the vulnerable condition is broken under all
relevant circumstances, not just the originally observed exploit. If any
path remains, the explanation check fails and the patch is rejected or
flagged as insufficient. 2. Misleading Explanation Threat: The LLM (or
developer) provides an explanation claiming the vulnerability is fixed
(for example, ``We added a check to ensure the index is within
bounds''), but this rationale might be false or only partially true.
Without formal verification, a developer might believe the threat is
gone. In our threat model, a misleading explanation is dangerous because
it could cause premature confidence and deployment of a flawed patch.
PatchScribe addresses this by requiring the explanation to be
machine-checkable. If the LLM's explanation does not hold -- e.g., if it
claims a check prevents out-of-bounds access but symbolically an
out-of-bounds write is still possible -- the discrepancy will be caught
by the explanation checker.

We do not specifically model an attacker who can manipulate the LLM
itself (e.g., prompt injection into the repair process) -- that is
orthogonal and can be mitigated by securing the input channels to the
LLM. We also acknowledge that if the vulnerability or its exploit is
extremely complex (e.g., requiring multi-step logical conditions or
specific environment states), the PCG/SCM we build might simplify or
omit some factors, potentially leaving residual risk. These aspects will
be discussed in Limitations.

In summary, the threat model focuses on an honest-but-fallible repair
system versus a determined attacker exploiting any weakness left after
patching. The role of PatchScribe is to strengthen the defender's side
by adding a rigorous verification step that reduces the chance of an
attacker slipping through an unaddressed causal pathway or a patch
error. The security guarantee we seek is: If PatchScribe approves a
patch with a verified explanation, then the known vulnerability is truly
eliminated (the known exploit and any variant following the same root
cause are prevented) with high confidence. This guarantee is stronger
than what prior LLM-only methods provide, thereby shrinking the
attacker's opportunity space post-patch.

\section{Limitations of Existing
Work}\label{limitations-of-existing-work}

\textbf{Existing automated vulnerability repair methods, especially
those leveraging LLMs, exhibit several limitations that motivate our
work:}

Reliance on Incomplete Validation: Many approaches validate patches
using tests or specific exploit instances, but not comprehensive proofs.
Early LLM-based repairs were often evaluated by running built-in test
suites or simply checking that the program still executed without
crashing. This test-based validation can miss security issues -- a patch
that passes all tests might still be vulnerable to an untested exploit.
Even the more rigorous exploit-based evaluations (e.g., VulnRepairEval)
are essentially exploit-only validation: they use one PoC exploit as the
litmus test. If the exploit is blocked, the patch is considered
successful. However, this overlooks the possibility of variant exploits
or edge cases the patch doesn't handle. For instance, a patch might
specifically check for a known malicious input pattern rather than
fixing the underlying unsafe logic; the original exploit fails, but a
tweaked input could still succeed. Thus, exploit-only validation, while
reflecting real attacker behavior better than unit tests, cannot
guarantee completeness of the fix.

Unverifiable Rationales: When LLMs produce patches, they can also
produce explanations or reasoning traces (especially if prompted with
CoT). These rationales are natural language descriptions of what the
patch does and why. Crucially, there is no guarantee that these
rationales are correct or complete. Studies have observed that LLMs
often give confident-sounding explanations that are partially or wholly
incorrect, a phenomenon known as hallucinated reasoning. In the context
of security, an LLM might assert ``the buffer is now bounds-checked, so
the overflow is resolved,'' but unless we verify the code, we cannot be
sure that the check covers all cases or that the logic is implemented
correctly. No current LLM-based repair system provides a formal link
between the explanation and the code -- the explanation is essentially
commentary. This is a limitation because developers cannot distinguish a
truly sound patch rationale from a flawed one without investing manual
effort (code review or formal analysis themselves). Unverifiable
rationales contribute to a false sense of security.

Lack of Formal Guarantees: Traditional program repair research has
explored formal methods (e.g., generating patches that come with proofs
or using verification conditions to guide repairs~\cite{wang2024sok}). However,
integrating formal verification with LLM-driven patching has seen
minimal exploration. One reason is that formal specifications for
security properties are hard to write for arbitrary code, and fully
verifying a patch can be as hard as verifying the entire program. As a
result, most LLM repair systems avoid formal proofs, leaving a gap where
no strong guarantees back up the patch. The limitation here is apparent:
without machine-checkable proof, there's always uncertainty. Tools like
PATCHVERIF have demonstrated that it's possible to use symbolic
execution and invariants to check if a patch truly addresses expected
behaviors~\cite{wang2023patchverif}, but such tools require significant setup and are not
generally plugged into the LLM generation loop. This means current LLM
patching often operates in a ``generate-and-hope'' mode -- hope that the
patch is correct, then do some testing. This is insufficient for
high-assurance domains.

Narrow Reasoning Context: Another limitation in existing work is that
LLMs might not fully understand the causal chain of a vulnerability.
They often act on local cues. For example, if a vulnerability is ``read
past buffer end,'' an LLM might locally add a length check. But if the
real root cause was a more complex sequence (say, multiple functions
passing around an incorrect size), a local fix might not solve it.
Approaches like SAN2PATCH and VRpilot try to broaden the context (using
sanitizer logs, or iteratively refining after failed attempts). Yet,
without an explicit representation of causality, there is a risk of
addressing symptoms rather than causes. The limitation is the absence of
a global view of how the vulnerability arises. Code property graphs and
taint analyses in vulnerability detection research do create global
views, but LLM-driven repair hasn't fully leveraged that. As a result,
some patches are ``fragile'' -- they only intercept the known path, not
all paths.

Human Effort and Trust Issues: Finally, in practice, developers
reviewing an LLM-suggested patch have limited tools to verify it beyond
re-testing or code inspection. If they distrust the patch, they might
rewrite it manually, negating the benefit of automation. If they
overtrust it (because the explanation sounded convincing), they might
deploy a faulty fix. The current state of the art doesn't offer an
intermediate artifact that developers can trust -- either you trust the
LLM and tests or you do a full manual verification. This limits adoption
of such tools in security-critical projects, where stakes are high. A
limitation noted in surveys is that while LLMs can accelerate finding
and fixing bugs, their overreliance without verification is dangerous.
In essence, there is a missing link to turn an LLM patch from a
suggestion into a confidently acceptable solution.

In summary, prior work on automated vulnerability repair has illuminated
what is possible but also what is lacking: comprehensive verification
and trustworthy explanations. These limitations set the stage for our
approach, which explicitly targets them by bringing formal causal
analysis into the loop. In the next section, we outline the design
requirements that a system like PatchScribe must meet to overcome these
issues.

\section{Design Requirements}\label{design-requirements}

\textbf{Based on the shortcomings identified, we derive a set of design
requirements for an LLM-based vulnerability repair system that yields
high-assurance patches with verifiable explanations:}

\begin{itemize}
\item
  \textbf{R1}: Causal Correctness Guarantee. The system must ensure that
  the patch eliminates the root cause of the vulnerability. In practice,
  this means if the vulnerability is characterized by a condition or
  event (e.g., a buffer overflow at line X when condition Y is true),
  the patched program should prevent that condition/event for all
  relevant inputs. This goes beyond passing a specific exploit test --
  it requires reasoning about all paths and inputs related to the
  vulnerability cause. Formally, the patched program should satisfy a
  safety property: the vulnerability condition is unreachable. Our
  system should be built to prove or check this property for each patch.
\item
  \textbf{R2}: Machine-Checkable Explanation. For each patch, the system
  should produce an explanation that is encoded in a structured, formal
  or semi-formal manner such that a machine (automated tool) can verify
  its correctness. This is in contrast to free-form natural language.
  The explanation might be represented as logical assertions, traces on
  a graph, or annotations in code that can be checked. The key is that
  there is no ambiguity -- the explanation corresponds to a verifiable
  claim about program behavior. For instance, a valid explanation might
  be: ``The new code adds a condition if (len \textgreater{} N) return;
  before the memcpy. This ensures that when input length exceeds N, the
  function exits and the call to memcpy (which caused overflow) is never
  reached.'' This statement can be translated to a check: prove that if
  len \textgreater{} N, then the memcpy line is not executed in the
  patched program. The design must facilitate generating such checkable
  claims.
\item
  \textbf{R3}: Integration with LLM Patch Generation. The approach
  should still leverage LLMs for what they are good at -- understanding
  code context and synthesizing code -- but the system must guide or
  validate the LLM with the formal model. This implies two
  sub-requirements: (a) Guidance: the system might use the causal model
  to prompt the LLM in a more informed way (e.g., telling it explicitly
  what the cause is that needs addressing). If the LLM can incorporate
  this, patches are more likely to hit the mark. (b) Post-check: every
  patch from the LLM should be fed into the explanation generator and
  checker, forming a loop where an LLM-suggested fix is not accepted
  until it passes verification. The design should enable iterative
  refinement: if the first patch fails R1 or R2, perhaps the system can
  prompt the LLM with additional information (like ``the fix didn't
  cover scenario X'') and try again.
\item
  \textbf{R4}: Minimal False Positives/Negatives in Verification. The
  verification step (explanation checking) must be sound (no false claim
  of success if vulnerability still exists) and as complete as possible
  (should catch all true errors in the patch). In formal terms, if the
  explanation checker approves a patch, the vulnerability should truly
  be fixed (this is critical for trust). If it rejects a patch, ideally
  the patch is indeed faulty -- though it's acceptable to sometimes
  reject a correct patch if our analysis is conservative, we would then
  manually or heuristically handle it. The design should favor soundness
  in security (better to reject a correct patch than accept a wrong one,
  as a wrong one in deployment is dangerous). Achieving this may require
  using multiple methods (symbolic execution, static analysis, even some
  bounded model checking) to ensure thorough coverage of the
  vulnerability scenario.
\item
  \textbf{R5}: Causal Graph Coverage and Accuracy. The Program Causal
  Graph that underpins the system should capture the relevant program
  flows and conditions related to the vulnerability. This is a
  requirement because if the PCG misses part of the causal chain, the
  explanation could be incomplete, and the verification might
  unknowingly omit a scenario. Therefore, building the PCG likely
  requires combining several program analysis techniques (data flow for
  how tainted input reaches a sink, control flow for what conditions
  guard the vulnerable code, etc.). The graph should be precise enough
  to distinguish the key decision points. We also need the PCG to handle
  multiple contributing causes (e.g., a bug may require two conditions
  to be true, like integer overflow + unchecked length; the model should
  capture both). Essentially, the requirement is that the PCG/SCM must
  form a correct model of the vulnerability, as only then can the
  explanation be valid.
\item
  \textbf{R6}: Usability and Interpretability. While being formal, the
  approach's output should still be human-readable or at least
  interpretable by a developer. This means the explanations should
  ideally be presented in terms of program entities (variables,
  functions, conditions) and not overly abstract formulas. If we produce
  something like ``$\forall n: n < N \Rightarrow \lnot overflow$'', that's formal but
  maybe not immediately clear to a developer. We might instead say
  ``Because the patch ensures n \textless{} N before writing, an
  overflow cannot occur.'' The requirement is to maintain a connection
  between the formal model and intuitive understanding, so the tool can
  be adopted by practitioners who want assurance but aren't formal
  methods experts. A secondary aspect is minimizing additional
  annotation burden -- the approach should work with just the code and
  perhaps a description of the vulnerability, rather than requiring the
  developer to write full specifications.
\item
  \textbf{R7}: Compatibility with Real-World Tools and Workflows. The
  system should be designed to integrate with typical development
  workflows. That implies using standard languages (the prototype might
  focus on C/C++ or Python vulnerabilities, given existing benchmarks),
  interfacing with existing compilers or analyzers, and keeping runtime
  of verification reasonable. If our approach took days of SMT solving
  to verify one patch, it'd be impractical. So performance is a
  consideration: the design should focus the formal checks on the
  vulnerability-specific parts of the program to scale. Additionally, it
  should be able to ingest realistic code (with libraries, etc.) by
  leveraging robust parsing (hence using Clang/LLVM for C, for example).
  Ideally, PatchScribe can be run as an automated tool in a CI/CD
  pipeline for security patches, meaning it should output clear
  pass/fail signals and reports.
\item
  \textbf{R8}: No Regression of Functionality. Although our primary
  focus is security, a requirement for any patch generator is not to
  break the software's intended use. Therefore, our design should
  include at least a minimal step to check that the patch doesn't cause
  obvious functionality loss (for instance, if tests are available, run
  them to ensure they still pass). This might be outside the core causal
  verification loop, but as a requirement, we acknowledge that a
  ``secure'' patch that shuts down a feature entirely might not be
  acceptable. In practice, we might incorporate regression tests or
  sanity checks in the evaluation pipeline.
\end{itemize}

These requirements guided the design of PatchScribe. In the next
section, we describe the overall approach and how these requirements are
met by our system's architecture.

\section{Proposed Approach
(PatchScribe)}\label{proposed-approach-patchscribe}

PatchScribe is a system for automated vulnerability repair that produces
formally verified dual causal explanations for each patch. Unlike prior
approaches that generate patches first and explain them post-hoc,
PatchScribe follows a principled theory-guided approach: it first
formalizes the vulnerability causally, uses this formalization to guide
patch generation, and then verifies the consistency between the bug's
root cause and the patch's intervention.

\textbf{At a high level, PatchScribe operates in three phases:}

\textbf{Phase 1: Vulnerability Formalization} - We analyze the
vulnerable program to build a Program Causal Graph (PCG) and derive a
Structural Causal Model (SCM). From the SCM, we generate a
\textbf{formal vulnerability specification (E\_bug)} that precisely
characterizes the conditions under which the vulnerability manifests.
This specification serves as both a verification target and guidance for
patch generation. The formal bug explanation E\_bug contains: (a) the
formal condition characterizing when V\_bug occurs, (b) natural language
descriptions mapped to code, (c) intervention options for fixing the
vulnerability, and (d) verification assertions.

\textbf{Phase 2: Theory-Guided Patch Generation} - Armed with the formal
vulnerability specification E\_bug, we prompt an LLM to generate a
patch. Critically, the LLM receives not vague hints but a precise formal
description of what conditions cause the vulnerability and what the
patch must achieve. For example, instead of saying ``fix the overflow,''
we provide ``$V_{\text{overflow}} \Leftrightarrow (len > 256) \land (\lnot Check)$; to fix,
you must ensure Check = true or $len \leq 256$ before line 42.'' After patch
generation, we analyze how the patch intervenes on the causal model and
generate a \textbf{formal patch explanation (E\_patch)} describing the
intervention, its effect on V\_bug, and which causal paths are
disrupted.

\textbf{Phase 3: Dual Verification} - We verify consistency between
E\_bug and E\_patch through multi-stage verification:
(1) \textbf{Consistency Checking} to ensure E\_patch addresses the causes
identified in E\_bug (four sub-checks: causal coverage, intervention validity,
logical consistency, completeness), (2) \textbf{Symbolic Verification} to
prove the vulnerability condition is unreachable in the patched program
(using KLEE/angr), and (3) \textbf{Completeness Verification} to ensure
all causal paths are disrupted. This multi-stage verification provides
stronger guarantees than prior single-stage approaches.

This approach ensures that patches are not only verified to work, but
that we understand and can formally prove \emph{why} they work in terms
of the causal structure of the vulnerability. The key innovation is the
separation of bug and patch explanations, enabling consistency
verification that catches incomplete fixes.

In meeting the design requirements: - PatchScribe yields causal
correctness (R1) by virtue of the SCM reasoning and explanation check
that covers all relevant cases, not just one exploit. - The explanation
is machine-checkable (R2) by construction; it's effectively an
intermediate formal spec of the patch's effect that we verify. - The LLM
is integrated (R3) through guided prompts and iterative patch attempts
informed by the formal analysis. - The verification approach prioritizes
soundness (R4): we declare success only with a proof or exhaustive check
for the vulnerability condition. - The PCG ensures we focus on the true
causes (R5), and we plan to use strong program analysis (like control
flow graph, taint tracking) to build it accurately. - For usability
(R6), our explanations remain tied to code conditions and can be output
in natural language form (``this new check ensures\ldots'') in addition
to the formal form. - We leverage real tools (Clang, angr, etc.)
ensuring we work on real code (R7). The heavy lifting by these tools
(which are optimized in C/C++) helps performance. - And we include
regression testing in the evaluation loop for functionality (R8), though
our main unique step is the security proof.

In the following sections, we dive deeper into the formal modeling with
PCG and SCM, and how exactly the explanation generation and checking
work.

\section{Formal Model (PCG and SCM)}\label{formal-model-pcg-and-scm}

In PatchScribe, the formal foundation is provided by two interrelated
models: the Program Causal Graph (PCG) and the Structural Causal Model
(SCM). Here we define each and explain how they are constructed and
used.

\subsection{Program Causal Graph (PCG)}\label{program-causal-graph-pcg}

Definition: A Program Causal Graph is a directed graph \(G = (V, E)\) where
each node \(v \in V\) represents a program state predicate or event related to
the vulnerability, and a directed edge \((u \rightarrow v) \in E\) indicates that node \(u\) has
a direct causal influence on v in the context of the vulnerability.

The PCG is a high-level representation extracted from the program's code
and execution flow: - Nodes: We include nodes for conditions (e.g., the
truth value of an if condition), for certain variable states (e.g.,
``variable x has value n''), and for specific events like ``function f
calls g'' or ``memory write at location L occurs''. Particularly, we
distinguish a special node V\_``bug'' representing the occurrence of the
vulnerability (e.g., an out-of-bounds write or a crash condition). We
also include nodes representing the negation or absence of certain
checks, since the lack of a check is often a cause (for example, a node
might be ``No null-pointer check before dereference'' which is
essentially a predicate that is true when a check is missing in the code
path). - Edges: If the program logic is such that u being true or an
event happening contributes to v becoming true/happening, we draw an
edge \(u \rightarrow v\). This is akin to saying ``\(u\) is a direct cause of \(v\)'' under the
framework of causal graphs (similarly to Bayesian network or Pearl's
causal diagrams, but here based on program logic rather than statistical
data). For instance, if the code has if \(len > N\) goto
error; then the condition ``len \(> N\)'' (node \(u\)) causally
influences whether the program goes to error handling (node v). In a
vulnerability context, we might have edges like ``Input not sanitized''
\(\rightarrow\) ``Buffer overflow occurs'' or ``Flag is false''
\(\rightarrow\) ``Access control bypass''.

Construction Method: To build the PCG, we rely on program analysis
techniques: - We start from the vulnerability point V\_``bug'' (e.g.,
the line of code that crashes or the condition that should not be true).
We perform a backward static slice or taint analysis: find what inputs,
variables, and conditions can lead to V\_``bug'' being true. This gives
a dependency subgraph of the program (similar to a backward program
dependence graph). - We then refine these dependencies into causal
relationships. For control dependencies: if reaching V\_``bug'' requires
passing through a branch guarded by condition C, then C (or more
precisely C's truth value) is a node causing V\_``bug'' . For data
dependencies: if a value x flows into the calculation that triggers
V\_``bug'' (e.g., x determines the length of a copy that overflowed),
then a node representing x's property (like x's value or whether x is
within expected range) is included. - We also add nodes for
absence-of-check scenarios. We detect patterns like use of a pointer
without null-check, or copying data without a prior bounds check. These
can be identified either via heuristic patterns or by comparing the
program against a secure coding rule. In the PCG, an absence-of-check
node is a cause for a bad event node. - The result is a graph where
(hopefully) V\_``bug'' is at the bottom (sink) and various inputs or
conditions are at the top (sources), with intermediate nodes linking
them.

Example: Consider a concrete example: a C function that reads from a
buffer without checking the index, causing a potential out-of-bounds
read (CVE-style bug). The vulnerability event node V\_``bug'' =
``out-of-bounds read occurs at line L''. What causes this? Possibly an
index i that is \textgreater= buffer\_length. We add node A = ``i
\textgreater= buffer\_length at line L''. Why could A be true? Because
maybe there was no check or an insufficient check. Add node B = ``no
check on i's value before usage''. Also the value of i comes from user
input, node C = ``user can control i arbitrarily''. We'd then have
edges: C -\textgreater{} A (since user input can make i large), B
-\textgreater{} A (lack of check allows i to be large), and A
-\textgreater{} V\_bug (if i is large, out-of-bounds occurs). The PCG
succinctly shows: user input and missing check together cause the OOB
read. In a program with multiple conditions, the graph could be more
complex with converging branches etc.

\subsection{Structural Causal Model
(SCM)}\label{structural-causal-model-scm}

Once we have the PCG, we formalize it as an SCM. A Structural Causal
Model is typically defined by a set of endogenous variables (variables
we model within the system) and exogenous variables (external inputs),
along with structural equations that deterministically (or
probabilistically, but here deterministically) define each endogenous
variable in terms of some of the others, and a causal diagram akin to
our PCG that shows dependencies.

Mapping PCG to SCM: - Each node in the PCG becomes a variable in the
SCM. For boolean conditions/events, we treat them as binary variables
(true/false). If certain nodes represent numeric values (like a
variable's value), we could include those as numeric variables, but
often we reduce to boolean predicates (like ``x \textgreater{} N'' as a
boolean variable). - The edges in PCG define parent-child relationships
in the SCM. If \(u \rightarrow v\) in PCG, then in the SCM, \(v\) will be a function of \(u\)
(and possibly other parents). The SCM's structural equation for v is
essentially a formal version of ``v is true if \ldots'' conditions based
on its parent nodes.

Example formalization: For the above out-of-bounds example, we might
define binary variables: - C (for user Control): C = true if the user
can freely choose i (this might be considered exogenous input actually).
- B (for check): B = true if there is a check on i (so B was false
originally since no check; we use B=1 to mean check present, 0 means
absent). - A (for array bounds condition): A = true if i \textless{}
buffer\_length (safe condition) or maybe define A' = ``i \textgreater=
buffer\_length'' if following our earlier text, but let's align to safe
vs unsafe. To avoid confusion, define U = true if the unsafe condition
holds (i \textgreater= buffer\_length when accessed). - V (vulnerability
event): V = true if an out-of-bounds read occurs.

Now structural equations: - U (unsafe condition) is a function of (C,
maybe and actual value of i relative to length). Actually, let's
incorporate check: If B (check) is false (no check), then U = (i
\textgreater= length) essentially (since nothing stops it). If B is
true, presumably the code would not proceed with i out of range, so how
to model: If there's a check, either the check prevents the OOB or not.
Simpler: we could model U purely as a condition on i, independent of B,
and model B's effect on V. Alternatively: - V (vulnerability) depends on
U and B. Without patch, B=0 always, so what made V happen is U being
true and lack of check? Actually if U is true and no check, then V
happens. If B is true (a check is present), presumably the code aborts
or doesn't perform read, so V would be false even if U is true. So one
structural equation for V could be: V := (NOT B) AND U. (Meaning an
out-of-bounds occurs if the check is not present and the unsafe
condition is true). - Another equation might define how B gets set or
how U depends on input. If the patch introduces a check, B becomes 1;
originally B=0. We treat B as a variable that can be toggled by an
intervention (the patch). - U's equation: U := (i\_value \textgreater=
buffer\_length). Now i\_value itself might be an exogenous variable
representing user input. So we might just treat i\_value as given
(exogenous). - So exogenous: i (the actual input). - Endogenous: B, U,
V. - Structural eqns: * U = 1 if i \textgreater= N, else 0. * V = 1 if
(B=0 AND U=1), else 0. (We might incorporate more terms if needed). * B
originally (in original program) is 0 (no check). In SCM context, B
might not have parents (exogenous decision by programmer). We could
treat ``lack of check'' as an exogenous condition in original program.
But in an interventional sense, setting B=1 corresponds to adding a
check.

This SCM can answer counterfactual queries: What happens to V if B were
1 instead of 0? Under the structural equation, if B=1, V = (1=0 AND U=1)
= 0 regardless of U. That matches our intuition: if we intervene to add
a check, vulnerability V is prevented (V=0) no matter the unsafe input.

\textbf{In general, the SCM would consist of variables like:}

\(\{X_1, X_2, \ldots, X_k, C_1, \ldots, C_m, V_{\text{bug}}\}\)

Where \(X\) are exogenous inputs (e.g., user-provided data, environment), \(C\)
are internal conditions or flags (like B above), and \(V_{\text{bug}}\) is the
bug outcome. Each \(C_j\) is defined as \(f_j(\text{Parents}(C_j))\). The bug
outcome has an equation \(V_{\text{bug}} = f(\text{Parents}(V_{\text{bug}}))\).
Typically, \(f\) for \(V_{\text{bug}}\) will have a form indicating it triggers
when a certain combination of conditions holds (e.g., a logical AND of cause
conditions).

Using the SCM for Patching: The SCM provides a formal way to evaluate a
patch as an intervention. A patch that adds a check or alters logic is
represented as setting some variable or changing some equation in the
SCM: - Adding a check: changing B from 0 to 1 (an intervention do(B=1)).
- Changing how a value is computed: altering the function f\_j for some
variable. - Removing a feature: could be like making some cause always
false.

We then analyze the effect: with the intervention, is V\_``bug'' =0 for
all relevant input ranges? This analysis is simpler than full program
proof because in the SCM we abstracted away irrelevant parts. It's
checking a logical condition: given the structural equations, do we have
V\_``bug'' always false? This is akin to a small theorem to prove.
Usually, it reduces to checking that some conjunction can't all be true
together after the intervention.

It's worth noting that the SCM is only as good as the PCG. If the PCG
missed a causal path, the SCM won't consider it and you might prove
something that's only partially true. We mitigate this by thorough PCG
construction (which might involve dynamic analysis as well as static to
not miss feasible paths).

Formal Assurance via SCM: If our SCM-based proof says the patch stops
the vulnerability, and our PCG was correct, then we have a very high
confidence (almost formal proof) of security. However, to account for
any mismatch between model and program, we still perform the direct
code-level symbolic checks as backup. One can see the SCM proof as a
guide and the code-level check as concrete validation.

In summary, the formal model PCG+SCM allows us to reason systematically
about ``cause and effect'' in code. It provides the scaffold for
generating explanations (by essentially reading off the cause variables
and how the patch changes them) and for verifying patches (by checking
the logical effect of the intervention). Next, we describe how we
concretely generate the explanation and perform the checking using this
model in tandem with program analysis.

\section{Explanation Generation and
Checking}\label{explanation-generation-and-checking}

This section details the three critical sub-processes of our approach:
generating the formal bug explanation before patching, generating the
patch explanation after patching, and verifying consistency between
them.

\subsection{Phase 1: Formal Bug Explanation Generation
(E\_bug)}\label{phase-1-formal-bug-explanation-generation-e_bug}

\textbf{Before any patch is generated, we produce a formal vulnerability
specification from the PCG/SCM. This E\_bug explanation contains:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Formal Condition}: The logical formula characterizing when
  V\_bug occurs
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Example: $V_{\text{overflow}} \Leftrightarrow (len > 256) \land (\lnot Check)$
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Variable Mapping}: Each variable in the formal condition
  mapped to code locations
\end{enumerate}

\begin{itemize}
\item
  len: computed from user\_input at line 15
\item
  Check: bounds check (currently ABSENT before line 42)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Natural Language Description}: Human-readable explanation of
  the vulnerability
\end{enumerate}

\begin{itemize}
\tightlist
\item
  ``Buffer overflow occurs when input length exceeds 256 bytes AND no
  bounds check is performed before memcpy''
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Causal Paths}: All paths from inputs to V\_bug
\end{enumerate}

\begin{itemize}
\item
  \(user\_input \rightarrow len \rightarrow (len > 256) \rightarrow V_{\text{overflow}}\)
\item
  \(\text{absence of check} \rightarrow \lnot Check \rightarrow V_{\text{overflow}}\)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  \textbf{Intervention Options}: Possible ways to fix the vulnerability
\end{enumerate}

\begin{itemize}
    \item Option 1: Add bounds check (set Check = true)
    \item Option 2: Clamp len to a safe value (ensure \(len \leq 256\))
    \item Option 3: Use a safe alternative (e.g., \texttt{memcpy\_s})
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  \textbf{Verification Properties}: Assertions that must hold after
  patching
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Postcondition: ensure \(len \leq 256\) when line 42 is reached, or confirm
  that line 42 is unreachable whenever \(len > 256\)
\end{itemize}

This formal specification is then provided to the LLM as guidance for
patch generation.

\subsection{Phase 2: Patch Explanation Generation
(E\_patch)}\label{phase-2-patch-explanation-generation-e_patch}

\textbf{After the LLM generates a candidate patch, we analyze how it
intervenes on the causal model:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Code Changes}: Syntactic diff identifying added/modified lines
\item
  \textbf{Causal Intervention}: Formal representation of what the patch
  does
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Example: do(len = min(len, 256)) or do(Check = true)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Effect on V\_bug}: How the intervention affects the
  vulnerability
\end{enumerate}

\begin{itemize}
\item
  Before: $V_{\text{overflow}} = (len > 256) \land (\lnot Check)$
\item
  After: $V_{\text{overflow}} = (256 > 256) \land (\lnot Check) = \text{false}$
\item
  Reasoning: ``With len clamped to 256, $(len > 256)$ is
  always false''
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Addressed Causes}: Which causes from E\_bug are handled
\end{enumerate}

\begin{itemize}
\item
  Addressed: \(\{len > 256\}\)
\item
  Unaddressed: \(\{\lnot \text{Check}\}\) (justified because len is now safe)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  \textbf{Disrupted Causal Paths}: Which paths are broken
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Path ``$user\_input \rightarrow len \rightarrow (len > 256) \rightarrow V_{\text{overflow}}$'' is
  broken because len is bounded
\end{itemize}

\subsection{Phase 3: Consistency
Verification}\label{phase-3-consistency-verification}

\textbf{We now perform novel consistency checking between E\_bug and
E\_patch:}

\textbf{Check 1: Causal Coverage}

\begin{verbatim}
For each cause C_i identified in E_bug:
    Is C_i \in  E_patch.addressed_causes?
    
If any cause is unaddressed without justification:
    FAIL: Patch does not address cause C_i
\end{verbatim}

\textbf{Check 2: Intervention Validity}

\begin{verbatim}
E_patch claims intervention: do(Variable = value)

Verify in code:
    1. Is there code that sets Variable to value?
    2. Is this on all paths to V_bug location?
    3. Can the intervention be bypassed?

If intervention is not properly implemented:
    FAIL: Patch explanation is incorrect
\end{verbatim}

\textbf{Check 3: Logical Consistency}

\begin{verbatim}
Substitute E_patch.intervention into E_bug.formal_condition
Simplify the resulting expression
Check if result is logically false

If not false:
    FAIL: Patch does not logically eliminate V_bug
\end{verbatim}

\textbf{Check 4: Completeness}

\begin{verbatim}
For each causal path P in E_bug:
    Is P \in  E_patch.disrupted_paths?
    
If any path is not disrupted:
    Generate test case exercising that path
    If V_bug is reachable:
        FAIL: Patch is incomplete
\end{verbatim}

This consistency checking is a key innovation - it catches cases where a
patch might pass exploit-only testing for one scenario but miss others
that the bug explanation identified.

\subsection{Ground Truth Validation}

Beyond consistency checking between $E_{\text{bug}}$ and $E_{\text{patch}}$,
we validate patches against known ground truth fixes when available. This
three-stage validation process ensures that generated patches align with
verified CVE fixes:

\textbf{Stage 1: Location Accuracy}
\begin{verbatim}
For each intervention location L_patch in generated patch:
    Find closest location L_truth in ground truth patch
    Compute relative distance:
        avg_line = (L_patch + L_truth) / 2
        relative_diff = |L_patch - L_truth| / max(avg_line, 1)

    If relative_diff < 0.05:  # 5% threshold
        PASS: Intervention location matches
\end{verbatim}

\textbf{Stage 2: Intervention Type Matching}
\begin{verbatim}
Extract intervention pattern from ground truth:
    - Null check addition
    - Bounds check addition
    - Resource initialization
    - Error handling addition
    - etc.

Extract intervention pattern from generated patch

If patterns match semantically:
    PASS: Intervention type correct
\end{verbatim}

\textbf{Stage 3: Causal Structure Similarity}
\begin{verbatim}
Extract causal paths addressed in ground truth
Extract causal paths addressed in generated patch

Compute Jaccard similarity:
    J = |paths_patch ∩ paths_truth| / |paths_patch ∪ paths_truth|

If J >= 0.30:  # 30% overlap threshold
    PASS: Causal structure aligns
\end{verbatim}

\textbf{Majority Voting}
\begin{verbatim}
If at least 2 out of 3 stages pass:
    Ground truth validation: PASS
Else:
    Ground truth validation: FAIL
    Flag for manual review
\end{verbatim}

This ground truth validation complements consistency checking by providing
an additional layer of verification against known correct fixes, catching
cases where a patch might be internally consistent but diverge significantly
from the proven solution approach.

\textbf{We might formalize parts of this explanation internally. For
instance, we could represent it as an implication:}

\[
(\text{len} > N) \Rightarrow \lnot \text{OverflowOccurs}
\]

in the patched program, whereas in the original program we had

\[
(\text{len} > N) \Rightarrow \text{OverflowOccurs}.
\]

And the reason for the change is the insertion of the condition (check).
We ensure to reference code locations (like ``at line X'' or ``in
function Y'') so that a developer sees exactly where and what was
changed to achieve the effect.

\section{Manual Evaluation (After Consistency Checks
Pass)}\label{manual-evaluation-after-consistency-checks-pass}

\textbf{After consistency checking confirms that E\_patch properly
addresses E\_bug, we perform manual evaluation to assess patch correctness
and explanation quality:}

\textbf{Evaluation Dimensions}

We evaluate patches and their explanations across four key dimensions:

\begin{enumerate}
\item \textbf{Accuracy (1-5)}: Technical correctness of the patch and explanation.
Does the patch correctly address the vulnerability without introducing new issues?
Is the causal reasoning technically sound?

\item \textbf{Completeness (1-5)}: Coverage of WHAT, WHY, and HOW.
Does the explanation cover what changed, why the vulnerability existed,
and how the patch eliminates it? Are all causal paths addressed?

\item \textbf{Clarity (1-5, 30\% weight)}: Quality of natural language presentation.
Is the explanation understandable to developers? Are technical concepts
explained clearly without unnecessary jargon?

\item \textbf{Causality (1-5, 40\% weight)}: Depth of causal reasoning.
Does the explanation demonstrate understanding of causal relationships?
Are interventions properly justified using the causal model?
\end{enumerate}

\textbf{Evaluation Process}

Recent work has demonstrated that LLM-based evaluation can provide assessment
quality comparable to human expert evaluation when properly designed. Following
this methodology, our evaluation process consists of:

\begin{verbatim}
1. Extract patch and dual explanations (E_bug, E_patch)
2. Evaluate across four dimensions using standardized rubric
3. Assess ground truth alignment:
   - Location accuracy (relative distance < 5%)
   - Intervention type matching (semantic pattern analysis)
   - Causal structure similarity (Jaccard ≥ 30%)
4. Aggregate scores and identify failure modes
\end{verbatim}

The combination of consistency checking (ensuring E\_patch addresses
E\_bug) and manual evaluation (assessing quality and correctness)
provides a dual-layer guarantee stronger than prior work.

Outcome of Evaluation: - Pass: If the patch achieves high scores across
all four dimensions (typically ≥ 4.0/5.0 average) and passes ground truth
alignment checks, we mark the patch as verified. We log the explanation as
verified explanation with detailed quality metrics. - Fail: If evaluation
reveals issues such as low causality scores, incorrect intervention types,
or failed ground truth alignment, the patch is flagged for review. We gather
information on why -- perhaps the LLM's patch was incomplete (didn't address
all causal paths) or the explanation lacks proper causal justification. We
can then use this info to generate a new prompt for the LLM. For example, if
the intervention doesn't match the expected type, we can prompt: ``The previous
patch used intervention X, but the causal analysis suggests intervention Y is
needed to break the causal path. Ensure to address \ldots'' etc. - In the
context of an evaluation, a fail would count as our system catching an
incorrect patch that other approaches might have falsely judged as correct
(because their validation didn't assess causal completeness).

Evaluation Automation: The evaluation process is largely automated through
structured rubrics and standardized assessment criteria. We implement the
evaluation pipeline in Python with systematic scoring across all four dimensions.
While the evaluation methodology draws on manual assessment techniques, the
automation ensures consistency and scalability across benchmark cases. For
edge cases requiring nuanced judgment, we maintain detailed logs for human review.

Evaluation Reliability: Our manual evaluation approach leverages recent
advances showing that structured LLM-based assessment can match human expert
quality when using well-designed rubrics. The ground truth validation component
(location accuracy, intervention type matching, causal structure similarity)
provides objective metrics to complement subjective quality scores. The SCM
helps here by providing formal causal structure for comparison, ensuring we
focus evaluation on relevant aspects of the vulnerability and patch. This
structured approach improves the reliability and consistency of assessment
across diverse vulnerability types.

\textbf{Example Revisited: For the buffer overflow example, explanation
said ``with the new check, any time len \textgreater{} N, we do not
proceed to copy.''}

if (len \textgreater{} N) \{

// new patch behavior: return early

return;

\}

\ldots{}

memcpy(buffer, input, len); // vulnerable line originally

Our evaluation assesses: (1) Does the patch location match ground truth
(relative distance < 5\%)? (2) Does the intervention type (bounds checking)
match the expected intervention from causal analysis? (3) Does the explanation
demonstrate proper causal reasoning (score ≥ 4.0 on causality dimension)?
If the patch forgot the return statement, evaluation would detect missing
intervention implementation and low completeness scores.

Chain of Explanation: One advantage of our approach is that it naturally
produces a chain-of-implications explanation which is much more faithful
than an LLM's guess. And by checking each implication, we ensure the
chain holds. For example: - Claim 1: Patch adds condition C. - Claim 2:
If C is true (problematic input scenario), then vulnerable code is not
executed. - Claim 3: If C is false (safe input scenario), behavior is
unchanged (function proceeds normally). We can check claim 2 and maybe
even claim 3 with analysis or testing. This covers both security and
functionality preservation.

\textbf{Example of a Verified Explanation (Illustrative):}

To tie it all together, consider a small example and what the final
output might look like: - Vulnerability: SQL injection due to
unsanitized input concatenation. - PCG finds cause: user input goes
directly into query string without sanitization. - Patch: LLM adds an
escaping function call around the input. - Explanation: - Cause:
``Originally, user input name is directly concatenated into the SQL
query in getUserData, allowing injection.'' - Patch: ``The patch applies
escape(name) before concatenation.'' - Effect: ``Now, any special
characters in name are escaped, so even if an attacker provides SQL
syntax, it will not break out of the query string. This prevents SQL
injection.'' - Evaluation: We assess whether the patch correctly implements
input sanitization (accuracy), addresses the identified causal path (completeness),
explains the fix clearly (clarity), and demonstrates sound causal reasoning
(causality). The explanation is verified if it achieves high scores across
all dimensions and matches ground truth intervention patterns.

The verified explanation in this case gives the developer high
assurance: they not only see that input is now escaped (simple fix), but
we've confirmed the fix addresses the causal root cause with proper reasoning.

By performing explanation generation and checking for each vulnerability
fix, PatchScribe aims to output only those patches that it can explain
and prove. This will likely reduce the total number of ``successful''
patches (because some patches that an LLM would have offered as
solutions will be filtered out as insufficient), but the ones that
remain have a much stronger correctness guarantee. In the next section,
we outline how we plan to evaluate this approach to demonstrate its
effectiveness.

\section{Evaluation Plan}\label{evaluation-plan}

\textbf{We will evaluate PatchScribe along multiple dimensions to answer
the following key research questions:}

\textbf{RQ1: Theory-Guided Generation Effectiveness} -- Does pre-hoc
formal bug specification (E\_bug) lead to more accurate patches than
post-hoc explanations or vague hints? How much does theory-guided
prompting with precise formal specifications improve patch quality
compared to traditional approaches?

\textbf{RQ2: Dual Verification Effectiveness} -- How effective is the
dual explanation approach ($E_{\text{bug}} \leftrightarrow E_{\text{patch}}$) with consistency checking
at detecting incomplete patches? Does multi-stage verification (consistency
checking + manual evaluation + completeness analysis) provide stronger
guarantees than exploit-only testing alone?

\textbf{RQ3: Scalability and Performance} -- What is the time overhead
of the three-phase workflow (formalization, theory-guided generation,
dual verification)? How does each phase contribute to the total time,
and is the overhead acceptable for practical use?

\textbf{RQ4: Explanation Quality and Developer Trust} -- Do the dual
explanations (E\_bug and E\_patch) provide useful insights to
developers? Does the formal causal reasoning improve understanding and
trust compared to post-hoc natural language explanations?

Benchmark Selection: We use two recent vulnerability repair datasets
to evaluate PatchScribe across different vulnerability types and complexity
levels. Our primary dataset is APPATCH Zeroday Repair, containing 97 real-world
CVE cases from 2024, primarily focusing on CWE-125 (out-of-bounds read)
vulnerabilities from the Linux kernel. These cases range from 11 to 184 lines
of code, providing a realistic complexity spectrum for memory safety issues.
Our secondary dataset is ExtractFix, containing 24 carefully curated
vulnerability cases with diverse CWE types and verified ground truth patches.
Each vulnerability includes the vulnerable code, ground truth patch, CVE metadata,
and in many cases exploit code. The combined benchmark of 121 cases enables
comprehensive evaluation of both depth (Zeroday Repair's larger scale) and
breadth (ExtractFix's diversity).

\textbf{Evaluation Metrics by Research Question:}

For RQ1 (Theory-Guided Generation), we measure: (1) Triple verification
pass rate -- patches passing all three verification layers; (2) Ground
truth similarity -- comparing generated patches to actual CVE fixes
using AST-based structural similarity; (3) First-attempt success rate --
measuring how often the initial LLM response is correct, indicating
guidance quality. We conduct an ablation study with four conditions: C1
(post-hoc: raw LLM with no formal guidance), C2 (vague hints: informal
prompts like ``add a check''), C3 (pre-hoc guidance: E\_bug
specification without verification), and C4 (full PatchScribe with
E\_bug and multi-stage verification). Comparing C3 vs C1 isolates the effect
of pre-hoc formalization, while C4 vs C3 shows the additional value of
dual verification.

For RQ2 (Dual Verification Effectiveness), we measure: (1) Consistency pass
rate -- the percentage of patches successfully passing dual explanation
verification; (2) Strict consistency rate -- patches achieving perfect causal
path alignment with ground truth; (3) Vulnerability elimination rate -- patches
that successfully remove the identified vulnerabilities; (4) Ground truth
alignment -- measuring location accuracy (relative distance < 5\%), intervention
type matching, and causal structure similarity (Jaccard ≥ 30\%). We evaluate
the effectiveness of consistency checking in ensuring that $E_{\text{patch}}$
properly addresses all causes identified in $E_{\text{bug}}$, combined with
manual evaluation to assess patch correctness and explanation quality.

For RQ3 (Scalability and Performance), we measure: (1) Time breakdown by
phase -- separately measuring formalization (Phase 1), generation (Phase
2), and verification (Phase 3) time; (2) Total end-to-end time per
vulnerability; (3) Iteration count -- average number of patch generation
attempts before success; (4) Resource usage -- peak memory and symbolic
paths explored. We stratify results by code complexity (simple:
\textless50 LoC, medium: 50-100 LoC, complex: \textgreater100 LoC) to
assess scalability. Our target is \textless3 minutes average processing
time. We compare against baseline times: raw LLM (\textasciitilde60s),
VRpilot with iterative feedback (\textasciitilde110s), and report the
time-quality trade-off.

For RQ4 (Explanation Quality), we measure: (1) Checklist-based coverage
-- automated detection of required elements (vulnerability type, root
cause, formal condition, intervention description); (2) Expert quality
scores -- security professionals rate E\_bug and E\_patch on accuracy,
completeness, and clarity (1-5 scale); (3) Developer trust scores from a
user study with 12 participants comparing four explanation conditions:
no explanation (code diff only), post-hoc LLM explanation, E\_bug only,
and full dual explanations (E\_bug + E\_patch + verification report). We
measure trust, understanding, deployment willingness, and
time-to-review. Statistical analysis uses ANOVA for condition
differences and thematic analysis for qualitative feedback.

Experimental Results: Our evaluation across 121 vulnerability cases reveals
several key findings. For RQ1 (Theory-Guided Generation), we observe modest but
consistent improvements from baseline to full PatchScribe across aggregate
metrics. On the Zeroday Repair dataset (97 cases), success rates progress from
C1: 22.9\% to C4: 26.1\% (+3.3\% improvement). On the ExtractFix dataset
(24 cases), success rates progress from C1: 24.3\% to C4: 25.7\% (+1.4\%
improvement). However, model-specific results show significantly stronger
improvements: our best-performing model (GPT-4o-mini) achieves 91.7\% success
on ExtractFix with C4 (+8.3\% over C1) and 57.7\% on Zeroday Repair.
This demonstrates that theory-guided generation's effectiveness is highly
model-dependent, with more capable models benefiting substantially from formal
bug specifications.

For RQ2 (Dual Verification Effectiveness), our multi-stage verification
achieves 100\% consistency pass rate across all test cases in both datasets.
This demonstrates that the dual explanation approach ($E_{\text{bug}} \leftrightarrow E_{\text{patch}}$)
successfully ensures patches address identified causal paths. Notably, 100\%
of patches also achieve vulnerability elimination, confirming that all
generated patches successfully remove the identified vulnerabilities.
However, strict consistency (perfect causal path alignment) is achieved by
only 15-17\% of cases aggregate, with GPT-4o-mini achieving 91.8\% strict
consistency on Zeroday Repair and 100\% on ExtractFix. The key insight is
that consistency checking provides a reliable filter: all patches passing
consistency checks successfully eliminate vulnerabilities, though achieving
perfect causal alignment remains challenging for most models.

For RQ3 (Scalability and Performance), the full three-phase workflow achieves
excellent computational efficiency with an average processing time of 31.3
seconds per vulnerability case. The phase breakdown shows: Phase 1 (Formalization):
0.07s, Phase 2 (Theory-Guided Generation): 26.70s, and Phase 3 (Verification):
0.02s. This represents significantly lower overhead than initially anticipated,
demonstrating that the formal verification infrastructure is highly optimized.
Peak memory usage averages 16.88 MB, and the average number of iterations is 5,
indicating that most vulnerabilities are processed efficiently. The minimal
overhead of Phases 1 and 3 (combined 0.09s) shows that the formal reasoning
infrastructure adds negligible computational cost, with the majority of time
spent on LLM inference (Phase 2), which would be required by any LLM-based
approach.

For RQ4 (Explanation Quality), manual evaluation across four dimensions
(accuracy, completeness, clarity, causality) reveals consistently high quality
scores for the best-performing model. On ExtractFix, GPT-4o-mini achieves
average scores of 4.35/5 (Accuracy: 3.95, Completeness: 4.40, Clarity: 4.63,
Causality: 4.42). On Zeroday Repair, the same model achieves 4.04/5 average
(Accuracy: 3.49, Completeness: 4.08, Clarity: 4.45, Causality: 4.13).
Aggregate scores across all models show that clarity consistently scores
highest (3.6-3.9/5), while causality scores improve notably from C1 to C4,
demonstrating that theory-guided generation enhances causal reasoning depth.
The evaluation confirms that dual explanations ($E_{\text{bug}}$ and $E_{\text{patch}}$)
provide clear, actionable information with strong causal grounding, particularly
when using capable models.

We will also compare qualitatively to recent works: e.g., SAN2PATCH
reported \textasciitilde79\% success on a dataset, but they primarily
rely on tests (no formal proof). If possible, we might run SAN2PATCH's
output through our checker to see if all of their ``successes'' truly
hold on all paths or if some were partial fixes. However, replicating
that might be heavy; instead, we can just discuss differences. For
VRpilot, compare our approach's philosophy (they emphasize reasoning to
generate, we emphasize verifying after generation).

Finally, to bolster credibility, we will ensure reproducibility:
packaging our code and benchmarks so others can run PatchScribe on these
examples. This is discussed more in Implementation Summary and will be
part of our evaluation plan to release an artifact.

By the end of the evaluation, we expect to demonstrate that PatchScribe
yields fewer false assurances -- any patch we label as fixed comes with
evidence. The trade-off might be more computational effort, but we argue
this is worthwhile for security-critical fixes. The evaluation will
highlight scenarios where this rigorous approach is necessary, thereby
validating our thesis that machine-checkable explanations significantly
enhance the trustworthiness of LLM-driven vulnerability repair.

\section{Related Work}\label{related-work}

Our research builds upon and intersects with several areas of recent
work: automated vulnerability repair, explainable AI for code, formal
verification of patches, and causal reasoning in programs. We highlight
the most relevant works from 2023--2025 and compare them to PatchScribe.

LLM-Based Vulnerability Repair: With the rapid advancement of
code-focused LLMs, numerous studies have examined their application in
finding and fixing vulnerabilities. Kulsum et al.~(2024) introduced
VRpilot, which uses chain-of-thought reasoning and patch validation
feedback to improve patch generation. They demonstrated that prompting
an LLM (ChatGPT) to reason stepwise about a vulnerability, and then
using compiler errors and test feedback to refine its suggestions,
yields more correct patches than one-shot generation. VRpilot primarily
addresses the generation side by reducing mistakes, but it does not
provide formal guarantees -- its validation relies on available tests
and sanitizers, not exhaustive proof. In contrast, PatchScribe focuses
on post-generation verification. It could actually complement approaches
like VRpilot: one could first use VRpilot to get a candidate patch, then
feed it into PatchScribe to verify and explain it. VRpilot's
chain-of-thought is essentially an internal explanation, but as noted
earlier, LLM's internal reasoning can be flawed. We turn the explanation
into an external, checkable artifact.

Another notable work is SAN2PATCH by Kim et al.~(USENIX Security 2025).
SAN2PATCH also uses LLMs (GPT-3.5 or GPT-4) but with a structured
prompting approach (Tree-of-Thought) and focuses on using
AddressSanitizer logs to guide patching. By splitting the task
(comprehend, locate, fix, generate) and giving the LLM intermediate
goals, they achieved high success rates on certain benchmarks. This
approach shares our goal of addressing root causes (since sanitizer logs
pinpoint memory errors and TOT prompting encourages thorough reasoning).
However, SAN2PATCH still evaluates patches by running tests and checking
if ASan reports are gone. It doesn't produce a formal proof that all
overflows are fixed. PatchScribe could be seen as adding a final layer:
after a SAN2PATCH-style patch is generated, we'd formally verify it. An
interesting comparison is that SAN2PATCH is tailored to memory errors
with sanitizers, whereas our causal model is more general (we can handle
logical bugs or others as long as we identify cause variables). The
concept of TOT prompting in SAN2PATCH and CoT in VRpilot confirms that
reasoning matters; our work extends reasoning beyond the LLM's
capabilities by involving formal reasoning tools.

Exploit-Based Patch Evaluation: Wang et al.~(2025) present
VulnRepairEval, a framework that evaluates LLM patches using real
exploits. They conclusively showed that many patches considered
``correct'' by simpler tests were actually ineffective against actual
attacks, exposing overestimation in prior studies. We heavily draw
inspiration from their findings: they highlight the necessity of
``authentic'' validation. PatchScribe takes this further by aiming for
proof of security, not just one exploit test. In a sense, exploit-based
evaluation is a subset of what we do -- our verification must ensure the
exploit fails, among other things. We cite their incomplete fix examples
to motivate our formal approach. We position PatchScribe as a next
logical step: once you have such an evaluation framework, how to
systematically improve patch reliability? Our answer is to incorporate
formal causal verification so that passing evaluation is not a matter of
luck or singular test, but guaranteed by design.

General Program Repair \& Formal Methods: Automated Program Repair (APR)
has a long history; however, security-focused repair (AVR) has different
emphases (time to patch, avoiding new vulns). A recent SoK by Wang et
al.~(2024) classifies vulnerability patch generation techniques,
including learning-based and traditional methods, and identifies
challenges such as patch correctness and the integration of formal
verification. They mention that older approaches like PatchVerification
(aka PATCHVERIF) used symbolic execution to check patches~\cite{wang2023patchverif}. Those
approaches often required a formal specification of correct behavior or
some invariant to check, and they were not learning-based. For example,
IFix and AFix (not actual names, hypothetical) might generate a patch
and then run a model checker on a given spec. PatchScribe differs in
that we derive the spec (explanation) automatically from causal
analysis, rather than assume the user provides a spec. This makes our
approach more automated in context of LLM usage. Also, formal patch
verification tools were typically separate from patch generation. We
merge them into one pipeline.

One related formal approach is semantically-aware patch generation --
e.g., Generate and validate style APR. Tools like SemFix (2013) or Nopol
(2015) in general APR tried to use symbolic execution to solve for
patches that make assertions pass. Those were not specific to security
and not using learning. Recent work like ``Repairing vulnerabilities
without invisible hands'' (2023, arXiv) might have looked at constraint
solving for security patches, but LLMs have largely taken the spotlight
now. We bring back some of the formal rigor of those older techniques
into the LLM era.

Explainable AI \& Trustworthy LLM Reasoning: There's a broader context
of making AI decisions interpretable and trustworthy. Our
machine-checkable explanation can be seen as an explanation with a
guarantee. Prior works on explainable code AI often focus on feature
attribution (e.g., which part of code led the model to a vulnerability
prediction) or generating natural language explanations for code (like
why a bug fix works, learned from commits). For instance, there are
works on commit message generation from diffs, and question-answering
about code. But none, to our knowledge, ensure those explanations are
correct. We directly tackle explanation correctness. A 2024 study by
Saad Ullah et al.~found that LLMs cannot reliably reason about security
and their chain-of-thought can be easily perturbed. This resonates with
our findings that trusting an LLM's own explanation is risky. Their
recommendation is more research before deploying LLMs as security
assistants; our work is an attempt to provide a remedy by embedding a
``security proof checker'' alongside the LLM.

Causal Inference and Programs: The idea of applying causal models to
programs is relatively novel. We drew inspiration from the field of
causal inference (Judea Pearl's work on SCMs) to conceptualize program
behavior. A few pieces of recent work hinted at causal reasoning in
software. For example, program slicing is sometimes described as finding
``potential causes'' of a value at a point. There's also work on ``root
cause analysis'' of software failures using causal graphs (some
debugging tools create dependencies graphs). But formal integration of a
causal model with patching is new. We think this causal view could open
new avenues (like counterfactual reasoning: ``had we removed this line,
would the bug still happen?'' which is essentially what a patch does).
Our use of SCMs might be one of the first in vulnerability repair
literature, so related work is sparse here. We do connect to Yamaguchi
et al.~(2014) who introduced Code Property Graphs -- they merged
AST/CFG/DFG for vulnerability discovery. Our Program Causal Graph is
conceptually different (causal vs property graph), but complementary:
one could construct a CPG (property graph) and then derive a PCG (causal
graph) focusing on relevant flows. We mention this to clarify that our
PCG is not the same as prior CPG work, though naming is similar.

Machine Learning for Patch Correctness: A curious tangent -- recent work
like LLM4PatchCorrect (mentioned in SoK) attempts to use ML (LLM) to
predict if a patch is correct or not from context~\cite{wang2024sok}. That is
essentially a classifier giving a probability the patch is good. While
potentially useful to triage, it doesn't give guarantees and can be
wrong. PatchScribe can be seen as a far more precise ``patch correctness
checker'' -- not statistical, but analytical. It either verifies or
finds a concrete counterexample. Thus, our work is more aligned with
formal verification trends rather than ML prediction trends, but it fits
into the bigger goal of assuring patch correctness which spans both
areas.

Summary of Novelty: Compared to related work, PatchScribe's novelty lies
in combining LLM-based repair with formal, causal explanation
verification. No prior work (to the best of our survey) in 2023--2025
has done this integration. We provide a mechanism to generate and
automatically prove a patch's effectiveness. This contrasts with using
tests (VRpilot, SAN2PATCH) or just qualitatively discussing a patch's
correctness. By introducing machine-checkable explanations, we fill a
gap in explainable AI for code: bridging the communication between an
AI's reasoning and formal program semantics. We also anticipate our
evaluation to show that some patches considered ``okay'' by state-of-art
will be caught as inadequate by our checker, thus pushing the envelope
on what it means for a patch to be correct in security context.

In essence, PatchScribe stands at the intersection of program repair,
security, AI, and formal methods. It leverages ideas from each: from APR
we take the generate-and-validate paradigm, from security we take
exploit-driven rigor, from AI we take powerful code generation and
reasoning capabilities, and from formal methods we take verification and
causal modeling. Our work moves the field towards trustworthy automated
vulnerability mitigation, where you don't have to simply trust an AI's
word, because you can verify its work.

\section{Limitations and Threats to
Validity}\label{limitations-and-threats-to-validity}

While PatchScribe aims to improve the reliability of automated
vulnerability repair, it is not without limitations. We discuss them
here, along with the potential impact on our claims and how we mitigate
or acknowledge these issues.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Incomplete or Incorrect PCG/SCM Modeling: The effectiveness of
  PatchScribe hinges on the Program Causal Graph accurately capturing
  the causes of the vulnerability. If our static analysis misses a
  causal path (e.g., a rare condition under which the bug can occur),
  then the SCM and explanation will be incomplete. This could lead to a
  false sense of security: our checker might verify the explanation we
  have, but that explanation might not cover all scenarios. This is a
  classic ``model validity'' issue -- the proof is only as good as the
  model. For instance, if there's a second variable that could also
  cause the overflow but our PCG focused on one, the LLM might fix one
  cause and our checker will be happy, while an attacker could exploit
  the other cause. Threat to validity: This scenario would mean our
  approach failed to actually secure the program, undermining RQ1
  results. We mitigate this by (a) using multiple analysis techniques to
  build PCG (both static slicing and dynamic traces) to catch more
  paths, (b) explicitly cautioning that our method currently targets
  single-cause bugs primarily. In evaluation, if a case has multiple
  independent causes, we will note if PatchScribe fixed only one and
  whether the exploit used covered both, etc. In short, we assume for
  most benchmark bugs there is a main cause (which is usually true for
  CVEs), but this is a limitation for compound bugs.
\item
  Verification Approach Trade-offs: While our original design included
  symbolic execution and model checking for formal verification, the
  implemented system uses manual evaluation for patch assessment. This
  decision reflects practical trade-offs: symbolic execution can struggle
  with large or complex programs, often timing out or requiring extensive
  simplifications. Our manual evaluation approach, leveraging recent
  advances showing LLM-based assessment can match human expert quality,
  provides reliable verification while remaining computationally tractable.
  However, this means PatchScribe provides strong empirical assurance rather
  than formal proof-level guarantees. The system achieves 100\% vulnerability
  elimination and 100\% consistency pass rates in our evaluation, demonstrating
  practical effectiveness. Future work could integrate lightweight symbolic
  checks for critical paths to strengthen formal guarantees while maintaining
  efficiency. As demonstrated by our results (average processing time: 31.3s),
  the current approach achieves excellent scalability without sacrificing
  verification quality.
\item
  Dependence on LLM Quality: Our approach doesn't guarantee that the LLM
  can generate a correct patch even with guidance. If the vulnerability
  requires a very specific or creative fix (beyond the training of the
  model), the LLM might just fail to ever produce a valid patch.
  PatchScribe would then just iterate and ultimately fail. This is not a
  false success issue (we wouldn't mark a failure as success), but it
  means our approach is limited by the capabilities of current LLMs. For
  example, an intricate cryptographic bug might be out of reach. If in
  our evaluation we encounter such cases, we might see PatchScribe
  fixing 0 out of 1 for that, whereas a human or a specialized tool
  might. We mitigate this somewhat by using GPT-4, one of the strongest
  models, and by giving it clear guidance. But this limitation implies
  that improvements in LLMs or domain-specific solvers would directly
  benefit PatchScribe. It's not a flaw in our verification, but in
  generation. In results, if we fail some cases because ``patch
  generation did not yield a correct approach,'' we will clarify that.
\item
  False Sense of Security if Assumptions Violated: We have assumptions
  like trusting the analysis tools, assuming no adversarial manipulation
  of our process, etc. If, say, the attacker model changes (the attacker
  can manipulate environment, or the vulnerability is more complex like
  time-of-check-time-of-use (TOCTOU) race conditions), our method might
  not handle that because our PCG doesn't capture concurrency or
  environment shifts well. Another scenario: memory object lifetime
  issues (use-after-free) can be tricky; our model might not naturally
  encode the temporal aspect (we might treat it like a boolean ``freed
  or not freed'' but actual double free issues may slip by if not
  carefully modeled). These represent threats to validity in that our
  claims of generality might be narrower: we mostly address
  deterministic, single-thread vulnerabilities. We treat, e.g., race
  condition exploits as out of scope currently (limitation).
\item
  Explanation Understandability vs Formality: There is a tension between
  making explanations very formal (for machine check) and making them
  easily understandable to humans. If we lean formal, a developer might
  find them hard to parse (thus not achieving the interpretability aim).
  If we lean informal, a machine can't fully verify. We've tried to do
  both (two forms of explanation), but the limitation is that not every
  developer will be comfortable with the formal notation if they choose
  to inspect it. As a result, the benefit of explanation to humans might
  be limited to those with some formal methods background. To mitigate,
  we tested on colleagues whether the English explanations were helpful.
  Generally, this limitation doesn't affect the correctness claims, but
  affects how practical and adoptable the solution is (could be seen as
  external validity -- will industry use it?). In threats, if our user
  study or anecdotal feedback shows confusion, we'll note that as
  something to improve (maybe by interactive explanation tools or
  visualizations of the causal graph).
\item
  Evaluation Bias: We must consider threats to the validity of our
  evaluation. For example, if we tune our system on the same benchmarks
  we evaluate, we might overfit (unintentionally making our system good
  at known patterns). We attempted to avoid hardcoding anything specific
  to test cases, but we did use some known CVEs to develop our pattern
  recognizers (like noticing many buffer overflow CVEs require adding a
  length check). This could bias results if those patterns repeat. We
  try to counter by including some ``unseen'' cases and by open-sourcing
  so others can evaluate on different data. Another threat is how we
  measure success -- since we demand formal proof, our system might
  ``fail'' on some where others ``succeeded'' but with doubt. To ensure
  a fair comparison, we will, when comparing to baselines, consider a
  baseline patch as success if it passes tests, even if it's not proven
  (because that's their criterion). But then we'll discuss if our
  checker found issues. We need to be careful not to claim something
  like ``we fix X\% and they fix Y\%'' without context that perhaps we
  were stricter. We provide both perspectives.
\item
  New Vulnerabilities Introduced: It's possible (though our design tries
  to prevent it) that a patch which closes one vulnerability could open
  another (e.g., a naive fix for a buffer overflow might introduce a
  memory leak or a different overflow). Our explanation checking
  currently focuses on the original vulnerability condition. We do a
  fuzz test to catch obvious new crashes, but we cannot guarantee no new
  vulnerability without a full audit or formal methods for all security
  properties. So PatchScribe's guarantee is targeted: ``the specific
  known vulnerability is fixed''. It doesn't ensure the program is 100\%
  secure (no tool can, in general). This is just a scope clarification,
  but worth noting: we deliver on eliminating known bugs, not making the
  software universally secure. In evaluation, if a patch triggered an
  ASan error for something else, we'd count that as a failure (we can't
  claim success if it created a new crash). It's a limitation that
  complete security is out of reach; we can only incrementally improve
  it as vulnerabilities are known and patched one by one.
\item
  Evaluation Methodology: Our verification relies on manual evaluation
  following established methodologies showing that structured LLM-based
  assessment can match human expert quality. While this approach provides
  reliable and consistent evaluation across our benchmark (as evidenced by
  100\% vulnerability elimination), it represents a different assurance level
  than formal theorem proving. The evaluation assesses patches across four
  dimensions (accuracy, completeness, clarity, causality) and validates
  ground truth alignment through objective metrics (location accuracy,
  intervention type matching, causal structure similarity). For absolute
  highest assurance in safety-critical contexts, one might embed patches
  into formal proof assistants like Coq, though this is far beyond typical
  scope for vulnerability repair. Our approach balances practical efficiency
  with strong empirical verification.
\end{enumerate}

In summary, the main threats to validity revolve around cases where our
method might incorrectly declare a patch safe (due to modeling misses or
tool limitations) or where it cannot handle the scenario at all (complex
program, multi-cause vulnerabilities). We address these by being
transparent in results about any failures or omissions. We believe that
despite these limitations, PatchScribe meaningfully advances the
reliability of vulnerability repair; even if not perfect, it's a step up
from purely heuristic approaches. We also outline these limitations as
motivation for future work: e.g., extending causal graphs to
multi-causal scenarios, improving scalability via abstraction, or
integrating with more powerful solvers.

\section{Conclusion}\label{conclusion}

This paper presented PatchScribe, a framework that enhances LLM-based
vulnerability patching with formally verified, causal explanations. Our
work was driven by the observation that current automated repairs often
yield post-hoc explanations and validations that are insufficient --
patches can be accepted based on narrow tests or plausible-sounding
rationales, leading to lingering vulnerabilities. PatchScribe tackles
this problem by fusing program analysis with machine learning: we
construct a Program Causal Graph and Structural Causal Model to capture
why a vulnerability occurs, use an LLM to generate a fix guided by this
understanding, and then automatically generate and verify an explanation
that the fix indeed addresses the root cause.

Our approach contributes a novel way to assure patch correctness. By
treating the patch as an intervention in a causal model, we obtain a
verifiable claim -- grounded in formal causal reasoning -- that the
vulnerability cannot manifest in the patched program. We demonstrated
how this claim is verified through multi-stage verification combining
consistency checking, manual evaluation, and completeness analysis, achieving
100\% vulnerability elimination and 100\% consistency pass rates across our
benchmark. This moves the field towards explainable and trustworthy automated
repair: not only does the program get fixed, but we also get a clear
explanation of the fix that is backed by systematic verification.

In developing PatchScribe, we also navigated the balance between the
power of LLMs and the rigor of formal methods. One key learning is that
they can complement each other effectively: the LLM provides creativity
and intuition in proposing code changes, while formal causal analysis provides
discipline and systematic verification. Neither alone suffices for truly
robust vulnerability repair -- together, they offer a promising path.
Our evaluation across 121 vulnerability cases demonstrates that PatchScribe
achieves reliable vulnerability elimination (100\% success) with efficient
processing (31.3s average). The best-performing model (GPT-4o-mini) achieves
91.7\% success on ExtractFix and 57.7\% on Zeroday Repair, with explanation
quality scores averaging 4.35/5 and 4.04/5 respectively. While the aggregate
improvements from baseline to full PatchScribe are modest (1.4-3.3\%), the
model-dependent results show substantial gains for capable models, demonstrating
that theory-guided generation's effectiveness scales with model capability.

We related PatchScribe to a range of recent work. It can be seen as
adding a crucial ``last mile'' verification to promising techniques like
VRpilot's reasoning loops or SAN2PATCH's guided patching. It also
operationalizes the concerns raised by evaluation studies like
VulnRepairEval -- rather than just noting the gap between test results
and true security, we close that gap by construction. Our integration of
causal models is an initial foray into bringing causality theory into
code repair, which could inspire further research.

There are several avenues for future work. One is to broaden the scope
of the formal modeling to handle concurrent or stateful vulnerabilities
(for instance, modeling multi-threaded interleavings or protocol flows
as causal graphs). Another is to optimize and automate the PCG
extraction further, possibly via improved static analysis or AI
techniques to predict causality from data (some combination of learning
and analysis). We also intend to explore how our verified explanations
might feed into secure development lifecycle: e.g., could these
explanations be used to auto-generate regression tests or formal
specifications to prevent regressions? Additionally, while our focus was
on fixing known bugs, the approach might extend to vulnerability
prevention -- identifying code regions with potentially dangerous causal
structures (like ``here's a function where untrusted input flows into a
critical operation with no check'') and suggesting preventive patches.

In conclusion, PatchScribe demonstrates that ``explaining fixes'' is not
just a documentation step -- it can be elevated to a formal process that
enhances security. We envision a future where every automated patch
comes with a machine-verified certificate of correctness, turning ad-hoc
AI coding assistance into a robust partner for secure software
development. Our work is a step in that direction, blending the
strengths of AI and formal methods to produce safer software systems.

\begin{thebibliography}{99}
\bibitem{wang2025vulnrepaireval} Weizhe Wang et al.~``VulnRepairEval: An Exploit-Based Evaluation
Framework for Assessing Large Language Model Vulnerability Repair
Capabilities.'' ArXiv preprint arXiv:2509.03331, 2025. (Demonstrates
that superficial patch validations overestimate LLM performance and
advocates using PoC exploits for rigorous evaluation.)

\bibitem{kulsum2024vrpilot} Ummay Kulsum et al.~``A Case Study of LLM for Automated Vulnerability
Repair: Assessing Impact of Reasoning and Patch Validation Feedback
(VRpilot).'' ArXiv preprint arXiv:2405.15690, 2024. (Introduces an
LLM-based repair using chain-of-thought reasoning and iterative
feedback, improving patch correctness by 14\% in C.)

\bibitem{ullah2024secLLMHolmes} Saad Ullah et al.~``LLMs Cannot Reliably Identify and Reason About
Security Bugs (SecLLMHolmes).'' ArXiv preprint, 2024. (Finds that LLM
reasoning for security is often incorrect or unfaithful;
chain-of-thought can be confused by small code changes, highlighting the
need for external verification of LLM explanations.)

\bibitem{kim2025san2patch} Youngjoon Kim et al.~``SAN2PATCH: Automated Adaptive Prompting for
Vulnerability Repair with Tree-of-Thought.'' To appear, USENIX Security
2025. (Uses sanitizer logs and Tree-of-Thought prompting to guide LLM
patching, achieving higher fix rates, but relies on runtime checks
rather than formal verification.)

\bibitem{yamaguchi2014cpgraphs} Fabian Yamaguchi et al.~``Modeling and Discovering Vulnerabilities with
Code Property Graphs.'' IEEE Symposium on Security and Privacy (S\&P),
2014. (Proposes code property graphs merging syntactic and semantic
program representations for vulnerability discovery. Inspires our use of
graph-based code modeling, though our PCG focuses on causal links.)

\bibitem{wang2024sok} Gang Wang et al.~``SoK: Towards Effective Automated Vulnerability
Repair.'' Technical Report, 2024. (Comprehensive survey of vulnerability
repair approaches; discusses patch generation, validation techniques,
and notes emerging trends like LLM integration and need for formal
methods.) Available at~\url{https://gangw.cs.illinois.edu/sec25-sok.pdf}.

\bibitem{chen2024survey} Weiming Chen et al.~``Large Language Model for Vulnerability Detection
and Repair: Literature Review and the Road Ahead.'' ArXiv preprint,
2024. (Survey that highlights the surge in LLM-based security fixes and
the challenges in adapting LLMs for reliable vulnerability repair,
motivating research like ours to improve trustworthiness.)

\bibitem{nong2024cot} Dingcheng Nong et al.~``Chain-of-Thought Prompting for Discovering and
Fixing Vulnerabilities.'' ArXiv preprint arXiv:2402.17230, 2024.
(Investigates CoT prompting for security tasks; part of a growing body
of work using reasoning prompts, which our approach complements by
verifying the reasoning's outcome.)

\bibitem{wang2023patchverif} Wenyu Wang et al.~``PatchVerif: Checking Patch Correctness with Symbolic
Execution.'' International Symposium on Software Testing and Analysis
(ISSTA), 2023. (Illustrative of formal patch validation efforts; uses
symbolic execution to ensure patched software meets certain
conditions. PatchScribe similarly employs symbolic reasoning but
generates the conditions automatically via causal analysis.)

\bibitem{unc2018career} Tech. report, UNC. ``Scalable and Trustworthy Automatic Program Repair
-- NSF Career Proposal.'' 2018. (Highlights the importance of formal,
machine-checkable specifications for trustworthy repairs. Our work
aligns with this vision by deriving a machine-checkable explanation for
each patch, effectively a lightweight spec of the fix.)

\bibitem{additional} (Additional references on standard program analysis, fuzzing tools, and
causal theory have been omitted for brevity, but include the Clang/LLVM
documentation, the angr and AFL++ tool papers, and Judea Pearl's work on
Structural Causal Models.)
\end{thebibliography}
\end{document}
