\documentclass[conference,compsoc]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{textcomp}
\providecommand{\tightlist}{\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}
\title{PatchScribe: Theory-Guided Vulnerability Repair with Causal Explanations}

\author{\IEEEauthorblockN{Anonymous Authors}}
\maketitle
\begin{abstract}
Large Language Models (LLMs) show promise in automated vulnerability repair, yet their patches often lack verifiable guarantees of correctness.
Current approaches rely on exploit-only validation and informal rationales, providing limited assurance that vulnerabilities are truly eliminated.
We present PatchScribe, a theory-guided framework that generates security patches with dual causal explanations through a two-phase workflow.
PatchScribe inverts traditional workflows: it first formalizes vulnerabilities using Program Causal Graphs (PCG) and Structural Causal Models (SCM) to generate a formal bug specification ($E_{\text{bug}}$), then guides LLM-based patch generation with precise constraints, producing patches accompanied by formal patch explanations ($E_{\text{patch}}$).
We evaluate patch and explanation quality through structured manual assessment using a multi-dimensional rubric (accuracy, completeness, clarity, causality).
Evaluated on 121 real-world CVEs from two complementary benchmarks (APPATCH Zeroday Repair: 97 cases, ExtractFix: 24 cases), PatchScribe achieves 31\% patch correctness rate, significantly outperforming raw LLM baseline (24\%, $p$<0.01) and informal guidance approaches (23\%, $p$<0.05), with average processing time of 75 seconds per CVE.
Our theory-guided approach provides clearer causal understanding than existing exploit-only methods while remaining computationally practical for real-world deployment.
\end{abstract}
\section{Introduction}\label{introduction}

LLM-based code assistants have shown promise in automatically fixing vulnerable code, but a critical gap remains: can we trust that the LLM's patch truly eliminates the vulnerability, and can we verify the reasoning behind it?
In current practice, an LLM might propose a code change along with a natural language explanation of the fix.
However, such explanations are post-hoc and often not verifiable by any rigorous means -- they could be incomplete, incorrect, or even hallucinated.
This lack of verifiability in patch rationales poses a security risk: a patch that ``sounds'' correct might still fail to eliminate the underlying exploit path or might introduce new issues, all while the developer is misled by a plausible but unproven explanation.

Recent research underscores the limitations of relying on informal validation of patches.
Studies have found that LLM-generated fixes can be plausible yet incorrect, passing unit tests or  superficial checks without truly removing the vulnerability.
For example, incomplete patches in real-world libraries (PyYAML, Pillow, Django) passed initial review but left residual flaws that attackers later exploited.
To counter this, efforts like VulnRepairEval advocate for exploit-based validation, judging a patch by whether a proof-of-concept (PoC) exploit is blocked.
This is a step toward realism -- requiring that the original attack no longer succeeds -- and has revealed that state-of-the-art LLMs fix only 22\% of known CVEs under these strict conditions.
However, even exploit-only validation is limited: it confirms that one particular attack input is mitigated, but it does not prove in general that the vulnerability is fully eradicated or that no new vulnerabilities are introduced by the patch.
Moreover, it provides no insight why the patch works (if it does).
Developers and security  auditors are left to trust the LLM's textual rationale, which may be unfaithful to the code's actual logic.

We argue that a more principled approach is needed -- one that combines the generative capabilities of LLMs with formal reasoning about causal relationships in the code.
Our key insight is twofold: (1) formalize the vulnerability before attempting to patch it, using the formalization to guide (not just verify) patch generation, and (2) generate separate formal explanations for the bug and the patch, enabling consistency checking between them.
If we can formally capture what conditions cause the vulnerability (\(E_{\text{bug}}\)), we can provide the LLM with precise guidance (e.g., ``\(V_{\text{overflow}}\) occurs when len \textgreater{} 256 AND no check exists; ensure one of these is false''). After the LLM generates a patch, we formalize how the patch intervenes (\(E_{\text{patch}}\)) and verify that the intervention actually addresses the causes identified in \(E_{\text{bug}}\).
This dual-explanation approach catches incomplete fixes that might pass exploit tests but miss edge cases.

We introduce several key concepts that distinguish our approach:

\textbf{Theory-Guided Generation:} Unlike post-hoc explanations that describe patches after generation, PatchScribe uses pre-hoc formalization—constructing a formal vulnerability specification before patch generation to guide the LLM with precise constraints.

\textbf{Dual Causal Explanations:} We generate two complementary explanations: $E_{\text{bug}}$ (formal specification of vulnerability's root cause) and $E_{\text{patch}}$ (formal specification of how the patch eliminates it). These dual explanations provide developers with clear causal understanding of both the vulnerability and its fix.

\textbf{Structured Manual Evaluation:} We assess patch and explanation quality through structured manual evaluation using a multi-dimensional rubric measuring accuracy (technical correctness), completeness (coverage of what/why/how), clarity (understandability), and causality (depth of causal reasoning).

Our key contributions are:
\begin{itemize}
\item A novel pre-hoc formalization approach using Program Causal Graphs and Structural Causal Models for vulnerability repair
\item A dual explanation framework enabling systematic consistency verification between bug causes and patch interventions
\item Comprehensive evaluation on 121 real-world CVEs demonstrating 29\% and 40\% relative improvements over unguided baselines (Zeroday and ExtractFix, respectively)
\item An anonymized implementation with reproducible artifacts prepared for public release upon acceptance
\end{itemize}

In this paper, we introduce PatchScribe, a framework that brings theory-guided causal reasoning to LLM-based vulnerability repair.
Unlike prior approaches that explain patches post-hoc, PatchScribe follows a pre-hoc methodology: formalize first, then generate patches with dual explanations. The approach builds a Program Causal Graph (PCG) to represent the causal structure of the vulnerability (for instance, how a lack of input validation leads to a buffer overflow) and instantiates a Structural Causal Model (SCM) on top of this graph.
From the SCM, we generate a formal bug explanation ($E_{\text{bug}}$) that precisely characterizes the vulnerability condition, causal paths, and intervention options. This formal specification is provided to the LLM as guidance, enabling it to generate more targeted patches.
After patch generation, we analyze how the patch intervenes on the causal model and generate a formal patch
explanation ($E_{\text{patch}}$) describing what changed and why the vulnerability is eliminated.
This two-phase approach (Formalization $\rightarrow$ Theory-Guided Generation) produces patches with dual causal explanations, providing clearer understanding than prior work and enabling systematic assessment of patch quality.

We demonstrate that PatchScribe significantly improves quality of automated patches.
By using formal causal specifications to guide patch generation, we help LLMs generate patches that address root causes rather than symptoms.
Our approach directly addresses the limitations of prior work: (1) It provides pre-hoc formalization that guides generation with precise constraints rather than post-hoc validation alone, and (2) it replaces unverifiable natural-language rationales with structured causal explanations ($E_{\text{bug}}$, $E_{\text{patch}}$) that developers can systematically assess.
This yields a double benefit: higher quality patches and clearer understanding of vulnerability and fix.

Section~\ref{sec:background} provides background and motivation.
Section~\ref{sec:approach} presents the framework overview.
Sections~\ref{sec:formal-model} and~\ref{sec:system} detail our formal model and system architecture.
Section~\ref{sec:evaluation} presents experimental evaluation.
Section~\ref{sec:related} discusses related work.
Section~\ref{sec:discussion} addresses security guarantees, limitations, and ethics.
Section~\ref{sec:conclusion} concludes.

\section{Background and Motivation}\label{sec:background}

\subsection{LLM-Based Vulnerability Repair}

Automated vulnerability repair has long been a goal of the security community.
With the rise of powerful code-focused LLMs, researchers have explored using these models to generate vulnerability fixes from code context and problem descriptions.
Early results are encouraging yet cautionary: LLM-generated patches can superficially appear correct while failing to eliminate the security issue.

Wang et al.'s VulnRepairEval~\cite{wang2025vulnrepaireval} explicitly uses real proof-of-concept exploits as validation, revealing that state-of-the-art LLMs fix only 22\% of known CVEs under strict conditions.
To improve generation, researchers have incorporated reasoning techniques.
Kulsum et al.'s VRpilot~\cite{kulsum2024vrpilot} uses chain-of-thought prompting plus iterative validation, improving patch success rates by 14\% for C vulnerabilities.
SAN2PATCH~\cite{kim2025san2patch} employs Tree-of-Thought prompting and AddressSanitizer guidance, achieving 63--79\% success rates on benchmark datasets.

However, all these approaches rely on the LLM's internal reasoning or external tests to judge correctness.
Whether it's VRpilot's chain-of-thought or SAN2PATCH's guided stages, the rationale is encapsulated in natural language explanations or test passing---neither provides formal guarantees of security.
Even if an exploit is thwarted, one cannot be sure that a variant wouldn't succeed or that the patch didn't introduce new vulnerabilities.

\subsection{Limitations of Current Approaches}

Despite progress, existing LLM-based repair methods exhibit critical limitations that motivate our work:

\textbf{L1: Incomplete Validation.}
Many approaches validate patches using tests or specific exploit instances, not comprehensive proofs.
Exploit-based evaluations use one PoC exploit as the litmus test---if blocked, the patch is deemed successful.
However, this overlooks variant exploits or edge cases.
A patch might check for a known malicious input pattern rather than fixing the underlying unsafe logic; the original exploit fails, but a tweaked input could still succeed.
Thus, exploit-only validation cannot guarantee completeness.

\textbf{L2: Unverifiable Rationales.}
LLMs produce natural language explanations that are post-hoc and unverifiable.
Studies observe LLMs giving confident-sounding explanations that are partially or wholly incorrect---hallucinated reasoning~\cite{ullah2024secLLMHolmes}.
An LLM might assert ``the buffer is now bounds-checked, so the overflow is resolved,'' but without code verification, we cannot confirm the check covers all cases or is correctly implemented.
No current system provides a formal link between explanation and code.

\textbf{L3: Lack of Formal Guarantees.}
Traditional program repair explored formal methods~\cite{wang2024sok}, but integrating formal verification with LLM-driven patching has seen minimal exploration.
Formal specifications for security properties are hard to write for arbitrary code, and fully verifying a patch can be as hard as verifying the entire program.
Most LLM repair systems avoid formal proofs, leaving no strong guarantees backing the patch.
Current LLM patching often operates in a ``generate-and-hope'' mode---insufficient for high-assurance domains.

\textbf{L4: Narrow Reasoning Context.}
LLMs might not fully understand the causal chain of a vulnerability, acting on local cues.
For a ``read past buffer end'' vulnerability, an LLM might locally add a length check, but if the root cause involves multiple functions passing incorrect sizes, a local fix might not solve it.
Without an explicit representation of causality, there is risk of addressing symptoms rather than causes.
Code property graphs and taint analyses create global views for detection, but LLM-driven repair hasn't fully leveraged them.
As a result, some patches are ``fragile''---they only intercept the known path, not all paths.

\subsection{Motivation for Causal Reasoning}

This motivates introducing formal, causal reasoning into the vulnerability repair loop.
Our motivation draws from explainable AI and formal methods.
In explainable AI, a distinction exists between interpretations (post-hoc, model-generated justifications) and explanations grounded in true causal factors.
Current LLM patch explanations are interpretations---fluent narratives that may not align with the program's actual logic.
We seek causal explanations reflecting genuine cause-effect relations in code.

Meanwhile, formal verification in security provides strong guarantees (e.g., proof-carrying code), but writing full specifications for arbitrary software is notoriously difficult.
Our approach threads a middle ground: we do not require full formal specification of program behavior, only formal characterization of the vulnerability condition and its causes.
This makes the problem tractable while yielding actionable proofs about the patch's effect.

PatchScribe bridges LLM-driven flexibility with systematic assurance by constructing a causal model of the vulnerability, ensuring patches address the core cause through dual explanation generation.

\subsection{Design Requirements}

\noindent\textbf{R1. Causal Correctness Guarantee.}
The system must ensure that a generated patch eliminates the root cause of the vulnerability across all relevant inputs.
Rather than relying on a single exploit, PatchScribe reasons over every causal path and seeks to prove that the vulnerability condition \(V_{\text{bug}}\) becomes unreachable once the patch is applied.

\noindent\textbf{R2. Machine-Checkable Explanation.}
Each patch must be accompanied by a structured explanation that a tool can verify.
We encode explanations as logical assertions over the PCG/SCM so that statements such as ``the new bounds check prevents the unsafe call'' translate into a concrete proof obligation.

\noindent\textbf{R3. Integration with LLM Patch Generation.}
The workflow still leverages LLMs for synthesis, but guidance and validation are driven by the causal model.
PatchScribe feeds \(E_{\text{bug}}\) into the prompt, captures the LLM's candidate patch, and iteratively refines or rejects it based on verification feedback.

\noindent\textbf{R4. Verification Precision.}
The explanation checker prioritizes soundness: if it approves a patch, the vulnerability should truly be fixed.
We accept conservative rejections when necessary and combine multiple analyses (symbolic execution, static reasoning) to minimize false decisions.

\noindent\textbf{R5. Causal Graph Coverage.}
The Program Causal Graph must capture every condition and flow relevant to the vulnerability.
We combine data-flow, control-flow, and security-pattern analyses so the derived SCM reflects all contributing causes, including multi-factor bugs.

\noindent\textbf{R6. Usability and Interpretability.}
Outputs remain grounded in program entities to keep the formal reasoning accessible to developers.
PatchScribe surfaces explanations using meaningful variable names and natural-language summaries without requiring additional annotations from practitioners.

\noindent\textbf{R7. Workflow Compatibility.}
The system integrates with standard toolchains (e.g., Clang/LLVM) and keeps runtimes practical so it can slot into CI/CD pipelines.
Automation targets realistic codebases while producing clear pass/fail signals for security reviews.

\noindent\textbf{R8. Functional Non-Regression.}
Security fixes must not break intended behavior.
We therefore incorporate regression safeguards—executing available tests or sanity checks—to flag patches that might disable functionality while fixing the vulnerability.

These requirements guided the design of PatchScribe, which we describe next.

\section{Approach Overview}\label{sec:approach}

PatchScribe is a system for automated vulnerability repair that produces dual causal explanations for each patch.
Unlike prior approaches that generate patches first and explain them post-hoc, PatchScribe follows a theory-guided approach: it first formalizes the vulnerability causally, then uses this formalization to guide patch generation, producing patches accompanied by dual explanations (\(E_{\text{bug}}\) and \(E_{\text{patch}}\)).
Figure~\ref{fig:overview} illustrates the overall architecture.

\begin{figure}[t]
\centering
\fbox{\parbox{0.9\columnwidth}{
\centering
\textbf{[Figure 1: System Architecture Diagram]}\\[0.5em]
\textbf{Input:} CVE Description + Vulnerable Code\\[0.5em]
\fbox{\parbox{0.85\columnwidth}{
\textbf{Phase 1: Vulnerability Formalization}\\
Static Analysis (Clang/LLVM) $\rightarrow$ PCG Construction\\
27 Security Patterns $\rightarrow$ SCM Instantiation\\
Output: $E_{\text{bug}}$ (formal condition, causal paths, interventions)
}}\\[0.5em]
$\downarrow$\\[0.5em]
\fbox{\parbox{0.85\columnwidth}{
\textbf{Phase 2: Theory-Guided Patch Generation}\\
LLM (GPT-4o-mini) + $E_{\text{bug}}$ Guidance $\rightarrow$ Patch Code\\
Causal Intervention Analysis $\rightarrow$ $E_{\text{patch}}$ Generation\\
Output: Patch + $E_{\text{patch}}$ (intervention, disrupted paths)
}}\\[0.5em]
$\downarrow$\\[0.5em]
\textbf{Output:} Patch + Dual Explanations ($E_{\text{bug}}$, $E_{\text{patch}}$)\\
}}
\caption{PatchScribe system architecture showing two-phase workflow: (1) Vulnerability formalization via PCG/SCM generates $E_{\text{bug}}$, (2) Theory-guided LLM generation produces patch and $E_{\text{patch}}$. Patches are evaluated through structured manual assessment (Section~\ref{sec:evaluation}).}
\label{fig:overview}
\end{figure}

\textbf{At a high level, PatchScribe operates in two phases:}

\textbf{Phase 1: Vulnerability Formalization} - We analyze the vulnerable program to build a Program Causal Graph (PCG) and derive a Structural Causal Model (SCM).
From the SCM, we generate a \textbf{formal vulnerability specification (\(E_{\text{bug}}\))} that precisely characterizes the conditions under which the vulnerability manifests.
This specification serves as guidance for patch generation.
The formal bug explanation \(E_{\text{bug}}\) contains: (a) the formal condition characterizing when \(V_{\text{bug}}\) occurs, (b) natural language descriptions mapped to code, (c) intervention options for fixing the vulnerability, and (d) causal paths leading to the vulnerability.

\textbf{Phase 2: Theory-Guided Patch Generation} - Armed with the formal vulnerability specification \(E_{\text{bug}}\), we prompt an LLM to generate a patch.
Critically, the LLM receives not vague hints but a precise formal description of what conditions cause the vulnerability and what the patch must achieve.
For example, instead of saying ``fix the overflow,'' we provide ``$V_{\text{overflow}} \Leftrightarrow (len > 256) \land (\lnot Check)$; to fix, you must ensure Check = true or $len \leq 256$ before line 42.''
After patch generation, we analyze how the patch intervenes on the causal model and generate a \textbf{formal patch explanation (\(E_{\text{patch}}\))} describing the intervention, its effect on \(V_{\text{bug}}\), and which causal paths are disrupted.

This approach ensures that patches are guided by formal causal specifications, helping LLMs generate patches that address root causes.
The key innovation is the generation of dual causal explanations (\(E_{\text{bug}}\) and \(E_{\text{patch}}\)), providing developers with systematic understanding of both the vulnerability and its fix.
We evaluate the quality of patches and their explanations through structured manual assessment (Section~\ref{sec:evaluation}).

In meeting these requirements, PatchScribe combines SCM reasoning with explanation checking to deliver causal correctness (R1) and machine-verifiable proofs (R2).
The causal specification guides LLM synthesis and iterative refinement (R3) while prioritizing sound verification outcomes (R4).
Comprehensive program analysis produces PCGs that capture true causes (R5), and the resulting explanations remain developer-friendly (R6).
Finally, the system integrates with standard tooling (R7) and executes regression safeguards to avoid functional regressions (R8).

The following sections detail the formal modeling, system architecture, and implementation.

\subsection{Threat Model and Scope}

\textbf{Assumptions.} We assume the vulnerability location is known (e.g., via CVE description, sanitizer log) and that the LLM and analysis tools are trusted. The threat is not the toolchain but the risk of incorrect or incomplete patches.

\textbf{Attacker Model.} An attacker can provide arbitrary inputs to exploit the vulnerability. Post-patch, they may attempt variant exploits or search for newly introduced weaknesses. PatchScribe addresses two main threats: (1) \emph{Incomplete Fix}---where alternative paths still trigger the vulnerability, and (2) \emph{Misleading Explanation}---where unverifiable rationales give false confidence.

\textbf{Defender Goals.} Produce patches that eliminate the vulnerability for all feasible inputs (not just known exploits) while preserving functionality and avoiding new vulnerabilities. We focus on security correctness, assuming basic regression testing catches functional issues.

\textbf{Out of Scope.} We do not model concurrency vulnerabilities, side-channels, or adversarial manipulation of the LLM itself. Complex multi-step vulnerabilities requiring extensive environmental state may exceed PCG/SCM modeling capabilities (discussed in Section~\ref{sec:discussion}).

\section{Formal Model}\label{sec:formal-model}

Table~\ref{tab:notation} summarizes key notation used throughout this section and the rest of the paper.

\begin{table}[h]
\centering
\caption{Key Notation and Terminology}
\label{tab:notation}
\begin{tabular}{ll}
\toprule
\textbf{Notation} & \textbf{Meaning} \\
\midrule
$P$ & Original (vulnerable) program \\
$P'$ & Patched program \\
$V_{\text{bug}}$ & Vulnerability condition/event \\
$E_{\text{bug}}$ & Formal bug explanation \\
$E_{\text{patch}}$ & Formal patch explanation \\
$G = (V, E)$ & Program Causal Graph (PCG) \\
$do(X=x)$ & Causal intervention (SCM notation) \\
PCG & Program Causal Graph \\
SCM & Structural Causal Model \\
\bottomrule
\end{tabular}
\end{table}

In PatchScribe, the formal foundation is provided by two interrelated models: the Program Causal Graph (PCG) and the Structural Causal Model (SCM).
Here we define each and explain how they are constructed and used.

\subsection{Program Causal Graph (PCG)}\label{program-causal-graph-pcg}

Definition: A Program Causal Graph is a directed graph \(G = (V, E)\) where
each node \(v \in V\) represents a program state predicate or event (e.g., an if condition, a variable state, a function call, a memory write) related to the vulnerability, and a directed edge \((u \rightarrow v) \in E\) indicates that node \(u\) has a direct causal influence on v in the context of the vulnerability.

The PCG is a high-level representation extracted from the program's code and execution flow: - Nodes: We include nodes for conditions (e.g., the truth value of an if condition), for certain variable states (e.g., ``variable x has value n''), and for specific events like ``function f calls g'' or ``memory write at location L occurs''.
Particularly, we distinguish a special node \(V_{\text{bug}}\) representing the occurrence of the vulnerability (e.g., an out-of-bounds write or a crash condition).
We also include nodes representing the negation or absence of certain checks, since the lack of a check is often a cause (for example, a node might be ``No null-pointer check before dereference'' which is essentially a predicate that is true when a check is missing in the code path).
Edges: If the program logic is such that u being true or an event happening contributes to v becoming true/happening, we draw an edge \(u \rightarrow v\).
This is akin to saying ``\(u\) is a direct cause of \(v\)'' under the framework of causal graphs (similarly to Bayesian network or Pearl's causal diagrams, but here based on program logic rather than statistical data).
For instance, if the code has if \(len > N\) goto error; then the condition ``len \(> N\)'' (node \(u\)) causally influences whether the program goes to error handling (node v).
In a vulnerability context, we might have edges like ``Input not sanitized'' \(\rightarrow\) ``Buffer overflow occurs'' or ``Flag is false'' \(\rightarrow\) ``Access control bypass''.

\subsubsection{PCG Construction Algorithm}

To build the PCG, we rely on program analysis techniques combined with pattern detection.
Algorithm~\ref{alg:pcg} presents our automated construction procedure.

\begin{algorithm}[t]
\caption{Program causal graph construction}
\label{alg:pcg}
\DontPrintSemicolon
\KwIn{Vulnerability location $L_v$, program $P$}
\KwOut{Program causal graph $G = (V, E)$}
$V \leftarrow \{V_{\text{bug}}\}$; $E \leftarrow \emptyset$\;
$\textit{slice} \leftarrow \textsc{BackwardSlice}(P, L_v)$\;
\ForEach{statement $s$ in $\textit{slice}$}{
  $D \leftarrow \textsc{DataDependencies}(s)$\;
  $C \leftarrow \textsc{ControlDependencies}(s)$\;
  \ForEach{$d$ in $D \cup C$}{
    $node_s \leftarrow \textsc{CreateNode}(s)$\;
    $node_d \leftarrow \textsc{CreateNode}(d)$\;
    $V \leftarrow V \cup \{node_s, node_d\}$\;
    \If{\textsc{IsCausalRelation}$(d, s)$}{
      $E \leftarrow E \cup \{(node_d, node_s)\}$\;
    }
  }
}
\ForEach{pattern $p$ in \textsc{DetectSecurityPatterns}$(\textit{slice})$}{
  $node_p \leftarrow \textsc{CreateAbsenceNode}(p)$\;
  $V \leftarrow V \cup \{node_p\}$\;
  $E \leftarrow E \cup \{(node_p, V_{\text{bug}})\}$\;
}
$G \leftarrow \textsc{RemoveTransitiveEdges}(V, E)$\;
\Return{$G$}\;
\end{algorithm}

\textbf{Implementation details.} Backward slicing relies on the Clang Static Analyzer within LLVM 14.0. Taint information is extracted through a custom LLVM pass derived from DataFlowSanitizer. We detect 27 security-relevant patterns covering missing checks, resource initialization issues, and related guard conditions. The pipeline is fully automated for single-function vulnerabilities, with manual refinement reserved for complex interprocedural cases.

The result is a graph where \(V_{\text{bug}}\) is at the bottom (sink) and various inputs or conditions are at the top (sources), with intermediate nodes linking them.

\textbf{Example:} Consider a C function that reads from a buffer without checking the index, causing a potential out-of-bounds read.
Figure~\ref{fig:pcg-example} illustrates the constructed PCG for this example.

\begin{figure}[t]
  \centering
  \fbox{
    \parbox{0.9\columnwidth}{
      \centering
      \textbf{[Figure: PCG for Out-of-Bounds Read Example]}\\[0.5em]
      \small
      \textbf{Vulnerable Code:}\\
      \texttt{int read\_buffer(char* buf, int len) \{}\\
      \texttt{~~int i = get\_user\_input();~~// No validation}\\
      \texttt{~~return buf[i];~~// Line 42: OOB read}\\
      \texttt{\}}\\[0.5em]

      \textbf{Program Causal Graph:}\\[0.3em]
      \fbox{\parbox{0.85\columnwidth}{
      \textbf{Exogenous (Input):}\\
      $\circ$ \texttt{user\_input} (untrusted source)\\[0.2em]

      $\downarrow$ \textit{data flow}\\[0.2em]

      \textbf{Endogenous (Program State):}\\
      $\circ$ \texttt{i\_value} = \texttt{get\_user\_input()}\\
      $\circ$ \texttt{(i >= buf\_len)} [unsafe condition]\\
      $\circ$ \texttt{no\_bounds\_check} [missing guard]\\[0.2em]

      $\downarrow$ \textit{both contribute}\\[0.2em]

      \textbf{Vulnerability:}\\
      $\circ$ $V_{\text{bug}}$ = Out-of-bounds read at Line 42\\[0.2em]

      \textbf{Causal Edges:}\\
      \texttt{user\_input} $\xrightarrow{\text{determines}}$ \texttt{i\_value}\\
      \texttt{i\_value} $\xrightarrow{\text{creates}}$ \texttt{(i >= buf\_len)}\\
      \texttt{(i >= buf\_len)} $\xrightarrow{\text{triggers}}$ $V_{\text{bug}}$\\
      \texttt{no\_bounds\_check} $\xrightarrow{\text{enables}}$ $V_{\text{bug}}$
      }}\\[0.3em]
    }
  }
  \caption{Program Causal Graph example for out-of-bounds read vulnerability showing how untrusted user input and missing bounds check both causally contribute to $V_{\text{bug}}$. The PCG captures two converging causal paths: (1) data flow from user input through unsafe condition, and (2) missing validation check enabling the vulnerability. Patch must disrupt at least one path to eliminate the vulnerability.}
  \label{fig:pcg-example}
\end{figure}

The PCG succinctly shows: user input and missing check together cause the out-of-bounds read.
For programs with multiple conditions, the graph captures more complex causal structures with converging branches.

\subsection{Structural Causal Model (SCM)}\label{structural-causal-model-scm}

Once we have the PCG, we formalize it as an SCM. A Structural Causal Model is typically defined by a set of endogenous variables (variables we model within the system) and exogenous variables (external inputs), along with structural equations that deterministically (or probabilistically, but here deterministically) define each endogenous variable in terms of some of the others, and a causal diagram akin to our PCG that shows dependencies.

Mapping PCG to SCM: - Each node in the PCG becomes a variable in the SCM.
For boolean conditions/events, we treat them as binary variables (true/false).
If certain nodes represent numeric values (like a variable's value), we could include those as numeric variables, but often we reduce to boolean predicates (like ``x \textgreater{} N'' as a boolean variable).
The edges in PCG define parent-child relationships in the SCM. If \(u \rightarrow v\) in PCG, then in the SCM, \(v\) will be a function of \(u\) (and possibly other parents). The SCM's structural equation for v is essentially a formal version of ``v is true if \ldots'' conditions based on its parent nodes.

Example formalization: For the above out-of-bounds example, we might define binary variables: - C (for user Control): C = true if the user can freely choose i (this might be considered exogenous input actually).
- B (for check): B = true if there is a check on i (so B was false originally since no check; we use B=1 to mean check present, 0 means absent).
- A (for array bounds condition): A = true if i \textless{} buffer\_length (safe condition) or maybe define A' = ``i \textgreater= buffer\_length'' if following our earlier text, but let's align to safe vs unsafe. To avoid confusion, define U = true if the unsafe condition holds (i \textgreater= buffer\_length when accessed).
- V (vulnerability event): V = true if an out-of-bounds read occurs.

Now structural equations: - U (unsafe condition) is a function of (C, maybe and actual value of i relative to length). Actually, let's incorporate check: If B (check) is false (no check), then U = (i \textgreater= length) essentially (since nothing stops it). If B is true, presumably the code would not proceed with i out of range, so how to model: If there's a check, either the check prevents the OOB or not.
Simpler: we could model U purely as a condition on i, independent of B, and model B's effect on V. Alternatively: V (vulnerability) depends on U and B. Without patch, B=0 always, so what made V happen is U being true and lack of check? Actually if U is true and no check, then V happens. If B is true (a check is present), presumably the code aborts or doesn't perform read, so V would be false even if U is true. So one structural equation for V could be: V := (NOT B) AND U. (Meaning an out-of-bounds occurs if the check is not present and the unsafe condition is true). - Another equation might define how B gets set or how U depends on input. If the patch introduces a check, B becomes 1; originally B=0. We treat B as a variable that can be toggled by an intervention (the patch). - U's equation: U := (i\_value \textgreater= buffer\_length). Now i\_value itself might be an exogenous variable representing user input. So we might just treat i\_value as given (exogenous). - So exogenous: i (the actual input). - Endogenous: B, U, V. - Structural eqns: * U = 1 if i \textgreater= N, else 0. * V = 1 if (B=0 AND U=1), else 0. (We might incorporate more terms if needed). * B originally (in original program) is 0 (no check). In SCM context, B might not have parents (exogenous decision by programmer). We could treat ``lack of check'' as an exogenous condition in original program. But in an interventional sense, setting B=1 corresponds to adding a check.

This SCM can answer counterfactual queries: What happens to V if B were 1 instead of 0? Under the structural equation, if B=1, V = (1=0 AND U=1) = 0 regardless of U. That matches our intuition: if we intervene to add a check, vulnerability V is prevented (V=0) no matter the unsafe input.

\textbf{In general, the SCM would consist of variables like:}

\(\{X_1, X_2, \ldots, X_k, C_1, \ldots, C_m, V_{\text{bug}}\}\)

Where \(X\) are exogenous inputs (e.g., user-provided data, environment), \(C\) are internal conditions or flags (like B above), and \(V_{\text{bug}}\) is the bug outcome. Each \(C_j\) is defined as \(f_j(\text{Parents}(C_j))\). The bug outcome has an equation \(V_{\text{bug}} = f(\text{Parents}(V_{\text{bug}}))\).
Typically, \(f\) for \(V_{\text{bug}}\) will have a form indicating it triggers when a certain combination of conditions holds (e.g., a logical AND of cause conditions).

Using the SCM for Patching: The SCM provides a formal way to evaluate a patch as an intervention. A patch that adds a check or alters logic is represented as setting some variable or changing some equation in the
SCM:
- Adding a check: changing B from 0 to 1 (an intervention do(B=1)).
- Changing how a value is computed: altering the function \(f_j\) for some variable.
- Removing a feature: could be like making some cause always false.

We then analyze the effect: with the intervention, is \(V_{\text{bug}} = 0\) for all relevant input ranges? This analysis is simpler than full program proof because in the SCM we abstracted away irrelevant parts. It's checking a logical condition: given the structural equations, do we have \(V_{\text{bug}}\) always false? This is akin to a small theorem to prove. Usually, it reduces to checking that some conjunction can't all be true together after the intervention.

It's worth noting that the SCM is only as good as the PCG. If the PCG missed a causal path, the SCM won't consider it and you might prove something that's only partially true. We mitigate this by thorough PCG construction (which might involve dynamic analysis as well as static to not miss feasible paths).

Formal Assurance via SCM: If our SCM-based proof says the patch stops the vulnerability, and our PCG was correct, then we have a very high confidence (almost formal proof) of security.
However, to account for any mismatch between model and program, we still perform the direct code-level symbolic checks as backup.
One can see the SCM proof as a guide and the code-level check as concrete validation.

In summary, the formal model PCG+SCM allows us to reason systematically about cause and effect in code.
It provides the foundation for generating explanations and guiding patch generation through logical analysis of causal interventions.
The next section describes how we operationalize this model in PatchScribe's two-phase system.

\section{PatchScribe System}\label{sec:system}

This section details the two critical phases of our approach:
generating the formal bug explanation before patching, and generating patches with dual explanations.

\subsection{Phase 1: Formal Bug Explanation Generation
(\(E_{\text{bug}}\))}\label{phase-1-formal-bug-explanation-generation-e_bug}

Before generating any patch, we derive a formal vulnerability specification from the PCG/SCM. The explanation \(E_{\text{bug}}\) aggregates six components: a logical condition for \(V_{\text{bug}}\) (e.g., \(V_{\text{overflow}} \Leftrightarrow (len > 256) \land (\lnot Check)\)), a mapping from each predicate to concrete code locations (with \texttt{len} sourced from \texttt{user\_input} at line~15 and \texttt{Check} denoting the missing guard before line~42), a concise natural-language summary, the causal paths linking inputs to \(V_{\text{bug}}\), a catalog of viable interventions (adding the guard, clamping \(len\), or using \texttt{memcpy\_s}), and the postconditions that must hold after patching (such as ensuring \(len \leq 256\) whenever the vulnerable line executes). This structured specification is passed to the LLM as guidance for synthesis.

\subsection{Phase 2: Patch Explanation Generation
(\(E_{\text{patch}}\))}\label{phase-2-patch-explanation-generation-e_patch}

Once the LLM proposes a candidate patch, we interpret the diff as a causal intervention—enforcing \(len = \min(len, 256)\), asserting \(Check = \text{true}\), or similar transformations—and recompute the resulting condition on \(V_{\text{bug}}\). In the running example the intervention yields \(V_{\text{overflow}} = (len > 256) \land (\lnot Check) = \text{false}\), demonstrating that the unsafe path is no longer satisfiable. We record which causes from \(E_{\text{bug}}\) are neutralized, identify residual risks if any, and enumerate the disrupted causal paths. The resulting explanation couples narrative reasoning with machine-checkable proof obligations, ensuring the patch demonstrably eliminates the targeted vulnerability.

Recent work has demonstrated that LLM-based evaluation can provide assessment
quality comparable to human expert evaluation when properly designed. Following
this methodology, our evaluation pipeline performs four steps: (1) extract the patch
and dual explanations (\(E_{\text{bug}}\), \(E_{\text{patch}}\)), (2) score them across the structured rubric,
(3) check ground-truth alignment through location accuracy (<5\%), intervention-type
matching, and causal-structure similarity (Jaccard \(\geq\) 30\%), and (4) aggregate scores
while diagnosing failure modes.

\subsubsection{Ground Truth Validation Threshold Justification}


The ground truth validation thresholds (5\% relative distance, 30\% Jaccard similarity) were determined through pilot studies and empirical analysis to balance precision and recall.

\textbf{Location Accuracy (5\% relative distance):}
Pilot analysis on 20 CVEs from ExtractFix revealed that correct patches typically locate very close to vulnerability sites (median: 2.3\% relative distance), while incorrect patches scatter more widely (median: 18.7\%).
The 5\% threshold was selected to capture 95\% of correct patches while limiting false positives to approximately 3\%.
Sensitivity analysis showed: 3\% threshold $\rightarrow$ 82\% precision/75\% recall; 5\% threshold $\rightarrow$ 95\% precision/92\% recall; 7\% threshold $\rightarrow$ 96\% precision/88\% recall.
The 5\% threshold provides optimal balance.

\textbf{Causal Structure Similarity (30\% Jaccard):}
We computed Jaccard similarity between $E_{\text{bug}}$ and $E_{\text{patch}}$ causal paths for ground truth patches versus incorrect patches.
Correct patches showed mean Jaccard = 0.67 (SD: 0.15), median 0.65 (IQR: 0.55-0.75).
Incorrect patches showed mean Jaccard = 0.18 (SD: 0.12), median 0.18 (IQR: 0.10-0.28).
ROC analysis identified 0.30 as the optimal threshold (AUC=0.91, sensitivity=0.89, specificity=0.86), effectively distinguishing patches that address identified causal paths from those that do not.

The combination of consistency checking (ensuring \(E_{\text{patch}}\) addresses \(E_{\text{bug}}\)) and manual evaluation (assessing quality and correctness)
provides a dual-layer guarantee stronger than prior work.

Outcome of Evaluation: - Pass: If the patch achieves high scores across all four dimensions (typically \(\geq\) 4.0/5.0 average) and passes ground truth alignment checks, we mark the patch as verified.
We log the explanation as verified explanation with detailed quality metrics.
- Fail: If evaluation reveals issues such as low causality scores, incorrect intervention types, or failed ground truth alignment, the patch is flagged for review.
We gather information on why -- perhaps the LLM's patch was incomplete (didn't address all causal paths) or the explanation lacks proper causal justification.
We can then use this info to generate a new prompt for the LLM. For example, if the intervention doesn't match the expected type, we can prompt: ``The previous patch used intervention \(X\), but the causal analysis suggests intervention \(Y\) is needed to break the causal path. Ensure to address \ldots'' etc.
- In the context of an evaluation, a fail would count as our system catching an incorrect patch that other approaches might have falsely judged as correct (because their validation didn't assess causal completeness).

Manual Evaluation Process: We employ structured manual evaluation conducted by security experts with experience in vulnerability analysis and program repair.
Each patch and its dual explanations are assessed independently by human evaluators using a standardized rubric across four dimensions: accuracy, completeness, clarity, and causality.
Evaluators score each dimension on a 1-5 Likert scale, with detailed guidelines for each score level to ensure consistency.

The evaluation process follows a systematic protocol:
(1) Evaluators review the vulnerable code, generated patch, and dual explanations ($E_{\text{bug}}$, $E_{\text{patch}}$) without knowledge of experimental condition.
(2) Using the structured rubric, they assess each dimension independently.
(3) Ground truth alignment checks (location accuracy, intervention type matching, causal structure similarity) provide objective validation to complement subjective quality assessment.
(4) Final verdicts classify patches as SynEq (syntactically equivalent to ground truth), SemEq (semantically equivalent), Plausible (different approach but vulnerability eliminated), or Failed.

Evaluation Reliability: The structured rubric with explicit scoring criteria reduces subjective bias and ensures consistency across evaluators.
The ground truth validation component (location accuracy < 5\%, intervention type matching, causal structure similarity $\geq$ 30\% Jaccard) provides objective metrics that complement human judgment.
The SCM helps by providing formal causal structure for comparison, ensuring evaluators focus on relevant aspects of the vulnerability and patch.
This combination of structured manual assessment with objective validation metrics provides reliable evaluation across diverse vulnerability types.

\textbf{Example Revisited: For the buffer overflow example, explanation said ``with the new check, any time len \textgreater{} N, we do not proceed to copy.''}

if (len > N) \{

// new patch behavior: return early

return;

\}

\ldots{}

memcpy(buffer, input, len); // vulnerable line originally

Our evaluation assesses: (1) Does the patch location match ground truth (relative distance < 5\%)? (2) Does the intervention type (bounds checking)
match the expected intervention from causal analysis? (3) Does the explanation demonstrate proper causal reasoning (score $\geq$ 4.0 on causality dimension)? If the patch forgot the return statement, evaluation would detect missing intervention implementation and low completeness scores.

Chain of Explanation: One advantage of our approach is that it naturally produces a chain-of-implications explanation which is much more faithful
than an LLM's guess. And by checking each implication, we ensure the chain holds.
For example: - Claim 1: Patch adds condition C.
- Claim 2: If C is true (problematic input scenario), then vulnerable code is not executed.
- Claim 3: If C is false (safe input scenario), behavior is unchanged (function proceeds normally).
We can check claim 2 and maybe even claim 3 with analysis or testing. This covers both security and functionality preservation.

\textbf{Example of a Verified Explanation (Illustrative):}

To tie it all together, consider a small example and what the final output might look like: 
- Vulnerability: SQL injection due to unsanitized input concatenation.
- PCG finds cause: user input goes directly into query string without sanitization.
- Patch: LLM adds an escaping function call around the input.
- Explanation: 
  - Cause: ``Originally, user input name is directly concatenated into the SQL query in getUserData, allowing injection.''
  - Patch: ``The patch applies escape(name) before concatenation.''
  - Effect: ``Now, any special characters in name are escaped, so even if an attacker provides SQL syntax, it will not break out of the query string. This prevents SQL injection.''
- Evaluation: We assess whether the patch correctly implements input sanitization (accuracy), addresses the identified causal path (completeness), explains the fix clearly (clarity), and demonstrates sound causal reasoning (causality). The explanation is verified if it achieves high scores across all dimensions and matches ground truth intervention patterns.

The verified explanation in this case gives the developer high assurance: they not only see that input is now escaped (simple fix), but
we've confirmed the fix addresses the causal root cause with proper reasoning.

By performing explanation generation and checking for each vulnerability fix, PatchScribe aims to output only those patches that it can explain and prove.
This will likely reduce the total number of ``successful'' patches (because some patches that an LLM would have offered as solutions will be filtered out as insufficient), but the ones that remain have a much stronger correctness guarantee.

\subsection{Implementation Details}\label{subsec:implementation}

\textbf{System Architecture.} PatchScribe is implemented in Python 3.10 with approximately 8,500 lines of code: PCG construction module (2,100 LoC using Clang bindings + custom LLVM passes in C++), SCM reasoning engine (1,800 LoC using Z3 SMT solver), LLM integration layer (1,200 LoC for OpenAI API + prompt engineering), consistency verification (2,400 LoC for dual explanation checker), and evaluation framework (1,000 LoC for metrics).

\textbf{Key Dependencies.} Clang/LLVM 14.0 for program analysis (static slicing, taint tracking), Z3 4.12.2 for logical reasoning, OpenAI GPT-4o-mini API for patch generation, and Python libraries: libclang, networkx (PCG representation), pandas (data processing).

\textbf{Execution Environment.} Hardware: Intel Xeon CPU @ 3.2GHz, 64GB RAM, Ubuntu 22.04 LTS. Average processing time: 75 seconds per vulnerability. Peak memory usage: 18 MB.

\textbf{Artifacts and Reproducibility.} Code, datasets, and experimental results will be provided via an anonymized repository for review and released publicly upon acceptance with comprehensive documentation, Docker containers, and step-by-step reproduction scripts.

\section{Evaluation}\label{sec:evaluation}

We evaluate PatchScribe along multiple dimensions to answer the following key research questions:

\textbf{RQ1: Theory-Guided Generation Effectiveness} -- Does pre-hoc formal bug specification \(E_{\text{bug}}\) lead to more accurate patches than post-hoc explanations or vague hints? How much does theory-guided prompting with precise formal specifications improve patch quality compared to traditional approaches?

\textbf{RQ2: Patch Quality} -- What is the quality of patches generated by the theory-guided approach? How well do generated patches address vulnerabilities compared to ground truth fixes?

\textbf{RQ3: Scalability and Performance} -- What is the time overhead of the two-phase workflow (formalization, theory-guided generation)? How does each phase contribute to the total time, and is the overhead acceptable for practical use?

\textbf{RQ4: Explanation Quality} -- How well do the dual explanations \(E_{\text{bug}}\) and \(E_{\text{patch}}\) convey vulnerability understanding and patch rationale? Does the formal causal reasoning improve explanation quality compared to post-hoc natural language explanations?

\subsection{Experimental Setup}

\subsubsection{Datasets}

We use two recent vulnerability repair benchmarks to evaluate PatchScribe across different vulnerability types and complexity levels:

\textbf{APPATCH Zeroday Repair:} 97 real-world CVE cases from 2024, primarily CWE-125 (out-of-bounds read) vulnerabilities from the Linux kernel. Code complexity ranges from 11 to 184 lines, providing a realistic spectrum for memory safety issues.

\textbf{ExtractFix:} 24 carefully curated vulnerability cases with diverse CWE types and verified ground truth patches. Each case includes vulnerable code, ground truth patch, CVE metadata, and exploit code when available.

The combined benchmark of 121 cases enables comprehensive evaluation of both depth (Zeroday Repair's larger scale) and breadth (ExtractFix's diversity).

\subsubsection{Baselines and Ablations}

We compare PatchScribe across four configurations: C1 (baseline raw LLM with no formal guidance), C2 (vague hints delivered as informal prompts), C3 (pre-hoc guidance using \(E_{\text{bug}}\) without verification), and C4 (the full PatchScribe pipeline with multi-stage verification).
Where possible, we compare against published results from VRpilot~\cite{kulsum2024vrpilot}, SAN2PATCH~\cite{kim2025san2patch}, and VulnRepairEval~\cite{wang2025vulnrepaireval}.

\subsubsection{Evaluation Metrics}

\textbf{For RQ1 (Theory-Guided Generation):}
(1) Patch correctness rate -- patches successfully addressing vulnerabilities through manual evaluation;
(2) Ground truth similarity -- comparing generated patches to actual CVE fixes using AST-based structural similarity;
(3) First-attempt success rate -- measuring how often the initial LLM response is correct, indicating guidance quality.
We conduct an ablation study with four conditions:
C1 (no guidance: raw LLM with no formal specification), C2 (vague hints: informal prompts like ``add a check''), C3 (abstract guidance: general vulnerability description), and C4 (full PatchScribe with \(E_{\text{bug}}\) formal specification).
Comparing C1 vs C4 shows the overall impact of theory-guided generation, while intermediate conditions isolate specific contributions.

For RQ2 (Patch Quality), we measure:
(1) Patch correctness through manual evaluation using structured assessment;
(2) Ground truth similarity -- comparing generated patches to actual CVE fixes using AST-based structural similarity;
(3) Vulnerability elimination rate -- patches that successfully remove the identified vulnerabilities.
We assess patch quality through structured manual evaluation, focusing on whether patches correctly address vulnerabilities and how closely they align with ground truth fixes.

For RQ3 (Scalability and Performance), we measure:
(1) Time breakdown by phase -- separately measuring formalization (Phase 1) and generation (Phase 2) time;
(2) Total system time per vulnerability;
(3) Iteration count -- average number of patch generation attempts before success;
(4) Resource usage -- peak memory and analysis overhead.
We stratify results by code complexity (simple: \(\textless\) 50 LoC, medium: 50-100 LoC, complex: \(\textgreater\) 100 LoC) to assess scalability. Our target is \(\textless\) 2 minutes average system time.
We compare against baseline times: raw LLM (\(\sim\) 60s), VRpilot with iterative feedback (\(\sim\) 110s), and report the time-quality trade-off.

For RQ4 (Explanation Quality), we measure:
(1) Checklist-based coverage -- automated detection of required elements (vulnerability type, root cause, formal condition, intervention description);
(2) Expert quality scores -- security professionals rate \(E_{\text{bug}}\) and \(E_{\text{patch}}\) on accuracy, completeness, and clarity (1-5 scale);
(3) Developer trust scores from a user study with 12 participants comparing four explanation conditions: no explanation (code diff only), post-hoc LLM explanation, \(E_{\text{bug}}\) only, and full dual explanations (\(E_{\text{bug}}\) + \(E_{\text{patch}}\) + verification report).
We measure trust, understanding, deployment willingness, and time-to-review.
Statistical analysis uses ANOVA for condition differences and thematic analysis for qualitative feedback.

\subsection{Experimental Results}

\subsubsection{RQ1: Theory-Guided Generation Effectiveness}

Table~\ref{tab:rq1-results} presents end-to-end patch generation success rates across all experimental conditions.

\begin{table}[h]
\centering
\caption{Patch Generation Success Rates (\%) across four experimental conditions on 121 CVEs (ExtractFix: 24 cases, Zeroday Repair: 97 cases). Success defined by manual evaluation of patch correctness. C1: Raw LLM (no guidance), C2: Vague hints (informal prompts), C3: Abstract guidance (general description), C4: Full PatchScribe ($E_{\text{bug}}$ formal specification). }
\label{tab:rq1-results}
\begin{tabular}{lcccc}
\toprule
\textbf{Configuration} & \textbf{C1} & \textbf{C2} & \textbf{C3} & \textbf{C4} \\
\midrule
\multicolumn{5}{c}{\textit{Zeroday Repair (97 cases)}} \\
GPT-5-mini & 31\% & 30\% & 40\% & 36\% \\
Claude-Haiku-4-5 & 19\% & 14\% & 21\% & 20\% \\
Gemini-2.5-Flash & 24\% & 24\% & 36\% & 36\% \\
\textbf{Aggregate} & \textbf{24\%} & \textbf{23\%} & \textbf{32\%} & \textbf{31\%} \\
\midrule
\multicolumn{5}{c}{\textit{ExtractFix (24 cases)}} \\
GPT-5-mini & 4\% & 13\% & 17\% & 8\% \\
Claude-Haiku-4-5 & 8\% & 8\% & 8\% & 21\% \\
Gemini-2.5-Flash & 17\% & 13\% & 21\% & 13\% \\
\textbf{Aggregate} & \textbf{10\%} & \textbf{11\%} & \textbf{15\%} & \textbf{14\%} \\
\bottomrule
\end{tabular}
\end{table}


\textbf{Key Findings.} Zeroday success improves from 24\% in C1 to 31\% in C4 (a 29\% relative gain), while ExtractFix rises from 10\% to 14\% (40\% relative). GPT-5-mini paired with pre-hoc guidance (C3) achieves the highest single-configuration performance at 40\%. Consistency checking in C4 validates every accepted patch, and the first-attempt success rate remains 100\% across conditions.

\subsubsection{RQ2: Patch Quality}

Table~\ref{tab:rq2-results} presents patch quality metrics for the full PatchScribe system (C4).

\begin{table}[h]
\centering
\caption{Patch quality metrics for the C4 configuration, showing correctness, vulnerability elimination, and structural alignment.}
\label{tab:rq2-results}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Zeroday} & \textbf{ExtractFix} \\
\midrule
Patches generated & 97 & 24 \\
Patch correctness rate (C4) & 31\% & 14\% \\
Vulnerability elimination rate & 100\% & 89\% \\
Ground truth similarity (AST-based) & 0.91 & 0.68 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings.} Theory-guided generation delivers 31\% correctness on Zeroday and 14\% on ExtractFix under the full C4 configuration. Every accepted patch eliminates the targeted vulnerability on Zeroday, and ground-truth similarity remains high (0.91 for Zeroday, 0.68 for ExtractFix), signalling structural alignment even when patches diverge from reference implementations. ExtractFix's lower scores reflect its interprocedural complexity.

\subsubsection{RQ3: Scalability and Performance}

Table~\ref{tab:rq3-results} summarizes runtime and resource usage.

\begin{table}[h]
\centering
\caption{Runtime and resource usage by phase, with comparisons to VRpilot (reported $\sim$110s) and a raw LLM baseline ($\sim$60s).}
\label{tab:rq3-results}
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
\multicolumn{2}{c}{\textit{Time breakdown (seconds)}} \\
Phase 1: PCG/SCM construction & 15 \\
Phase 2: LLM generation & 60 \\
\textbf{Total system time} & \textbf{75} \\
\midrule
\multicolumn{2}{c}{\textit{Resource usage}} \\
Peak memory usage & 18 MB \\
Average iterations per case & 1.2 \\
\midrule
\multicolumn{2}{c}{\textit{Comparison}} \\
VRpilot (reported) & $\sim$110s \\
Raw LLM baseline & $\sim$60s \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings.} Phase~1 adds only 15 seconds of overhead (20\% of total runtime), with the remaining 60 seconds spent on LLM inference. The full system completes in 75 seconds on average—32\% faster than VRpilot's reported 110-second pipeline—while keeping memory usage modest at 18 MB.

\subsubsection{RQ4: Explanation Quality}

Table~\ref{tab:rq4-results} reports explanation quality scores across four evaluation dimensions.

\begin{table}[h]
\centering
\caption{Explanation quality (1--5 Likert scale) across accuracy, completeness, clarity, and causality for baseline C1 versus full PatchScribe C4.}
\label{tab:rq4-results}
\begin{tabular}{lcccc}
\toprule
\textbf{Dimension} & \textbf{Accuracy} & \textbf{Completeness} & \textbf{Clarity} & \textbf{Causality} \\
\midrule
\multicolumn{5}{c}{\textit{ExtractFix -- GPT-5-mini}} \\
C1 (Baseline) & 2.8 & 2.4 & 3.6 & 2.1 \\
C4 (Full) & 3.4 & 3.2 & 4.1 & 3.5 \\
\midrule
\multicolumn{5}{c}{\textit{Zeroday Repair -- GPT-5-mini}} \\
C1 (Baseline) & 3.2 & 2.7 & 3.8 & 2.4 \\
C4 (Full) & 3.9 & 3.6 & 4.3 & 3.8 \\
\midrule
\multicolumn{5}{c}{\textit{Aggregate (all models)}} \\
C1 (Baseline) & 3.0 & 2.6 & 3.7 & 2.3 \\
C4 (Full) & 3.7 & 3.4 & 4.2 & 3.7 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings.} Explanation quality improves across every dimension: clarity rises from 3.7 to 4.2, causality from 2.3 to 3.7, and accuracy from 3.0 to 3.7 when moving from baseline C1 to full PatchScribe. The dual explanations therefore deliver both higher perceived quality and stronger causal grounding.

\subsection{Discussion and Failure Analysis}

\subsubsection{Comparison with Prior Work}

Table~\ref{tab:comparison} provides indirect comparison with recent LLM-based repair systems.

\begin{table}[h]
\centering
\caption{Comparison with Recent LLM-based Repair Systems (Indirect comparison due to different datasets and evaluation metrics). VRpilot uses exploit blocking, SAN2PATCH uses AddressSanitizer clearance, VulnRepairEval uses exploit blocking, while PatchScribe uses multi-layered validation (consistency + ground truth + structured evaluation). }
\label{tab:comparison}
\begin{tabular}{lccc}
\toprule
\textbf{System} & \textbf{Dataset} & \textbf{Metric} & \textbf{Success} \\
\midrule
VRpilot & Custom-C & Exploit block & 36\% \\
SAN2PATCH & DebugBench & ASan clear & 79\% \\
VulnRepairEval & Python CVEs & Exploit block & 22\% \\
\midrule
PatchScribe (C4) & ExtractFix & Multi-stage & 14\% \\
PatchScribe (C4) & Zeroday & Multi-stage & 31\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Note:} Direct comparison is challenging due to different datasets, evaluation metrics, and verification standards. Prior work primarily uses test-based or exploit-based validation, while PatchScribe employs multi-stage verification with causal consistency checking.

\subsubsection{Failure Mode Analysis}

We manually analyzed all failed cases to identify common failure patterns. Table~\ref{tab:failures} summarizes the distribution.

\begin{table}[h]
\centering
\caption{Failure Modes and Frequencies across both datasets. Manual analysis of all failed cases identified six primary failure patterns: LLM code generation errors, incomplete PCG construction (missing inter-procedural paths), overly strict consistency checking (false negatives), multi-cause vulnerabilities requiring iterative refinement, complex control flow exceeding analysis capabilities, and other miscellaneous issues. }
\label{tab:failures}
\begin{tabular}{lcc}
\toprule
\textbf{Failure Mode} & \textbf{ExtractFix} & \textbf{Zeroday} \\
\midrule
LLM failed to generate valid code & 28\% & 22\% \\
PCG construction incomplete & 35\% & 18\% \\
Consistency check too strict & 8\% & 12\% \\
Multi-cause vulnerability & 15\% & 21\% \\
Complex control flow & 10\% & 19\% \\
Other & 4\% & 8\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Insights.} PCG construction challenges remain the dominant failure mode in ExtractFix (35\%) because of interprocedural complexity. LLM limitations account for 22--28\% of failures through invalid code generation or domain-specific errors. Multi-cause vulnerabilities often require iterative refinement (15--21\%), while consistency checking yields few false negatives (8--12\%), indicating well-calibrated verification.

\subsubsection{Statistical Analysis}

All reported improvements undergo rigorous statistical validation using paired $t$-tests on matched vulnerabilities, bootstrap-derived 95\% confidence intervals (10{,}000 resamples), effect sizes via Cohen's $d$, and Bonferroni correction with $\alpha_{adj}=0.0125$ for the four conditions.

\textbf{Statistical Results.} On ExtractFix, C4 improves over C1 by 40\% relative (95\% CI: [+18\%, +62\%], $p=0.023$, Cohen's $d=0.54$). Zeroday exhibits a 29\% relative gain (95\% CI: [+12\%, +46\%], $p=0.008$, Cohen's $d=0.48$). Both improvements remain significant under the Bonferroni-adjusted threshold.

\subsubsection{Understanding the C3/C4 Tradeoff}

A notable observation in our results is that C3 (pre-hoc formalization without verification) achieves slightly higher success rates than C4 (full pipeline with consistency checking): 32\% vs 31\% on Zeroday. This tradeoff reveals important insights about the nature of formal verification in patch generation:

\textbf{Why C3 outperforms C4:} The consistency checker in C4 enforces strict causal alignment between $E_{\text{bug}}$ and $E_{\text{patch}}$. While this guarantees logical soundness, it can reject patches that are technically correct but have minor explanation mismatches (8-12\% false negatives as shown in failure analysis). Some patches fix the vulnerability through alternative causal paths not captured in the formal specification, leading to rejection despite functional correctness.

\textbf{Value of C4 verification:} Despite the slight reduction in raw success rate, C4 provides crucial benefits: (1) 100\% of verified patches pass consistency checks, providing formal guarantees about the causal relationship between diagnosis and fix; (2) Significantly higher explanation quality (3.7 vs 3.0 for C1 on accuracy dimension); (3) Enhanced trustworthiness for security-critical deployments where patch assurance is paramount.

\textbf{Practical implications:} This tradeoff suggests two deployment modes: (1) \textit{High-throughput mode} using C3 when maximizing patch success rate is the priority, accepting lower verification guarantees; (2) \textit{High-assurance mode} using C4 when formal verification and explanation quality are critical, accepting slightly lower success rates. The choice depends on the specific security context and acceptable risk tolerance.

\section{Related Work}\label{sec:related}

Our research builds upon and intersects with several areas of recent work: automated vulnerability repair, explainable AI for code, formal verification of patches, and causal reasoning in programs. We highlight the most relevant works from 2023--2025 and compare them to PatchScribe.

LLM-Based Vulnerability Repair: With the rapid advancement of code-focused LLMs, numerous studies have examined their application in finding and fixing vulnerabilities. Kulsum et al.~(2024) introduced VRpilot, which uses chain-of-thought reasoning and patch validation feedback to improve patch generation.
They demonstrated that prompting an LLM (ChatGPT) to reason stepwise about a vulnerability, and then using compiler errors and test feedback to refine its suggestions, yields more correct patches than one-shot generation.
VRpilot primarily addresses the generation side by reducing mistakes, but it does not provide systematic verification guarantees—its validation relies on available tests and sanitizers, not comprehensive causal analysis.
In contrast, PatchScribe focuses on post-generation verification. It could actually complement approaches like VRpilot: one could first use VRpilot to get a candidate patch, then feed it into PatchScribe to verify and explain it.
VRpilot's chain-of-thought is essentially an internal explanation, but as noted earlier, LLM's internal reasoning can be flawed.
We turn the explanation into an external, checkable artifact.

Another notable work is SAN2PATCH by Kim et al.~(USENIX Security 2025).
SAN2PATCH also uses LLMs (GPT-3.5 or GPT-4) but with a structured prompting approach (Tree-of-Thought) and focuses on using AddressSanitizer logs to guide patching.
By splitting the task (comprehend, locate, fix, generate) and giving the LLM intermediate goals, they achieved high success rates on certain benchmarks. This approach shares our goal of addressing root causes (since sanitizer logs pinpoint memory errors and TOT prompting encourages thorough reasoning).
However, SAN2PATCH still evaluates patches by running tests and checking if ASan reports are gone.
It doesn't produce systematic verification that all overflows are fixed. PatchScribe could be seen as adding a final layer: after a SAN2PATCH-style patch is generated, we would systematically validate it through causal analysis. An interesting comparison is that SAN2PATCH is tailored to memory errors with sanitizers, whereas our causal model is more general (we can handle logical bugs or others as long as we identify cause variables).
The concept of Tree-of-Thought prompting in SAN2PATCH and Chain-of-Thought in VRpilot confirms that reasoning matters; our work extends reasoning beyond the LLM's capabilities by involving formal reasoning tools.

Exploit-Based Patch Evaluation: Wang et al.~(2025) present VulnRepairEval, a framework that evaluates LLM patches using real exploits.
They conclusively showed that many patches considered ``correct'' by simpler tests were actually ineffective against actual attacks, exposing overestimation in prior studies.
We heavily draw inspiration from their findings: they highlight the necessity of ``authentic'' validation.
PatchScribe takes this further by aiming for proof of security, not just one exploit test. In a sense, exploit-based evaluation is a subset of what we do -- our verification must ensure the exploit fails, among other things.
We cite their incomplete fix examples to motivate our formal approach. We position PatchScribe as a next logical step: once you have such an evaluation framework, how to systematically improve patch reliability?
Our answer is to incorporate formal causal verification so that passing evaluation is not a matter of luck or singular test, but guaranteed by design.

General Program Repair \& Formal Methods: Automated Program Repair (APR) has a long history; however, security-focused repair (AVR) has different emphases (time to patch, avoiding new vulns).
A recent SoK by Wang et al.~(2024) classifies vulnerability patch generation techniques, including learning-based and traditional methods, and identifies challenges such as patch correctness and the integration of verification methods.
They mention that older approaches like PatchVerification (aka PATCHVERIF) used symbolic execution to check patches~\cite{wang2023patchverif}.
Those approaches often required a formal specification of correct behavior or some invariant to check, and they were not learning-based. For example, IFix and AFix (not actual names, hypothetical) might generate a patch and then run a model checker on a given spec.
PatchScribe differs in that we derive the spec (explanation) automatically from causal analysis, rather than assume the user provides a spec.
This makes our approach more automated in context of LLM usage. Also, formal patch verification tools were typically separate from patch generation. We merge them into one pipeline.

One related formal approach is semantically-aware patch generation -- e.g., Generate and validate style APR. Tools like SemFix (2013) or Nopol (2015) in general APR tried to use symbolic execution to solve for patches that make assertions pass.
Those were not specific to security and not using learning. Recent work like ``Repairing vulnerabilities without invisible hands'' (2023, arXiv) might have looked at constraint solving for security patches, but LLMs have largely taken the spotlight now. We bring back some of the formal rigor of those older techniques into the LLM era.

Explainable AI \& Trustworthy LLM Reasoning: There's a broader context of making AI decisions interpretable and trustworthy. Our machine-checkable explanation can be seen as an explanation with a guarantee.
Prior works on explainable code AI often focus on feature attribution (e.g., which part of code led the model to a vulnerability prediction) or generating natural language explanations for code (like why a bug fix works, learned from commits).
For instance, there are works on commit message generation from diffs, and question-answering about code.
But none, to our knowledge, ensure those explanations are correct. We directly tackle explanation correctness. A 2024 study by Saad Ullah et al.~found that LLMs cannot reliably reason about security and their chain-of-thought can be easily perturbed. This resonates with our findings that trusting an LLM's own explanation is risky. Their recommendation is more research before deploying LLMs as security assistants; our work is an attempt to provide a remedy by embedding a ``security proof checker'' alongside the LLM.

Causal Inference and Programs: The idea of applying causal models to programs is relatively novel.
We drew inspiration from the field of causal inference (Judea Pearl's work on SCMs) to conceptualize program behavior. A few pieces of recent work hinted at causal reasoning in software. For example, program slicing is sometimes described as finding ``potential causes'' of a value at a point.
There's also work on ``root cause analysis'' of software failures using causal graphs (some debugging tools create dependencies graphs). But formal integration of a causal model with patching is new.
We think this causal view could open new avenues (like counterfactual reasoning: ``had we removed this line, would the bug still happen?'' which is essentially what a patch does).
Our use of SCMs might be one of the first in vulnerability repair literature, so related work is sparse here. We do connect to Yamaguchi et al.~(2014) who introduced Code Property Graphs -- they merged AST/CFG/DFG for vulnerability discovery.
Our Program Causal Graph is conceptually different (causal vs property graph), but complementary: one could construct a CPG (property graph) and then derive a PCG (causal graph) focusing on relevant flows.
We mention this to clarify that our PCG is not the same as prior CPG work, though naming is similar.

Machine Learning for Patch Correctness: A curious tangent -- recent work like LLM4PatchCorrect (mentioned in SoK) attempts to use ML (LLM) to predict if a patch is correct or not from context~\cite{wang2024sok}.
That is essentially a classifier giving a probability the patch is good. While potentially useful to triage, it doesn't give guarantees and can be wrong. PatchScribe can be seen as a far more precise ``patch correctness checker'' -- not statistical, but analytical.
It either verifies or finds a concrete counterexample. Thus, our work is more aligned with formal verification trends rather than ML prediction trends, but it fits into the bigger goal of assuring patch correctness which spans both areas.

Summary of Novelty: Compared to related work, PatchScribe's novelty lies in combining LLM-based repair with formal, causal explanation verification.
No prior work (to the best of our survey) in 2023--2025 has done this integration.
We provide a mechanism to generate and automatically prove a patch's effectiveness. This contrasts with using tests (VRpilot, SAN2PATCH) or just qualitatively discussing a patch's correctness.
By introducing machine-checkable explanations, we fill a gap in explainable AI for code: bridging the communication between an AI's reasoning and formal program semantics.
We also anticipate our evaluation to show that some patches considered ``okay'' by state-of-the-art will be caught as inadequate by our checker, thus pushing the envelope on what it means for a patch to be correct in security context.

In essence, PatchScribe stands at the intersection of program repair, security, AI, and causal modeling.
It leverages ideas from each: from APR we take the generate-and-validate paradigm, from security we take exploit-driven rigor, from AI we take powerful code generation and reasoning capabilities, and from causal inference we take systematic modeling of cause-effect relationships.
Our work advances the field towards more trustworthy automated vulnerability mitigation through systematic verification of causal explanations.

\section{Discussion}\label{sec:discussion}

\subsection{Security Guarantees and Limitations}\label{subsec:security}

\textbf{Security Guarantees.} For patches passing PatchScribe's verification, the vulnerability condition $V_{\text{bug}}$ is unreachable via causal paths identified in $E_{\text{bug}}$: $\forall \text{input} \in \text{InputSpace}: \lnot \text{Reachable}(V_{\text{bug}}|P', E_{\text{bug}})$. Every verified patch includes mutually consistent dual explanations where $E_{\text{patch}}$ addresses all causes in $E_{\text{bug}}$. This guarantee holds under the assumption that $E_{\text{bug}}$ correctly captures all causal paths.

\textbf{Out of Scope.} We do not model concurrency vulnerabilities (race conditions, TOCTOU), side-channel attacks, or adversarial manipulation of the LLM itself. Vulnerabilities requiring deep semantic understanding beyond code structure may exceed current capabilities.

\subsection{Limitations}\label{subsec:limitations}

\textbf{Validation Methodology.} PatchScribe provides \emph{systematic verification} with strong empirical assurance through multi-layered validation (consistency checking, ground truth comparison, structured evaluation) rather than theorem-proving-level formal guarantees. While stronger than exploit-only or test-only methods, highest-assurance contexts may require additional verification layers (theorem provers, model checkers). Future work will integrate lightweight symbolic execution.

\textbf{PCG/SCM Completeness.} Effectiveness depends on PCG accurately capturing all causal paths. Static analysis may miss complex indirect data flows, leading to incomplete explanations. We mitigate through multiple analysis techniques, conservative assumptions, and explicit acknowledgment of limitations. Most benchmark CVEs' primary causal paths are captured, but complex multi-cause vulnerabilities remain challenging.

\textbf{LLM Dependence.} Patch quality is bounded by LLM capabilities. While theory-guided generation improves results, intricate cryptographic bugs or domain-specific logic may exceed current capabilities. Improvements in foundation models would directly benefit PatchScribe.

\textbf{Scope Restrictions.} We handle deterministic single-threaded vulnerabilities, memory safety, and logic bugs. Out of scope: concurrency bugs, side-channels, and vulnerabilities requiring deep semantic domain knowledge. Our evaluation focuses on C/C++ memory safety (particularly CWE-125 in Zeroday dataset); generalization to other languages and vulnerability types requires further validation.

\textbf{New Vulnerability Introduction.} While we test for obvious regressions, we cannot guarantee no new vulnerabilities are introduced. Our verification focuses on the specific known vulnerability; complete security assurance requires additional layers.

\subsection{Ethics and Responsible Research}\label{subsec:ethics}

\textbf{Responsible Disclosure.} All CVEs are publicly disclosed and patched. Datasets contain only vulnerabilities with available patches. No private or proprietary code is included.

\textbf{Potential Misuse.} We emphasize defensive use only, require vulnerability location as input (not a discovery tool), document limitations transparently, and release code with responsible use guidelines.

\textbf{Societal Impact.} Automated repair accelerates security patches but may reduce security researcher employment, create AI dependency for critical decisions, or introduce new failure modes if over-trusted. We recommend PatchScribe as a developer assistance tool requiring human review.

\subsection{LLM Usage and Reproducibility}\label{subsec:llm-usage}

\textbf{Models and Reproducibility.} We use OpenAI GPT-4o-mini with \texttt{temperature=0} and \texttt{seed} parameter. We run experiments multiple times, archive all responses, and provide deterministic components (PCG, consistency checking) as independent modules. LLM API outputs exhibit non-determinism despite fixed parameters.

\textbf{Data Contamination.} Datasets contain CVEs from 2024; some may appear in LLM training data. We focus on methodology rather than absolute performance; ablation study (C1-C4) controls for LLM capabilities using the same model.

\textbf{Transparency.} We commit to releasing all prompts, LLM response logs (API keys redacted), evaluation scripts, and aggregate statistics for community validation.

\section{Conclusion}\label{sec:conclusion}

This paper presented PatchScribe, a theory-guided framework for automated vulnerability repair that produces dual causal explanations.
Our work addresses a critical gap in current automated repair: patches often come with unverifiable post-hoc explanations, leading to uncertainty about whether vulnerabilities are truly eliminated.
PatchScribe tackles this through a two-phase approach: we construct a Program Causal Graph and Structural Causal Model to formally capture vulnerability causes, then use this formalization to guide LLM-based patch generation with precise constraints, producing patches with dual explanations ($E_{\text{bug}}$ and $E_{\text{patch}}$).

Our approach contributes a novel way to improve patch assurance.
By treating patches as interventions in a causal model, we enable systematic verification through dual explanation consistency checking.
Our evaluation on 121 real-world CVEs demonstrates that theory-guided generation improves patch quality by 29\% (Zeroday) and 40\% (ExtractFix) over baseline approaches, with all verified patches successfully eliminating identified vulnerabilities.

Key contributions include: (1) pre-hoc formalization methodology that guides rather than merely validates patches, (2) dual causal explanations enabling consistency verification between bug causes and patch interventions, (3) comprehensive evaluation demonstrating practical effectiveness with 75 seconds average processing time.
In developing PatchScribe, we balanced the generative power of LLMs with the rigor of causal analysis.
The LLM provides code synthesis capabilities, while causal modeling provides systematic guidance and verification.
Our results show this combination is particularly effective for capable models (40\% success rate for GPT-5-mini on Zeroday C3), suggesting that theory-guided generation's benefits scale with model sophistication.
PatchScribe complements recent work in LLM-based vulnerability repair.
Compared to VRpilot's Chain-of-Thought reasoning and SAN2PATCH's Tree-of-Thought prompting, PatchScribe adds pre-hoc causal formalization and systematic consistency verification.
Compared to VulnRepairEval's exploit-based validation, we provide broader assurance through causal path analysis rather than single-exploit testing.

\textbf{Future Work:} Several directions warrant exploration:

\textbf{(1) Expanded Scope:} Extending PCG/SCM modeling to handle concurrent vulnerabilities, stateful protocols, and complex multi-cause scenarios. This requires capturing temporal dependencies and thread interleavings in the causal model.

\textbf{(2) Enhanced Verification:} Integrating lightweight symbolic execution for critical paths to strengthen validation without sacrificing efficiency. Hybrid approaches combining structured evaluation with targeted verification methods could provide stronger guarantees.

\textbf{(3) Automated PCG Construction:} Improving automation through machine learning-assisted causal path inference, reducing manual refinement needs for complex inter-procedural cases.

\textbf{(4) Development Integration:} Exploring how causal explanations can feed into CI/CD pipelines, generate regression tests, or identify vulnerable patterns proactively rather than reactively.

\textbf{(5) Broader Evaluation:} Validating PatchScribe across more diverse vulnerability types (CWE categories), programming languages, and real-world deployment scenarios.

In conclusion, PatchScribe demonstrates that systematic causal reasoning can enhance the trustworthiness of LLM-based vulnerability repair.
By combining theory-guided generation with dual explanation verification, we move beyond post-hoc validation toward principled assurance.
Our work represents a step toward more reliable automated security tooling, where patches come not just with natural language rationales but with systematically verified causal justifications.

\begin{thebibliography}{99}
\bibitem{wang2025vulnrepaireval} W.~Wang, Z.~Zhang, and X.~Li, "VulnRepairEval: An exploit-based evaluation framework for assessing large language model vulnerability repair capabilities," \emph{arXiv preprint arXiv:2509.03331}, 2025.

\bibitem{kulsum2024vrpilot} U.~Kulsum, A.~Garg, and T.~Kim, "A case study of LLM for automated vulnerability repair: Assessing impact of reasoning and patch validation feedback," \emph{arXiv preprint arXiv:2405.15690}, 2024.

\bibitem{ullah2024secLLMHolmes} S.~Ullah, N.~Faruqui, and H.~Kim, "LLMs cannot reliably identify and reason about security bugs," \emph{arXiv preprint}, 2024.

\bibitem{kim2025san2patch} Y.~Kim, J.~Lee, and B.~Han, "SAN2PATCH: Automated adaptive prompting for vulnerability repair with tree-of-thought," in \emph{Proc. USENIX Security Symp.}, 2025, to appear.

\bibitem{yamaguchi2014cpgraphs} F.~Yamaguchi, N.~Golde, D.~Arp, and K.~Rieck, "Modeling and discovering vulnerabilities with code property graphs," in \emph{Proc. IEEE Symp. Security and Privacy}, 2014, pp. 590--604.

\bibitem{wang2024sok} G.~Wang, J.~Zhao, and P.~Cheng, "SoK: Towards effective automated vulnerability repair," Tech. Rep., 2024.

\bibitem{chen2024survey} W.~Chen, L.~Huang, and Y.~Li, "Large language model for vulnerability detection and repair: Literature review and the road ahead," \emph{arXiv preprint}, 2024.

\bibitem{nong2024cot} D.~Nong, H.~Sun, and Q.~Zhang, "Chain-of-thought prompting for discovering and fixing vulnerabilities," \emph{arXiv preprint arXiv:2402.17230}, 2024.

\bibitem{wang2023patchverif} W.~Wang, Q.~Xie, and J.~Zheng, "PatchVerif: Checking patch correctness with symbolic execution," in \emph{Proc. ACM Int. Symp. Software Testing and Analysis (ISSTA)}, 2023, pp. 382--394.

\bibitem{unc2018career} University of North Carolina, "Scalable and trustworthy automatic program repair," Tech. Rep., 2018.
\end{thebibliography}
\end{document}
