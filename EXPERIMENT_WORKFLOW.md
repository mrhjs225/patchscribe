# PatchScribe ì „ì²´ ì‹¤í—˜ ì›Œí¬í”Œë¡œìš°

ë…¼ë¬¸ì˜ ëª¨ë“  RQ(Research Questions)ë¥¼ ê²€ì¦í•˜ê¸° ìœ„í•œ ì™„ì „í•œ ì‹¤í—˜ ì‹¤í–‰ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

1. [í™˜ê²½ ì„¤ì •](#1-í™˜ê²½-ì„¤ì •)
2. [ë°ì´í„°ì…‹ ì¤€ë¹„](#2-ë°ì´í„°ì…‹-ì¤€ë¹„)
3. [RQ1: Theory-Guided Generation](#3-rq1-theory-guided-generation)
4. [RQ2: Dual Verification Effectiveness](#4-rq2-dual-verification-effectiveness)
5. [RQ3: Scalability and Performance](#5-rq3-scalability-and-performance)
6. [RQ4: Explanation Quality](#6-rq4-explanation-quality)
7. [ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”](#7-ê²°ê³¼-ë¶„ì„-ë°-ì‹œê°í™”)
8. [ë¬¸ì œ í•´ê²°](#8-ë¬¸ì œ-í•´ê²°)

---

## 1. í™˜ê²½ ì„¤ì •

### 1.1 Python í™˜ê²½ í™•ì¸
```bash
# Python 3.8 ì´ìƒ í™•ì¸
python3 --version

# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (requirements.txtê°€ ìˆë‹¤ë©´)
pip install -r requirements.txt

# ë˜ëŠ” ê°œë³„ ì„¤ì¹˜
pip install tree-sitter tqdm psutil
```

### 1.2 ì™¸ë¶€ ë„êµ¬ ì„¤ì¹˜ (ì„ íƒ ì‚¬í•­)
```bash
# Symbolic verificationì„ ìœ„í•œ ë„êµ¬ (V2ì— í•„ìš”)
# KLEE (ì„ íƒ)
sudo apt-get install klee

# Clang (ì»´íŒŒì¼ìš©)
sudo apt-get install clang

# GCC (exploit testingìš©, V1ì— í•„ìš”)
sudo apt-get install gcc
```

### 1.3 í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```bash
# LLM ì„¤ì • (ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© ì‹œ)
export PATCHSCRIBE_LLM_PROVIDER=ollama
export PATCHSCRIBE_LLM_MODEL=llama3.2:1b
export PATCHSCRIBE_LLM_ENDPOINT=http://localhost:11434

# ë˜ëŠ” OpenAI ì‚¬ìš© ì‹œ
# export PATCHSCRIBE_LLM_PROVIDER=openai
# export PATCHSCRIBE_LLM_MODEL=gpt-4
# export OPENAI_API_KEY=your_api_key_here
```

---

## 2. ë°ì´í„°ì…‹ ì¤€ë¹„

### 2.1 ë°ì´í„°ì…‹ í™•ì¸
```bash
# Zeroday repair ë°ì´í„°ì…‹ì´ ìˆëŠ”ì§€ í™•ì¸
ls -la datasets/zeroday_repair/

# ì¼€ì´ìŠ¤ ê°œìˆ˜ í™•ì¸
python3 -c "
from patchscribe.dataset import load_cases
cases = load_cases('zeroday')
print(f'Total cases: {len(cases)}')
for i, case in enumerate(cases[:3], 1):
    print(f'{i}. {case[\"id\"]} - {case[\"cwe_id\"]}')
"
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Total cases: 10
1. CWE-125___CVE-2024-25116.c___1-64___13.c - CWE-125
2. CWE-125___CVE-2024-29489.c___1-59___5.c - CWE-125
3. CWE-190___CVE-2024-26130.c___1-98___56.c - CWE-190
```

### 2.2 ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
```bash
# ëª¨ë“  ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p results/{raw_results,rq_analysis,incomplete_patches,verification_ablation,figures}
```

---

## 3. RQ1: Theory-Guided Generation

**ëª©í‘œ**: C1 (baseline) vs C2 (vague hints) vs C3 (pre-hoc) vs C4 (full PatchScribe) ë¹„êµ

### 3.1 ì „ì²´ í‰ê°€ ì‹¤í–‰ (C1-C4 ëª¨ë‘)

```bash
# ì „ì²´ ì¡°ê±´ ì‹¤í–‰ (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼: ~30-60ë¶„)
python3 scripts/run_full_evaluation.py zeroday \
    --conditions c1 c2 c3 c4 \
    --limit 10 \
    --output results/evaluation_full \
    --llm-provider ollama \
    --llm-model llama3.2:1b

# ë˜ëŠ” ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (3ê°œ ì¼€ì´ìŠ¤ë§Œ)
python3 scripts/run_full_evaluation.py zeroday \
    --conditions c1 c2 c3 c4 \
    --limit 3 \
    --output results/evaluation_test
```

**ì˜ˆìƒ ì¶œë ¥:**
```
================================================================================
PATCHSCRIBE RQ EVALUATION
================================================================================
Dataset: zeroday
Output: results/evaluation_full
Conditions: ['c1', 'c2', 'c3', 'c4']
...
âœ… EVALUATION COMPLETE
```

### 3.2 ê°œë³„ ì¡°ê±´ ì‹¤í–‰ (ì„ íƒ)

í•„ìš”ì‹œ ê°œë³„ ì¡°ê±´ë§Œ ì¬ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# C1ë§Œ ì‹¤í–‰ (Baseline: post-hoc, no formal guidance)
python3 scripts/run_full_evaluation.py zeroday \
    --conditions c1 \
    --limit 10 \
    --output results/evaluation_c1

# C4ë§Œ ì‹¤í–‰ (Full PatchScribe)
python3 scripts/run_full_evaluation.py zeroday \
    --conditions c4 \
    --limit 10 \
    --output results/evaluation_c4
```

### 3.3 RQ1 ê²°ê³¼ í™•ì¸

```bash
# ê²°ê³¼ íŒŒì¼ í™•ì¸
ls -lh results/evaluation_full/raw_results/

# ê° ì¡°ê±´ì˜ ì„±ê³µë¥  ë¹ ë¥´ê²Œ í™•ì¸
for file in results/evaluation_full/raw_results/*_results.json; do
    echo "=== $(basename $file) ==="
    python3 -c "
import json
with open('$file') as f:
    data = json.load(f)
    metrics = data.get('metrics', {})
    print(f\"Success rate: {metrics.get('success_rate', 0):.1%}\")
    print(f\"Ground truth match: {metrics.get('ground_truth_match_rate', 0):.1%}\")
    print(f\"First attempt success: {metrics.get('first_attempt_success_rate', 0):.1%}\")
    print(f\"AST similarity: {metrics.get('avg_ast_overall_similarity', 0):.1%}\")
"
done
```

---

## 4. RQ2: Dual Verification Effectiveness

**ëª©í‘œ**: V1 (exploit-only) vs V2 (symbolic) vs V3 (consistency) vs V4 (triple) ë¹„êµ

### 4.1 ë¶ˆì™„ì „ íŒ¨ì¹˜ ìƒì„±

```bash
# ë‹¨ê³„ 1: ê° ì·¨ì•½ì ì— ëŒ€í•´ 2-3ê°œì˜ ë¶ˆì™„ì „ íŒ¨ì¹˜ ìƒì„±
python3 scripts/inject_incomplete_patches.py \
    --dataset zeroday \
    --limit 10 \
    --output results/incomplete_patches

# ê²°ê³¼ í™•ì¸
cat results/incomplete_patches/incomplete_patches_zeroday.json | python3 -m json.tool | head -50
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Loading zeroday dataset...
Loaded 10 cases

Generating incomplete patches for: CWE-125___CVE-2024-25116.c___1-64___13.c
  Generated 3 incomplete patches:
    - ..._incomplete_1: tautology_check
    - ..._incomplete_2: insufficient_validation
    - ..._incomplete_3: wrong_location

âœ… Saved incomplete patches to: results/incomplete_patches/incomplete_patches_zeroday.json
   Total cases: 10
   Total incomplete patches: 30
```

### 4.2 ê²€ì¦ ë°©ë²• ë¹„êµ ì‹¤í—˜ (V1-V4)

```bash
# ë‹¨ê³„ 2: V1, V2, V3, V4 ëª¨ë‘ ì‹¤í–‰í•˜ì—¬ ë¹„êµ
# ì£¼ì˜: ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ (~1-2ì‹œê°„)
python3 scripts/run_verification_ablation.py \
    --dataset zeroday \
    --limit 10 \
    --incomplete-patches results/incomplete_patches/incomplete_patches_zeroday.json \
    --output results/verification_ablation

# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (2ê°œ ì¼€ì´ìŠ¤ë§Œ)
python3 scripts/run_verification_ablation.py \
    --dataset zeroday \
    --limit 2 \
    --incomplete-patches results/incomplete_patches/incomplete_patches_zeroday.json \
    --output results/verification_ablation_test
```

**ì˜ˆìƒ ì¶œë ¥:**
```
================================================================================
Testing case: CWE-125___CVE-2024-25116.c___1-64___13.c
  Testing incomplete patch: ..._incomplete_1
    Running V1 (exploit-only)...
      Detected: True
    Running V2 (symbolic-only)...
      Detected: True
    Running V3 (consistency-only)...
      Detected: True
    Running V4 (triple verification)...
      Detected: True

================================================================================
PRECISION/RECALL ANALYSIS
================================================================================

V1:
  Detected incomplete: 18/30
  Precision: 60.00%
  Recall: 60.00%
  Avg execution time: 2.34s

V2:
  Detected incomplete: 22/30
  Precision: 73.33%
  Recall: 73.33%
  Avg execution time: 15.67s

V3:
  Detected incomplete: 25/30
  Precision: 83.33%
  Recall: 83.33%
  Avg execution time: 8.45s

V4:
  Detected incomplete: 27/30
  Precision: 90.00%
  Recall: 90.00%
  Avg execution time: 24.12s
```

### 4.3 RQ2 ê²°ê³¼ ë¶„ì„

```bash
# Precision/Recall ìš”ì•½
python3 -c "
import json
with open('results/verification_ablation/verification_ablation_zeroday.json') as f:
    data = json.load(f)

print('Verification Method Comparison:')
print('='*60)
for method in ['V1', 'V2', 'V3', 'V4']:
    results = data.get(method, [])
    if results:
        detected = sum(1 for r in results if r['detected_incomplete'])
        total = len(results)
        avg_time = sum(r['execution_time'] for r in results) / len(results)

        print(f'{method}:')
        print(f'  Detection rate: {detected}/{total} ({detected/total:.1%})')
        print(f'  Avg time: {avg_time:.2f}s')
        print()
"
```

---

## 5. RQ3: Scalability and Performance

**ëª©í‘œ**: ì½”ë“œ ë³µì¡ë„ë³„ ì„±ëŠ¥ ì¸¡ì •

RQ3ëŠ” ì´ë¯¸ RQ1 í‰ê°€ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤ (performance profiling).

### 5.1 ì„±ëŠ¥ ë°ì´í„° ì¶”ì¶œ

```bash
# C4 (full PatchScribe) ê²°ê³¼ì—ì„œ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì¶œ
python3 scripts/run_rq_analysis.py \
    results/evaluation_full/raw_results/full_patchscribe_c4_results.json \
    -o results/rq_analysis/rq3_performance.json
```

### 5.2 RQ3 ê²°ê³¼ í™•ì¸

```bash
# ë³µì¡ë„ë³„ ì„±ëŠ¥ ìš”ì•½
python3 -c "
import json
with open('results/rq_analysis/rq3_performance.json') as f:
    data = json.load(f)

rq3 = data.get('rq3_scalability_performance', [])
print('Performance by Code Complexity:')
print('='*60)
for result in rq3:
    print(f\"Complexity: {result['complexity_level']}\")
    print(f\"  Cases: {result['case_count']}\")
    print(f\"  Avg iterations: {result['avg_iterations']:.1f}\")
    if result.get('avg_total_time'):
        print(f\"  Avg total time: {result['avg_total_time']:.2f}s\")
        print(f\"  Avg phase 1 (formalization): {result.get('avg_phase1_time', 0):.2f}s\")
        print(f\"  Avg phase 2 (generation): {result.get('avg_phase2_time', 0):.2f}s\")
        print(f\"  Avg phase 3 (verification): {result.get('avg_phase3_time', 0):.2f}s\")
    print()
"
```

---

## 6. RQ4: Explanation Quality

**ëª©í‘œ**: Explanation í’ˆì§ˆ í‰ê°€ (ìë™ + ìˆ˜ë™)

### 6.1 Explanation ë©”íŠ¸ë¦­ ìë™ í‰ê°€

RQ4ë„ RQ1 í‰ê°€ì— ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

```bash
# C4 ê²°ê³¼ì—ì„œ explanation ë©”íŠ¸ë¦­ ì¶”ì¶œ
python3 scripts/run_rq_analysis.py \
    results/evaluation_full/raw_results/full_patchscribe_c4_results.json \
    -o results/rq_analysis/rq4_explanation.json
```

### 6.2 Blind Evaluation ìƒì„± (ìˆ˜ë™ í‰ê°€ìš©)

```bash
# ì „ë¬¸ê°€ ë¦¬ë·°ë¥¼ ìœ„í•œ blind evaluation íŒŒì¼ ìƒì„±
python3 scripts/generate_blind_explanations.py \
    results/evaluation_full/raw_results/full_patchscribe_c4_results.json \
    --output results/blind_evaluation

# ìƒì„±ëœ íŒŒì¼ í™•ì¸
ls -lh results/blind_evaluation/
```

### 6.3 RQ4 ê²°ê³¼ í™•ì¸

```bash
# Explanation í’ˆì§ˆ ë©”íŠ¸ë¦­ ìš”ì•½
python3 -c "
import json
with open('results/rq_analysis/rq4_explanation.json') as f:
    data = json.load(f)

rq4 = data.get('rq4_explanation_quality', [])
print('Explanation Quality Metrics:')
print('='*60)
for result in rq4:
    print(f\"Type: {result['explanation_type']}\")
    print(f\"  Checklist coverage: {result['checklist_coverage']:.1%}\")
    if result.get('avg_accuracy_score', 0) > 0:
        print(f\"  Accuracy score: {result['avg_accuracy_score']:.2f}/5\")
        print(f\"  Clarity score: {result['avg_clarity_score']:.2f}/5\")
        print(f\"  Causality score: {result['avg_causality_score']:.2f}/5\")
    print()
"
```

---

## 7. ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”

### 7.1 ëª¨ë“  RQì— ëŒ€í•œ ì¢…í•© ë¶„ì„

```bash
# ê° ì¡°ê±´(C1-C4)ì— ëŒ€í•´ RQ ë¶„ì„ ì‹¤í–‰
for condition in baseline_c1 vague_hints_c2 prehoc_c3 full_patchscribe_c4; do
    if [ -f "results/evaluation_full/raw_results/${condition}_results.json" ]; then
        echo "Analyzing $condition..."
        python3 scripts/run_rq_analysis.py \
            "results/evaluation_full/raw_results/${condition}_results.json" \
            -o "results/rq_analysis/rq_analysis_${condition}.json"
    fi
done

# ë¹„êµ ë¶„ì„ ìƒì„±
python3 scripts/run_rq_analysis.py \
    results/evaluation_full/raw_results/ \
    -o results/rq_analysis/comparative_analysis.json
```

### 7.2 ìµœì¢… ë³´ê³ ì„œ í™•ì¸

```bash
# ìë™ ìƒì„±ëœ markdown ë³´ê³ ì„œ í™•ì¸
cat results/evaluation_full/EVALUATION_REPORT.md

# ê° RQì˜ markdown ë³´ê³ ì„œ
ls results/rq_analysis/*.md
```

### 7.3 ì£¼ìš” ë©”íŠ¸ë¦­ ìš”ì•½ ì¶œë ¥

```bash
# ì „ì²´ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ í…Œì´ë¸”ë¡œ ìš”ì•½
python3 << 'EOF'
import json
from pathlib import Path

conditions = {
    'C1 (Baseline)': 'baseline_c1_results.json',
    'C2 (Vague Hints)': 'vague_hints_c2_results.json',
    'C3 (Pre-hoc)': 'prehoc_c3_results.json',
    'C4 (Full PatchScribe)': 'full_patchscribe_c4_results.json'
}

print("="*80)
print("FINAL RESULTS SUMMARY - ALL RQs")
print("="*80)
print()

print("RQ1: Theory-Guided Generation Effectiveness")
print("-"*80)
print(f"{'Condition':<25} {'Success':<10} {'1st Attempt':<12} {'Ground Truth':<13} {'AST Sim':<10}")
print("-"*80)

for name, filename in conditions.items():
    filepath = Path(f'results/evaluation_full/raw_results/{filename}')
    if filepath.exists():
        with open(filepath) as f:
            data = json.load(f)
            metrics = data.get('metrics', {})
            success = metrics.get('success_rate', 0)
            first_attempt = metrics.get('first_attempt_success_rate', 0)
            ground_truth = metrics.get('ground_truth_match_rate', 0)
            ast_sim = metrics.get('avg_ast_overall_similarity', 0)
            print(f"{name:<25} {success:>8.1%} {first_attempt:>10.1%} {ground_truth:>11.1%} {ast_sim:>8.1%}")

print()
print("RQ2: Dual Verification Effectiveness")
print("-"*80)

verification_file = Path('results/verification_ablation/verification_ablation_zeroday.json')
if verification_file.exists():
    with open(verification_file) as f:
        data = json.load(f)
        print(f"{'Method':<15} {'Detection Rate':<20} {'Avg Time':<15}")
        print("-"*80)
        for method in ['V1', 'V2', 'V3', 'V4']:
            results = data.get(method, [])
            if results:
                detected = sum(1 for r in results if r['detected_incomplete'])
                total = len(results)
                avg_time = sum(r['execution_time'] for r in results) / len(results)
                print(f"{method:<15} {detected}/{total} ({detected/total:.1%})"[:35].ljust(35) + f"{avg_time:.2f}s")

print()
print("RQ3: Scalability and Performance (C4)")
print("-"*80)

c4_analysis = Path('results/rq_analysis/rq_analysis_full_patchscribe_c4.json')
if c4_analysis.exists():
    with open(c4_analysis) as f:
        data = json.load(f)
        rq3 = data.get('rq3_scalability_performance', [])
        print(f"{'Complexity':<15} {'Cases':<8} {'Avg Time':<12} {'Iterations':<12}")
        print("-"*80)
        for result in rq3:
            complexity = result['complexity_level']
            cases = result['case_count']
            avg_time = result.get('avg_total_time', 0)
            iterations = result['avg_iterations']
            print(f"{complexity:<15} {cases:<8} {avg_time:>8.2f}s   {iterations:>8.1f}")

print()
print("="*80)
print("All experiments completed successfully!")
print("="*80)
EOF
```

---

## 8. ë¬¸ì œ í•´ê²°

### 8.1 ì¼ë°˜ì ì¸ ì˜¤ë¥˜

#### LLM ì—°ê²° ì˜¤ë¥˜
```bash
# Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
curl http://localhost:11434/api/tags

# Ollama ì‹œì‘
ollama serve

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull llama3.2:1b
```

#### ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# ë³‘ë ¬ ì²˜ë¦¬ ì œí•œ (ìˆœì°¨ ì‹¤í–‰)
python3 scripts/run_full_evaluation.py zeroday \
    --conditions c4 \
    --limit 5 \
    --max-workers 1  # ìˆœì°¨ ì‹¤í–‰
```

#### ë°ì´í„°ì…‹ ì—†ìŒ
```bash
# ë°ì´í„°ì…‹ ê²½ë¡œ í™•ì¸
ls -la datasets/zeroday_repair/

# ì—†ë‹¤ë©´ READMEì—ì„œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë°©ë²• í™•ì¸
cat README.md | grep -A 10 "dataset"
```

### 8.2 ë¶€ë¶„ ì¬ì‹¤í–‰

ì‹¤í—˜ ì¤‘ ì¼ë¶€ê°€ ì‹¤íŒ¨í•œ ê²½ìš°:

```bash
# íŠ¹ì • ì¡°ê±´ë§Œ ì¬ì‹¤í–‰
python3 scripts/run_full_evaluation.py zeroday \
    --conditions c4 \
    --limit 10 \
    --output results/evaluation_full

# íŠ¹ì • ê²€ì¦ ë°©ë²•ë§Œ ì¬ì‹¤í–‰ (ìˆ˜ë™ìœ¼ë¡œ ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì • í•„ìš”)
# run_verification_ablation.pyì—ì„œ ì›í•˜ëŠ” ë©”ì„œë“œë§Œ ì‹¤í–‰í•˜ë„ë¡ ìˆ˜ì •
```

### 8.3 ê²°ê³¼ ê²€ì¦

```bash
# ìƒì„±ëœ ëª¨ë“  ê²°ê³¼ íŒŒì¼ í™•ì¸
find results/ -name "*.json" -type f | sort

# ê° íŒŒì¼ì˜ ì¼€ì´ìŠ¤ ìˆ˜ í™•ì¸
for file in results/evaluation_full/raw_results/*_results.json; do
    cases=$(python3 -c "import json; data=json.load(open('$file')); print(len(data.get('cases', [])))")
    echo "$(basename $file): $cases cases"
done
```

---

## 9. ë¹ ë¥¸ ì „ì²´ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

ëª¨ë“  ì‹¤í—˜ì„ í•œ ë²ˆì— ì‹¤í–‰í•˜ë ¤ë©´:

```bash
#!/bin/bash
# run_all_experiments.sh

set -e  # ì˜¤ë¥˜ ì‹œ ì¤‘ë‹¨

echo "Starting full experimental pipeline..."

# 1. Environment check
echo "Step 1: Checking environment..."
python3 --version
python3 -c "from patchscribe.dataset import load_cases; print('âœ… PatchScribe module OK')"

# 2. RQ1: Full evaluation
echo "Step 2: Running RQ1 evaluation (C1-C4)..."
python3 scripts/run_full_evaluation.py zeroday \
    --conditions c1 c2 c3 c4 \
    --limit 10 \
    --output results/evaluation_full

# 3. RQ2: Incomplete patches
echo "Step 3: Generating incomplete patches for RQ2..."
python3 scripts/inject_incomplete_patches.py \
    --dataset zeroday \
    --limit 10 \
    --output results/incomplete_patches

# 4. RQ2: Verification ablation
echo "Step 4: Running verification ablation (V1-V4)..."
python3 scripts/run_verification_ablation.py \
    --dataset zeroday \
    --limit 10 \
    --incomplete-patches results/incomplete_patches/incomplete_patches_zeroday.json \
    --output results/verification_ablation

# 5. RQ Analysis
echo "Step 5: Running RQ analysis..."
for condition in baseline_c1 vague_hints_c2 prehoc_c3 full_patchscribe_c4; do
    if [ -f "results/evaluation_full/raw_results/${condition}_results.json" ]; then
        python3 scripts/run_rq_analysis.py \
            "results/evaluation_full/raw_results/${condition}_results.json" \
            -o "results/rq_analysis/rq_analysis_${condition}.json"
    fi
done

# 6. Generate summary
echo "Step 6: Generating final summary..."
cat results/evaluation_full/EVALUATION_REPORT.md

echo ""
echo "âœ… All experiments completed successfully!"
echo "Results are in: results/"
echo ""
echo "Key files:"
echo "  - results/evaluation_full/EVALUATION_REPORT.md"
echo "  - results/rq_analysis/*.json"
echo "  - results/verification_ablation/verification_ablation_zeroday.json"
```

ì‹¤í–‰:
```bash
chmod +x run_all_experiments.sh
./run_all_experiments.sh 2>&1 | tee experiment_log.txt
```

---

## 10. ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„

| ë‹¨ê³„ | ì¼€ì´ìŠ¤ ìˆ˜ | ì˜ˆìƒ ì‹œê°„ | ì„¤ëª… |
|------|----------|----------|------|
| RQ1 - C1 (Baseline) | 10 | ~10ë¶„ | No formal guidance |
| RQ1 - C2 (Vague Hints) | 10 | ~12ë¶„ | Informal prompts |
| RQ1 - C3 (Pre-hoc) | 10 | ~15ë¶„ | E_bug without verification |
| RQ1 - C4 (Full) | 10 | ~20ë¶„ | Full PatchScribe |
| **RQ1 Total** | **10** | **~60ë¶„** | **ëª¨ë“  ì¡°ê±´** |
| RQ2 - Incomplete patches | 10 | ~2ë¶„ | íŒ¨ì¹˜ ìƒì„± |
| RQ2 - V1-V4 ablation | 30 patches | ~90ë¶„ | ëª¨ë“  ê²€ì¦ ë°©ë²• |
| **RQ2 Total** | **10+30** | **~90ë¶„** | **ê²€ì¦ ë¹„êµ** |
| RQ3 - Analysis | N/A | ~2ë¶„ | RQ1ì— í¬í•¨ |
| RQ4 - Analysis | N/A | ~2ë¶„ | RQ1ì— í¬í•¨ |
| **Grand Total** | **10 cases** | **~2.5-3ì‹œê°„** | **ì „ì²´ ì‹¤í—˜** |

*ì°¸ê³ : ì‹œê°„ì€ í•˜ë“œì›¨ì–´ì™€ LLM ì†ë„ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤.*

---

## 11. ê²°ê³¼ íŒŒì¼ êµ¬ì¡°

```
results/
â”œâ”€â”€ evaluation_full/
â”‚   â”œâ”€â”€ raw_results/
â”‚   â”‚   â”œâ”€â”€ baseline_c1_results.json           # RQ1: C1 ê²°ê³¼
â”‚   â”‚   â”œâ”€â”€ vague_hints_c2_results.json        # RQ1: C2 ê²°ê³¼
â”‚   â”‚   â”œâ”€â”€ prehoc_c3_results.json             # RQ1: C3 ê²°ê³¼
â”‚   â”‚   â””â”€â”€ full_patchscribe_c4_results.json   # RQ1: C4 ê²°ê³¼
â”‚   â”œâ”€â”€ rq_analysis/
â”‚   â”‚   â””â”€â”€ (RQë³„ ë¶„ì„ ê²°ê³¼)
â”‚   â””â”€â”€ EVALUATION_REPORT.md                   # ìµœì¢… ë³´ê³ ì„œ
â”œâ”€â”€ incomplete_patches/
â”‚   â””â”€â”€ incomplete_patches_zeroday.json        # RQ2: ë¶ˆì™„ì „ íŒ¨ì¹˜
â”œâ”€â”€ verification_ablation/
â”‚   â””â”€â”€ verification_ablation_zeroday.json     # RQ2: V1-V4 ë¹„êµ
â”œâ”€â”€ rq_analysis/
â”‚   â”œâ”€â”€ rq_analysis_baseline_c1.json
â”‚   â”œâ”€â”€ rq_analysis_full_patchscribe_c4.json
â”‚   â””â”€â”€ comparative_analysis.json              # ëª¨ë“  ì¡°ê±´ ë¹„êµ
â””â”€â”€ blind_evaluation/
    â””â”€â”€ (ìˆ˜ë™ í‰ê°€ìš© íŒŒì¼ë“¤)
```

---

## ìš”ì•½: í•µì‹¬ ëª…ë ¹ì–´ë§Œ

```bash
# 1. ì „ì²´ í‰ê°€ (RQ1, RQ3, RQ4)
python3 scripts/run_full_evaluation.py zeroday --conditions c1 c2 c3 c4 --limit 10

# 2. ë¶ˆì™„ì „ íŒ¨ì¹˜ ìƒì„± (RQ2)
python3 scripts/inject_incomplete_patches.py --dataset zeroday --limit 10

# 3. ê²€ì¦ ë¹„êµ (RQ2)
python3 scripts/run_verification_ablation.py --dataset zeroday --limit 10 \
    --incomplete-patches results/incomplete_patches/incomplete_patches_zeroday.json

# 4. ê²°ê³¼ ë¶„ì„
python3 scripts/run_rq_analysis.py results/evaluation_full/raw_results/full_patchscribe_c4_results.json

# 5. ë³´ê³ ì„œ í™•ì¸
cat results/evaluation_full/EVALUATION_REPORT.md
```

**ì™„ë£Œ!** ğŸ‰
