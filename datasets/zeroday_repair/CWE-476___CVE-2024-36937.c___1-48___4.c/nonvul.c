static int xdp_do_generic_redirect_map(struct net_device *dev,
				       struct sk_buff *skb,
				       struct xdp_buff *xdp,
				       struct bpf_prog *xdp_prog, void *fwd,
				       enum bpf_map_type map_type, u32 map_id,
				       u32 flags)
{
	struct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);
	struct bpf_map *map;
	int err;

	switch (map_type) {
	case BPF_MAP_TYPE_DEVMAP:
		fallthrough;
	case BPF_MAP_TYPE_DEVMAP_HASH:
		if (unlikely(flags & BPF_F_BROADCAST)) {
			map = READ_ONCE(ri->map);

			/* The map pointer is cleared when the map is being torn
			 * down by bpf_clear_redirect_map()
			 */
			if (unlikely(!map)) {
				err = -ENOENT;
				break;
			}

			WRITE_ONCE(ri->map, NULL);
			err = dev_map_redirect_multi(dev, skb, xdp_prog, map,
						     flags & BPF_F_EXCLUDE_INGRESS);
		} else {
			err = dev_map_generic_redirect(fwd, skb, xdp_prog);
		}
		if (unlikely(err))
			goto err;
		break;
	case BPF_MAP_TYPE_XSKMAP:
		err = xsk_generic_rcv(fwd, xdp);
		if (err)
			goto err;
		consume_skb(skb);
		break;
	case BPF_MAP_TYPE_CPUMAP:
		err = cpu_map_generic_redirect(fwd, skb);
		if (unlikely(err))
			goto err;
		break;
	default:
		err = -EBADRQC;
		goto err;
	}

	_trace_xdp_redirect_map(dev, xdp_prog, fwd, map_type, map_id, ri->tgt_index);
	return 0;
err:
	_trace_xdp_redirect_map_err(dev, xdp_prog, fwd, map_type, map_id, ri->tgt_index, err);
	return err;
}
